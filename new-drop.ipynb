{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6664112,"sourceType":"datasetVersion","datasetId":3845414},{"sourceId":7072261,"sourceType":"datasetVersion","datasetId":4072982},{"sourceId":7079574,"sourceType":"datasetVersion","datasetId":4078099}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport torchvision\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.utils.data as util_data\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torchvision.datasets as dsets\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torchvision import models\nimport cv2\nimport time\n\nimport csv\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport shutil\nfrom typing import Any\nimport random\nimport torchvision","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_root = '/kaggle/input/poly600/poly_600'\n#dataset_root = ‘../input/datagood’\n\ndataset_train_root = os.path.join(dataset_root, 'train')\ndataset_val_root = os.path.join(dataset_root, 'test/')\ndataset_all_dataset_root = os.path.join(dataset_root, 'all_dataset')\n\n\n# win设为0 \nnum_workers = 8\n\nnum_threads =8 \n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs =100\n# epoch_lr_decrease = 100\nlearning_rate = 0.004\nencode_length = 256\nnum_classes =60","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 配置训练集DataSet类\nclass SessionDataset(Dataset):\n    \n    # root数据集根目录\n    def __init__(self, root, transform=None):\n        # 对继承自父类的属性进行初始化\n#         super(Oxford102Dataset, self).__init__()\n        \n        #读取图像标签  [  0   0   0 ... 599 599 599]\n        self.labels = np.array([int(x.split('_')[0]) for x in os.listdir(path=root)])\n\n        # 构造图像路径\n\n        self.image_files = np.array([x.path for x in os.scandir(path=root)])\n        \n        # 数据增强\n        self.transform = transform\n        \n    \n    def __getitem__(self, index):\n        \n        # 根据Index读取图像\n        img = cv2.imread(self.image_files[index])\n        \n        #根据index读取标签\n        label = self.labels[index]\n\n\n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n    \n    \n    def __len__(self):\n        return np.size(self.image_files)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transform = {\n    \"train\": transforms.Compose([\n        transforms.ToPILImage(),#将数据类型转变为图片数据\n#         transforms.RandomResizedCrop((224, 224)),#随机裁剪224*224\n        transforms.RandomResizedCrop((128, 128)),#随机裁剪224*224\n        transforms.RandomHorizontalFlip(),#水平防线随机反转\n        transforms.ToTensor(),#转为Tensor\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626)),#标准化处理\n        \n    ]),\n    \"val\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \"all_dataset\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_batch_size = 32#64\nval_batch_size = 32\n\n\n\n\nsession_train_dataset = SessionDataset(root=dataset_train_root,\n                                    transform=data_transform['train'])\n\ntrain_loader = DataLoader(session_train_dataset,\n                         batch_size=train_batch_size,\n                         shuffle=True,\n                         num_workers=num_workers)\n\n\nsession_val_dataset = SessionDataset(root=dataset_val_root,\n                                    transform=data_transform['val'])\nval_loader = DataLoader(session_val_dataset,\n                         batch_size=val_batch_size,\n                         shuffle=False,\n                         num_workers=num_workers)\n\n# 包含整个验证集\n\nsessiondatabaseDataset = SessionDataset(root=dataset_all_dataset_root,\n                                    transform=data_transform['all_dataset'])\n\ndatabase_loader = DataLoader(sessiondatabaseDataset,\n                         batch_size=train_batch_size,\n                         shuffle=False,\n                         num_workers=num_workers)\n\ntrain_num = len(session_train_dataset)#4800\nval_num = len(session_val_dataset)#1200\ndatasetloader_num = len(sessiondatabaseDataset)#6000\nprint(train_num, val_num,datasetloader_num)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n__all__ = ['iresnet50_cbam']\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(CBAM, self).__init__()\n        self.channel_gate = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        self.spatial_gate = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Channel-wise attention\n        channel_avg = self.channel_gate(x)\n        x = x * channel_avg\n\n        # Spatial-wise attention\n        channel_max = self.spatial_gate(x)\n        x = x * channel_max\n\n        return x\n\nclass IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1):\n        super(IBasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05,)\n        self.conv1 = conv3x3(inplanes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.prelu = nn.PReLU(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn3 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n        out = self.bn1(x)\n        out = self.conv1(out)\n        out = self.bn2(out)\n        out = self.prelu(out)\n        out = self.conv2(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        return out\n\nclass IResNet(nn.Module):\n    fc_scale = 7 * 7\n\n    def __init__(self, block, layers, dropout=0, num_features=512, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n        super(IResNet, self).__init__()\n        self.fp16 = fp16\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n        self.prelu = nn.PReLU(self.inplanes)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n        self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05,)\n        self.dropout = nn.Dropout(p=dropout, inplace=True)\n        self.cbam = CBAM(512 * block.expansion)\n        self.fc = nn.Linear(4 * 512 * block.expansion * self.fc_scale, num_features)\n        self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n        nn.init.constant_(self.features.weight, 1.0)\n        self.features.weight.requires_grad = False\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, 0, 0.1)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, IBasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion, eps=1e-05, ),\n            )\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        with torch.cuda.amp.autocast(self.fp16):\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.prelu(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n            x = self.bn2(x)\n            x = self.dropout(x)\n            x = self.cbam(x)\n            x = torch.flatten(x, 1)\n        x = self.fc(x.float() if self.fp16 else x)\n        x = self.features(x)\n        return x\n\ndef iresnet50_cbam(pretrained=False, progress=True, **kwargs):\n    return IResNet(IBasicBlock, [3, 4, 14, 3], **kwargs)\n\n ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n \n\n\ndef logistic(x, k=1, x0=0):\n    \"\"\"\n    Logistic function: L / (1 + exp(-k * (x - x0)))\n    \n    Parameters:\n    - x: Input values\n    - k: Steepness of the curve (default: 1)\n    - x0: x-value of the sigmoid's midpoint (default: 0)\n    \n    Returns:\n    - Result of the logistic function\n    \"\"\"\n    return 1 / (1 + np.exp(-k * (x - x0)))\n\n\ndef new_dropout(x, level=0.5):  \n    if level < 0. or level >= 1:#level是概率值，必须在0~1之间  \n        raise Exception('Dropout level must be in interval [0, 1].')  \n    retain_prob = 1. - level  \n    #我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样  \n    #硬币 正面的概率为p，n表示每个神经元试验的次数  \n    #因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。 \n     \n   # sample=nn.random.binomial(n=1,p=retain_prob,size=x.shape)#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了  \n    sample = torch.from_numpy(np.array(logistic(0.3, 1000))).to(device)\n\n#\n  #  sample=np.array(logistic(0.36,x.dim()))#\n#     print (sample)  \n#     print(type(sample))\n    \n    x *=sample          #屏蔽某些神经元\n    \n    x /= retain_prob   #此处是dropout\n    return x\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SecureRandomProjectionLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, num_classes):\n        super(SecureRandomProjectionLayer, self).__init__()\n        self.random_projection_matrices = nn.Parameter(torch.randn(num_classes, input_dim, output_dim) * 0.01, requires_grad=False)\n\n    def forward(self, x, class_labels):\n        class_specific_matrices = self.random_projection_matrices[class_labels]\n        transformed_data = torch.matmul(x, class_specific_matrices)\n        return transformed_data\n\nclass hash(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        return torch.sign(input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\ndef hash_layer(input):\n    return hash.apply(input)\n\nclass CNN_ResNet(nn.Module):\n    def __init__(self, encode_length, num_classes):\n        super(CNN_ResNet, self).__init__()\n        self.res = iresnet50_cbam(pretrained=False, num_features=1000).to(device)\n        self.res.fc = nn.Linear(in_features=32768, out_features=1000, bias=True)\n        self.secure_projection = SecureRandomProjectionLayer(1000, 1000,num_classes)\n        self.fc_plus = nn.Linear(1000, encode_length)\n        self.fc = nn.Linear(encode_length, num_classes, bias=False)\n\n    def forward(self, x, class_labels=None):\n        x = self.res(x)\n        if class_labels is not None:\n            class_specific_matrices = self.secure_projection.random_projection_matrices[class_labels]\n            x = torch.matmul(x, class_specific_matrices)\n        if x.dim() != 3:\n            x = x.view(x.size(0), -1, 1)\n\n        n, j = x.shape[1], x.shape[2]\n        x = x.view(x.size(0), n, j)\n        max_values, _ = x.max(dim=2)\n        x = self.fc_plus(max_values)\n        code = hash_layer(x)\n        \n        # If class_labels are provided, use the class-specific projection matrix\n        if class_labels is not None:\n            class_specific_matrices = self.secure_projection.random_projection_matrices[class_labels]\n            x = torch.matmul(x, class_specific_matrices)\n        \n        output = self.fc(code)\n        \n        return output, x, code\n\n# Example usage\n \nnum_projection_classes =60\n#net = CNN_ResNet(encode_length=encode_length, num_classes=num_classes).to(device)\nprint('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass SecureRandomProjectionLayer(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(SecureRandomProjectionLayer, self).__init__()\n        self.random_projection_matrix = nn.Parameter(torch.randn(input_dim, output_dim) * 0.01, requires_grad=False)\n\n\n    def forward(self, x):\n        # Apply secure random projection\n        transformed_data = torch.matmul(x, self.random_projection_matrix)\n        return transformed_data\n# new layer\nclass hash(Function):\n    @staticmethod\n    def forward(ctx, input):\n        #ctx.save_for_backward(input)\n        #使用在pytroch中的sign函数得到二值哈希码\n        return torch.sign(input)    \n    @staticmethod\n    def backward(ctx, grad_output):\n        #input,  = ctx.saved_tensors\n        #grad_output = grad_output.data\n        return grad_output\ndef hash_layer(input):\n    return hash.apply(input)\n\n\nclass CNN_ResNet(nn.Module):\n    def __init__(self, encode_length, num_classes ):\n        super(CNN_ResNet, self).__init__()\n        self.res = iresnet50_cbam(pretrained=False, num_features=1000).to(device)\n        \n        self.res.fc = nn.Linear(in_features=32768, out_features=1000)\n        self.secure_projection = SecureRandomProjectionLayer(1000,1000)\n      \n        self.fc_plus = nn.Linear(1000, encode_length) \n        \n        # Add the secure random projection layer\n        \n        \n        self.fc = nn.Linear( encode_length, num_classes, bias=False)\n\n    def forward(self, x):\n        x = self.res(x)\n        \n        x = new_dropout(x)\n        \n        # Apply the secure random projection before x = self.fc_plus(x)\n        x = self.secure_projection(x)\n               \n        if x.dim() != 3:\n            x = x.view(x.size(0), -1, 1)\n\n        # Get the dimensions\n        n, j = x.shape[1], x.shape[2]\n\n        # Reshape x to have dimensions (batch_size, n, j)\n        x = x.view(x.size(0), n, j)\n\n        # Find the maximum element in each subspace\n        max_values, _ = x.max(dim=2)\n\n \n        x = self.fc_plus(max_values)\n\n        code = hash_layer(x)\n\n   \n        output = self.fc(code)\n        \n        return output, x, code\n    \n\nnet = CNN_ResNet(encode_length=encode_length, num_classes=num_classes).to(device)\nprint('ok')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compress(train, test, model, classes=60):\n    retrievalB = list([])\n    retrievalL = list([])\n    with torch.no_grad():\n        for batch_step, (data, target) in enumerate(train):\n            \n            var_data = data.to(device)\n            \n            # code为网络输出的哈希码\n            _,_, code = model(var_data)\n            \n            retrievalB.extend(code.cpu().data.numpy())\n            retrievalL.extend(target)\n            \n        queryB=list([])\n        queryL=list([])\n        for batch_step, (data, target) in enumerate(test):     \n            var_data = data.to(device)\n            _,_, code = model(var_data)\n            \n            queryB.extend(code.cpu().data.numpy())\n            queryL.extend(target)\n            \n        retrievalB=np.array(retrievalB)\n        retrievalL=np.eye(classes)[np.array(retrievalL)]\n            \n        queryB=np.array(queryB)\n        queryL=np.eye(classes)[np.array(queryL)]   \n        \n            \n        return retrievalB, retrievalL, queryB, queryL\n\n\n    \n    \ndef calculate_hamming(B1, B2):\n    \"\"\"\n    :param B1:  vector [n]\n    :param B2:  vector [r*n]\n    :return: hamming distance [r]\n    \"\"\"\n    q = B2.shape[1] # max inner product value\n    distH = 0.5 * (q - np.dot(B1, B2.transpose()))\n    return distH\n\n\ndef calculate_map(qB, rB, queryL, retrievalL):\n    \"\"\"\n       :param qB: {-1,+1}^{mxq} query bits\n       :param rB: {-1,+1}^{nxq} retrieval bits\n       :param queryL: {0,1}^{mxl} query label\n       :param retrievalL: {0,1}^{nxl} retrieval label\n       :return:\n    \"\"\"\n    num_query = queryL.shape[0]\n    map = 0\n    for iter in range(num_query):\n        # gnd : check if exists any retrieval items with same label\n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        # tsum number of items with same label\n        tsum = np.sum(gnd).astype(int)\n        if tsum == 0:\n            continue\n        # sort gnd by hamming dist\n        hamm = calculate_hamming(qB[iter, :], rB)\n        ind = np.argsort(hamm)\n        gnd = gnd[ind]\n\n        count = np.linspace(1, tsum, tsum) # [1,2, tsum]\n        tindex = np.asarray(np.where(gnd == 1)) + 1.0\n        map_ = np.mean(count / (tindex))\n        # print(map_)\n        map = map + map_\n    map = map / num_query\n    return map\n\n\n\ndef calculate_top_map(qB, rB, queryL, retrievalL, topk):\n    \"\"\"\n    :param qB: {-1,+1}^{mxq} query bits\n    :param rB: {-1,+1}^{nxq} retrieval bits\n    :param queryL: {0,1}^{mxl} query label\n    :param retrievalL: {0,1}^{nxl} retrieval label\n    :param topk:\n    :return:\n    \"\"\"\n    num_query = queryL.shape[0]\n    topkmap = 0\n    for iter in range(num_query):\n        \n        # 当前测试集标签 乘 训练集标签的转置\n        # dot是为找到与当前查询二值哈希码与库中的哈希码相似（或者完全相同）的哈希码 >0的结果为True or False\n        # .astype(np.float32)将true变为1， false变为0\n        # gnd应该是标签的相似向量1为相似，0为不相似\n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        hamm = calculate_hamming(qB[iter, :], rB)\n        ind = np.argsort(hamm)\n        gnd = gnd[ind]\n\n        tgnd = gnd[0:topk]\n        tsum = np.sum(tgnd).astype(int)\n        if tsum == 0:\n            continue\n        count = np.linspace(1, tsum, tsum)\n\n        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n        topkmap_ = np.mean(count / (tindex))\n        # print(topkmap_)\n        topkmap = topkmap + topkmap_\n    topkmap = topkmap / num_query\n    return topkmap\n\n\n# 计算mAP\n# rB整个数据集二值哈希码 qB测试集二值哈希码 retrievalL整个标签 queryL测试集标签（标签都是oneHot编码） topk\ndef myCalcTopMap(rB, qB, retrievalL, queryL, topk):\n    \n    # 测试集标签的数量\n    num_query = queryL.shape[0]\n    \n    topkmap = 0\n    \n    # 计算AP\n    for iter in tqdm(range(num_query)):\n        \n        # 当前测试集标签 乘 训练集标签的转置\n        # dot是为找到与当前查询二值哈希码与库中的哈希码相似（或者完全相同）的哈希码 >0的结果为True or False\n        # .astype(np.float32)将true变为1， false变为0\n        # gnd应该是标签的相似向量1为相似，0为不相似\n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        \n        # 计算当前测试集二值哈希码与全部数据集二值哈希码之间的汉明距离\n        hamm = CalcHammingDist(qB[iter, :], rB)\n        \n        # 取排序后的下标（升序）\n        ind = np.argsort(hamm)\n        \n        # 取按照汉明距离对gnd（标签相似度）排序\n        gnd = gnd[ind]\n        \n        # 取前topK个gnd\n        # tgnd 1,1,0,0,1..\n        tgnd = gnd[0:topk]\n        \n        # 对tgnd求和--tgnd中非1即0\n        # 表示topK中检索正确的个数\n        tsum = np.sum(tgnd).astype(int)\n        if tsum == 0:\n            continue\n        \n        # linspace(start, stop, num)--1,2,3...tsum\n        count = np.linspace(1, tsum, tsum)\n        \n        # asarray-生成ndarray数组，相比array,当输入为ndarray时，不会重新创建，更省内存\n        # tgnd [1,1,,0,0,1..]长度为topk\n        # np.where,找到数组tgnd == 1的下标\n        # tindex含义为tgnd中值为1的下标数组+1\n        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n        \n        # count [1,2,3,...topk] tindex [1,3,5...]\n        # 其含义是AP?\n        topkmap_ = np.mean(count / (tindex))\n        # AP累加\n        topkmap = topkmap + topkmap_\n    # mAP\n    topkmap = topkmap / num_query\n    \n    return topkmap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练过程\n# 训练过程\nimport matplotlib.pyplot as plt\n\n# Initialize an empty list to store accuracy values for each epoch\nepoch_accuracies = []\nbest = 0.0\n\ntorch.set_num_threads(num_threads)\n\n\n# 根据条件判断是否使用多GPU\nif torch.cuda.device_count() > 1:\n    net = nn.DataParallel(net)\n    \nnet.to(device)\n\n\n# Train the Model\nacc_dict={}\nfor epoch in range(num_epochs):\n    \n    net.train()\n#     adjust_learning_rate(optimizer, epoch)\n    \n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs, feature, _ = net(images)\n\n\n        loss1 = criterion(outputs, labels)\n        #loss2 = F.mse_loss(torch.abs(feature), Variable(torch.ones(feature.size()).cuda()))\n#         loss2 = torch.mean(torch.abs(torch.pow(torch.abs(feature) - Variable(torch.ones(feature.size()).cuda()), 3)))\n        loss2 = torch.mean(torch.abs(torch.pow(torch.abs(feature) - torch.ones(feature.size()).to(device), 3)))\n        loss = loss1 + 0.1 * loss2\n        loss.backward()\n        optimizer.step()\n\n#         if (i + 1) % (len(oxford102TrainDataset) // batch_size / 2) == 0:\n        print ('Epoch [%d/%d], Iter [%d/%d] Loss1: %.4f Loss2: %.4f'\n               % (epoch + 1, num_epochs, i + 1, len(sessiondatabaseDataset) // train_batch_size,\n                  loss1.item(), loss2.item()))\n\n    # Test the Model\n    net.eval()  # Change model to 'eval' mode\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        \n        for images, labels in val_loader:\n            images = images.to(device)\n            # 取网络最后全连接层的参数\n            outputs, _, _ = net(images)\n            # 第一个参数为最大值，第二个是最大值的下标\n            _, predicted = torch.max(outputs.cpu().data, 1)\n\n            total += labels.size(0)\n\n            correct += (predicted == labels).sum()\n\n    print('Test Accuracy of the model: %.2f %%' % (100.0 * correct / total))\n\n    if 1.0 * correct / total > best:\n        best = 1.0 * correct / total\n        torch.save(net.state_dict(), 'temp44.pkl')\n       \n    \n        \n    print('best: %.2f %%' % (best * 100.0))\n\n    net.eval()\n\n    retrievalB, retrievalL, queryB, queryL = compress(train_loader, val_loader, net)\n    print('---calculate map1---')\n    result1 = calculate_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL)\n    #torch.save(net.state_dict(), 'map1_{}_module_all_obj.pkl'.format(round(result1,3)))\n    print(result1)\n    retrievalB, retrievalL, queryB, queryL = compress(database_loader, val_loader, net)\n    print('---calculate map2---')\n    \n    result2 = calculate_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL)\n    print(result2)\n    epoch_accuracies.append(result2)\n\n\n# Plot the accuracy vs. epoch graph\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, num_epochs + 1), epoch_accuracies, marker='o', linestyle='-')\nplt.title('Accuracy vs. Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (result2)')\nplt.grid(True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        #torch.save(net.state_dict(), 'map2_{}_module_all_obj.pkl'.format(round(result2,3)))\n    genuine_scores = []  # Store positive scores\n    imposter_scores = []  # Store negative scores\n\n    with torch.no_grad():\n        for i in range(len(queryB)):\n            for j in range(len(retrievalB)):\n                if np.all(queryL[i] == retrievalL[j]):  # Genuine pair\n                    queryB_tensor = torch.from_numpy(queryB[i])  # Convert to a PyTorch tensor\n                    retrievalB_tensor = torch.from_numpy(retrievalB[j])  # Convert to a PyTorch tensor\n                    genuine_scores.append(-1 * torch.dist(queryB_tensor, retrievalB_tensor, p=2).item())\n                else:  # Imposter pair\n                    queryB_tensor = torch.from_numpy(queryB[i])  # Convert to a PyTorch tensor\n                    retrievalB_tensor = torch.from_numpy(retrievalB[j])  # Convert to a PyTorch tensor\n                    imposter_scores.append(-1 * torch.dist(queryB_tensor, retrievalB_tensor, p=2).item())\n\n    # Set a threshold (you may need to fine-tune this)\n    threshold = 0.0  # Adjust the threshold as needed\n\n    # Calculate FAR and FRR\n    far = np.mean(np.array(imposter_scores) >= threshold)\n    frr = np.mean(np.array(genuine_scores) < threshold)\n\n    # Calculate EER\n    eer = 0.5  # Initialize EER\n    threshold_range = np.linspace(min(imposter_scores), max(genuine_scores), num=1000)\n\n    for t in threshold_range:\n        far_t = np.mean(np.array(imposter_scores) >= t)\n        frr_t = np.mean(np.array(genuine_scores) < t)\n\n        if abs(far_t - frr_t) < abs(far - frr):\n            far, frr = far_t, frr_t\n            eer = 0.5 * (far + frr)\n\n    print(f'FAR: {far * 100:.2f}%')\n    print(f'FRR: {frr * 100:.2f}%')\n    print(f'EER: {eer * 100:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_transform = {\n    \"train\": transforms.Compose([\n        transforms.ToPILImage(),#将数据类型转变为图片数据\n#         transforms.RandomResizedCrop((224, 224)),#随机裁剪224*224\n        transforms.RandomResizedCrop((128, 128)),#随机裁剪224*224\n        transforms.RandomHorizontalFlip(),#水平防线随机反转\n        transforms.ToTensor(),#转为Tensor\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626)),#标准化处理\n        \n    ]),\n    \"val\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \"all_dataset\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \n}","metadata":{"execution":{"iopub.status.busy":"2024-02-23T00:41:53.316690Z","iopub.execute_input":"2024-02-23T00:41:53.317116Z","iopub.status.idle":"2024-02-23T00:41:53.770730Z","shell.execute_reply.started":"2024-02-23T00:41:53.317081Z","shell.execute_reply":"2024-02-23T00:41:53.769076Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_transform \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\u001b[38;5;66;03m#将数据类型转变为图片数据\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#         transforms.RandomResizedCrop((224, 224)),#随机裁剪224*224\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\u001b[38;5;66;03m#随机裁剪224*224\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\u001b[38;5;66;03m#水平防线随机反转\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\u001b[38;5;66;03m#转为Tensor\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.51568358\u001b[39m, \u001b[38;5;241m0.51568358\u001b[39m, \u001b[38;5;241m0.51568358\u001b[39m), (\u001b[38;5;241m0.10332626\u001b[39m, \u001b[38;5;241m0.10332626\u001b[39m, \u001b[38;5;241m0.10332626\u001b[39m)),\u001b[38;5;66;03m#标准化处理\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \n\u001b[1;32m     10\u001b[0m     ]),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     12\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[1;32m     13\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\n\u001b[1;32m     14\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     15\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.51568358\u001b[39m, \u001b[38;5;241m0.51568358\u001b[39m, \u001b[38;5;241m0.51568358\u001b[39m), (\u001b[38;5;241m0.10332626\u001b[39m, \u001b[38;5;241m0.10332626\u001b[39m, \u001b[38;5;241m0.10332626\u001b[39m))\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m     ]),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     19\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToPILImage(),\n\u001b[1;32m     20\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\n\u001b[1;32m     21\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     22\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.51568358\u001b[39m, \u001b[38;5;241m0.51568358\u001b[39m, \u001b[38;5;241m0.51568358\u001b[39m), (\u001b[38;5;241m0.10332626\u001b[39m, \u001b[38;5;241m0.10332626\u001b[39m, \u001b[38;5;241m0.10332626\u001b[39m))\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m     ]),\n\u001b[1;32m     25\u001b[0m     \n\u001b[1;32m     26\u001b[0m }\n","\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"],"ename":"NameError","evalue":"name 'transforms' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ","metadata":{}}]}