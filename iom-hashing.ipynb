{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6445846,"sourceType":"datasetVersion","datasetId":3720773},{"sourceId":6462275,"sourceType":"datasetVersion","datasetId":3731996},{"sourceId":6463818,"sourceType":"datasetVersion","datasetId":3732977},{"sourceId":6469702,"sourceType":"datasetVersion","datasetId":3736610},{"sourceId":6471822,"sourceType":"datasetVersion","datasetId":3738085},{"sourceId":6472084,"sourceType":"datasetVersion","datasetId":3738285},{"sourceId":6472826,"sourceType":"datasetVersion","datasetId":3738814},{"sourceId":6482088,"sourceType":"datasetVersion","datasetId":3745087},{"sourceId":6482261,"sourceType":"datasetVersion","datasetId":3745223},{"sourceId":6486712,"sourceType":"datasetVersion","datasetId":3748081},{"sourceId":6496639,"sourceType":"datasetVersion","datasetId":3754944}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport torchvision\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.utils.data as util_data\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torchvision.datasets as dsets\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torchvision import models\nimport cv2\nimport time\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport shutil\nfrom typing import Any\nimport random\nimport torchvision","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-10-12T05:46:34.960586Z","iopub.execute_input":"2023-10-12T05:46:34.961358Z","iopub.status.idle":"2023-10-12T05:46:39.755671Z","shell.execute_reply.started":"2023-10-12T05:46:34.961325Z","shell.execute_reply":"2023-10-12T05:46:39.754782Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset_root = '/kaggle/input/palm-p600/palm_6000'\n#dataset_root = 'F:\\深度哈希\\新建文件夹\\data'\n\ndataset_train_root = os.path.join(dataset_root, 'train')\ndataset_val_root = os.path.join(dataset_root, 'val/')\ndataset_all_dataset_root = os.path.join(dataset_root, 'all_dataset')\n\n\n# win设为0 \nnum_workers = 8\n\nnum_threads =8 \n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:39.757694Z","iopub.execute_input":"2023-10-12T05:46:39.758658Z","iopub.status.idle":"2023-10-12T05:46:39.765658Z","shell.execute_reply.started":"2023-10-12T05:46:39.758591Z","shell.execute_reply":"2023-10-12T05:46:39.764521Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"num_epochs =100\n# epoch_lr_decrease = 100\nlearning_rate = 0.004\nencode_length =256\nnum_classes =60\n ","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:39.768391Z","iopub.execute_input":"2023-10-12T05:46:39.769160Z","iopub.status.idle":"2023-10-12T05:46:39.784776Z","shell.execute_reply.started":"2023-10-12T05:46:39.769122Z","shell.execute_reply":"2023-10-12T05:46:39.783696Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 配置训练集DataSet类\nclass SessionDataset(Dataset):\n    \n  \n    def __init__(self, root, transform=None):\n   \n        self.labels = np.array([int(x.split('_')[0]) for x in os.listdir(path=root)])\n\n      \n\n        self.image_files = np.array([x.path for x in os.scandir(path=root)])\n        \n     \n        self.transform = transform\n        \n    \n    def __getitem__(self, index):\n        \n    \n        img = cv2.imread(self.image_files[index])\n        \n    \n        label = self.labels[index]\n\n\n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n    \n    \n    def __len__(self):\n        return np.size(self.image_files)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:39.787380Z","iopub.execute_input":"2023-10-12T05:46:39.788293Z","iopub.status.idle":"2023-10-12T05:46:39.799895Z","shell.execute_reply.started":"2023-10-12T05:46:39.788253Z","shell.execute_reply":"2023-10-12T05:46:39.799063Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_transform = {\n    \"train\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.RandomResizedCrop((224, 224)),\n#         transforms.RandomResizedCrop((128, 128)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),#转为Tensor\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626)),#标准化处理\n        \n    ]),\n    \"val\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \"all_dataset\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \n}","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:39.801376Z","iopub.execute_input":"2023-10-12T05:46:39.801855Z","iopub.status.idle":"2023-10-12T05:46:39.814844Z","shell.execute_reply.started":"2023-10-12T05:46:39.801818Z","shell.execute_reply":"2023-10-12T05:46:39.814035Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_batch_size =32#64\nval_batch_size = 32\n\n\n\n\nsession_train_dataset = SessionDataset(root=dataset_train_root,\n                                    transform=data_transform['train'])\n\ntrain_loader = DataLoader(session_train_dataset,\n                         batch_size=train_batch_size,\n                         shuffle=True,\n                         num_workers=num_workers)\n\n\nsession_val_dataset = SessionDataset(root=dataset_val_root,\n                                    transform=data_transform['val'])\nval_loader = DataLoader(session_val_dataset,\n                         batch_size=val_batch_size,\n                         shuffle=False,\n                         num_workers=num_workers)\n\n# 包含整个验证集\n\nsessiondatabaseDataset = SessionDataset(root=dataset_all_dataset_root,\n                                    transform=data_transform['all_dataset'])\n\ndatabase_loader = DataLoader(sessiondatabaseDataset,\n                         batch_size=train_batch_size,\n                         shuffle=False,\n                         num_workers=num_workers)\n\ntrain_num = len(session_train_dataset)#4800\nval_num = len(session_val_dataset)#1200\ndatasetloader_num = len(sessiondatabaseDataset)#6000\nprint(train_num, val_num,datasetloader_num)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:39.816318Z","iopub.execute_input":"2023-10-12T05:46:39.816959Z","iopub.status.idle":"2023-10-12T05:46:40.214398Z","shell.execute_reply.started":"2023-10-12T05:46:39.816922Z","shell.execute_reply":"2023-10-12T05:46:40.213186Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"480 120 600\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Original resnet 18,50,100******","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n__all__ = ['iresnet18', 'iresnet34', 'iresnet50', 'iresnet100', 'iresnet200']\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=3,\n                     stride=stride,\n                     padding=dilation,\n                     groups=groups,\n                     bias=False,\n                     dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes,\n                     out_planes,\n                     kernel_size=1,\n                     stride=stride,\n                     bias=False)\n\n\nclass IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 groups=1, base_width=64, dilation=1):\n        super(IBasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05,)\n        self.conv1 = conv3x3(inplanes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.prelu = nn.PReLU(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn3 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n        out = self.bn1(x)\n        out = self.conv1(out)\n        out = self.bn2(out)\n        out = self.prelu(out)\n        out = self.conv2(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        return out\n\n\nclass IResNet(nn.Module):\n    fc_scale = 7 * 7\n    def __init__(self,\n                 block, layers, dropout=0, num_features=512, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n        super(IResNet, self).__init__()\n        self.fp16 = fp16\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n        self.prelu = nn.PReLU(self.inplanes)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n        self.layer2 = self._make_layer(block,\n                                       128,\n                                       layers[1],\n                                       stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block,\n                                       256,\n                                       layers[2],\n                                       stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block,\n                                       512,\n                                       layers[3],\n                                       stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05,)\n        self.dropout = nn.Dropout(p=dropout, inplace=True)\n        self.fc = nn.Linear(4 * 512 * block.expansion * self.fc_scale, num_features)\n        self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n        nn.init.constant_(self.features.weight, 1.0)\n        self.features.weight.requires_grad = False\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, 0, 0.1)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, IBasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion, eps=1e-05, ),\n            )\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, self.groups,\n                  self.base_width, previous_dilation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(self.inplanes,\n                      planes,\n                      groups=self.groups,\n                      base_width=self.base_width,\n                      dilation=self.dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        with torch.cuda.amp.autocast(self.fp16):\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.prelu(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n            x = self.bn2(x)\n            x = torch.flatten(x, 1)\n            x = self.dropout(x)\n        x = self.fc(x.float() if self.fp16 else x)\n        x = self.features(x)\n        return x\n\n\ndef _iresnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = IResNet(block, layers, **kwargs)\n    if pretrained:\n        raise ValueError()\n    return model\n\n\ndef iresnet18(pretrained=False, progress=True, **kwargs):\n    return _iresnet('iresnet18', IBasicBlock, [2, 2, 2, 2], pretrained,\n                    progress, **kwargs)\n\n\n \n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:40.216206Z","iopub.execute_input":"2023-10-12T05:46:40.216625Z","iopub.status.idle":"2023-10-12T05:46:40.243745Z","shell.execute_reply.started":"2023-10-12T05:46:40.216590Z","shell.execute_reply":"2023-10-12T05:46:40.242548Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"added CBAM with resnet 50","metadata":{}},{"cell_type":"code","source":"iresnet18()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:40.244924Z","iopub.execute_input":"2023-10-12T05:46:40.245250Z","iopub.status.idle":"2023-10-12T05:46:41.063415Z","shell.execute_reply.started":"2023-10-12T05:46:40.245225Z","shell.execute_reply":"2023-10-12T05:46:41.062335Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"IResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (prelu): PReLU(num_parameters=64)\n  (layer1): Sequential(\n    (0): IBasicBlock(\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=64)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): IBasicBlock(\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=64)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): IBasicBlock(\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=128)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): IBasicBlock(\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=128)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): IBasicBlock(\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=256)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): IBasicBlock(\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=256)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): IBasicBlock(\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=512)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): IBasicBlock(\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (prelu): PReLU(num_parameters=512)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout): Dropout(p=0, inplace=True)\n  (fc): Linear(in_features=100352, out_features=512, bias=True)\n  (features): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"Hashing Layer","metadata":{}},{"cell_type":"code","source":"import numpy as np\n \n\ndef logistic(x, k=1, x0=0):\n    \"\"\"\n    Logistic function: L / (1 + exp(-k * (x - x0)))\n    \n    Parameters:\n    - x: Input values\n    - k: Steepness of the curve (default: 1)\n    - x0: x-value of the sigmoid's midpoint (default: 0)\n    \n    Returns:\n    - Result of the logistic function\n    \"\"\"\n    return 1 / (1 + np.exp(-k * (x - x0)))\n\n\ndef new_dropout(x, level=0.5):  \n    if level < 0. or level >= 1:#level是概率值，必须在0~1之间  \n        raise Exception('Dropout level must be in interval [0, 1].')  \n    retain_prob = 1. - level  \n    #我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样  \n    #硬币 正面的概率为p，n表示每个神经元试验的次数  \n    #因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。 \n     \n   # sample=nn.random.binomial(n=1,p=retain_prob,size=x.shape)#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了  \n    sample = torch.from_numpy(np.array(logistic(0.3, 100))).to(device)\n\n#\n  #  sample=np.array(logistic(0.36,x.dim()))#\n#     print (sample)  \n#     print(type(sample))\n    \n    x *=sample          #屏蔽某些神经元\n    \n    x /= retain_prob   #此处是dropout\n    return x\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.064758Z","iopub.execute_input":"2023-10-12T05:46:41.065677Z","iopub.status.idle":"2023-10-12T05:46:41.074281Z","shell.execute_reply.started":"2023-10-12T05:46:41.065640Z","shell.execute_reply":"2023-10-12T05:46:41.073191Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class hash(Function):\n    # 静态方法\n    @staticmethod\n    def forward(ctx, input):\n        # ctx.save_for_backward(input)\n        return torch.sign(input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \n        return grad_output\n\ndef hash_layer(input):\n    return hash.apply(input)\nclass GreedyHash(nn.Module):\n    def __init__(self):      \n        super(GreedyHash, self).__init__()      \n        self.wres = iresnet18(pretrained=False,num_features=num_classes).to(device)\n        self.fc_plus = nn.Linear(num_classes, encode_length)\n        self.fc = nn.Linear(encode_length, num_classes, bias=False)\n    def get_hash_length(self):\n        return self.encode_length\n    def forward(self, x):\n        x = self.wres(x)\n        x = new_dropout(x)\n        x = self.fc_plus(x)       \n        code = hash_layer(x)      \n        output = self.fc(code)        \n        return output, x, code\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.078220Z","iopub.execute_input":"2023-10-12T05:46:41.078922Z","iopub.status.idle":"2023-10-12T05:46:41.093601Z","shell.execute_reply.started":"2023-10-12T05:46:41.078894Z","shell.execute_reply":"2023-10-12T05:46:41.092827Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\nnet = GreedyHash().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.095251Z","iopub.execute_input":"2023-10-12T05:46:41.096067Z","iopub.status.idle":"2023-10-12T05:46:41.438347Z","shell.execute_reply.started":"2023-10-12T05:46:41.096029Z","shell.execute_reply":"2023-10-12T05:46:41.437305Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class ArcFace(nn.Module):\n    def __init__(self, s=64.0, m=0.5):\n        super(ArcFace, self).__init__()\n        self.s = s\n        self.m = m\n\n    def forward(self, cosine: torch.Tensor, label):\n        index = torch.where(label != -1)[0]\n        m_hot = torch.zeros(index.size()[0], cosine.size()[1], device=cosine.device)\n        m_hot.scatter_(1, label[index, None], self.m)\n        cosine.acos_()\n        cosine[index] += m_hot\n        cosine.cos_().mul_(self.s)\n        return cosine\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.439901Z","iopub.execute_input":"2023-10-12T05:46:41.440535Z","iopub.status.idle":"2023-10-12T05:46:41.446336Z","shell.execute_reply.started":"2023-10-12T05:46:41.440503Z","shell.execute_reply":"2023-10-12T05:46:41.445547Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\nclass FocalLoss(nn.Module):\n\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n        self.ce = torch.nn.CrossEntropyLoss()\n\n    def forward(self, input, target):\n        logp = self.ce(input, target)\n        p = torch.exp(-logp)\n        loss = (1 - p) ** self.gamma * logp\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.447576Z","iopub.execute_input":"2023-10-12T05:46:41.447847Z","iopub.status.idle":"2023-10-12T05:46:41.462218Z","shell.execute_reply.started":"2023-10-12T05:46:41.447824Z","shell.execute_reply":"2023-10-12T05:46:41.461065Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.463745Z","iopub.execute_input":"2023-10-12T05:46:41.464488Z","iopub.status.idle":"2023-10-12T05:46:41.476770Z","shell.execute_reply.started":"2023-10-12T05:46:41.464451Z","shell.execute_reply":"2023-10-12T05:46:41.475885Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def compress(train, test, model, classes=100):\n    retrievalB = list([])\n    retrievalL = list([])\n    with torch.no_grad():\n        for batch_step, (data, target) in enumerate(train):         \n            var_data = data.to(device)            \n            _,_, code = model(var_data)        \n            retrievalB.extend(code.cpu().data.numpy())\n            retrievalL.extend(target)\n         \n        queryB=list([])\n        queryL=list([])\n        for batch_step, (data, target) in enumerate(test):     \n            var_data = data.to(device)\n            _,_, code = model(var_data)\n            \n            queryB.extend(code.cpu().data.numpy())\n            queryL.extend(target)\n            \n        retrievalB=np.array(retrievalB)\n        retrievalL=np.eye(classes)[np.array(retrievalL)]\n            \n        queryB=np.array(queryB)\n        queryL=np.eye(classes)[np.array(queryL)]   \n        \n            \n        return retrievalB, retrievalL, queryB, queryL\n\n\n    \n    \ndef calculate_hamming(B1, B2):\n    \"\"\"\n    :param B1:  vector [n]\n    :param B2:  vector [r*n]\n    :return: hamming distance [r]\n    \"\"\"\n    q = B2.shape[1] # max inner product value\n    distH = 0.5 * (q - np.dot(B1, B2.transpose()))\n    return distH\n\n\ndef calculate_map(qB, rB, queryL, retrievalL):\n    \"\"\"\n       :param qB: {-1,+1}^{mxq} query bits\n       :param rB: {-1,+1}^{nxq} retrieval bits\n       :param queryL: {0,1}^{mxl} query label\n       :param retrievalL: {0,1}^{nxl} retrieval label\n       :return:\n    \"\"\"\n    num_query = queryL.shape[0]\n    map = 0\n    for iter in range(num_query):\n       \n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        \n        tsum = np.sum(gnd).astype(int)\n        if tsum == 0:\n            continue\n        \n        hamm = calculate_hamming(qB[iter, :], rB)\n        ind = np.argsort(hamm)\n        gnd = gnd[ind]\n\n        count = np.linspace(1, tsum, tsum) # [1,2, tsum]\n        tindex = np.asarray(np.where(gnd == 1)) + 1.0\n        map_ = np.mean(count / (tindex))\n       \n        map = map + map_\n    map = map / num_query\n    return map\n\n\n\ndef calculate_top_map(qB, rB, queryL, retrievalL, topk):\n    \"\"\"\n    :param qB: {-1,+1}^{mxq} query bits\n    :param rB: {-1,+1}^{nxq} retrieval bits\n    :param queryL: {0,1}^{mxl} query label\n    :param retrievalL: {0,1}^{nxl} retrieval label\n    :param topk:\n    :return:\n    \"\"\"\n    num_query = queryL.shape[0]\n    topkmap = 0\n    for iter in range(num_query):\n        \n       \n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        hamm = calculate_hamming(qB[iter, :], rB)\n        ind = np.argsort(hamm)\n        gnd = gnd[ind]\n\n        tgnd = gnd[0:topk]\n        tsum = np.sum(tgnd).astype(int)\n        if tsum == 0:\n            continue\n        count = np.linspace(1, tsum, tsum)\n\n        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n        topkmap_ = np.mean(count / (tindex))\n      \n        topkmap = topkmap + topkmap_\n    topkmap = topkmap / num_query\n    return topkmap\n\n\n\ndef myCalcTopMap(rB, qB, retrievalL, queryL, topk):\n    \n    \n    num_query = queryL.shape[0]\n    \n    topkmap = 0\n    \n    \n    for iter in tqdm(range(num_query)):\n        \n        \n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        \n        \n        hamm = CalcHammingDist(qB[iter, :], rB)\n        \n        \n        ind = np.argsort(hamm)\n        \n       \n        gnd = gnd[ind]\n        \n        \n        tgnd = gnd[0:topk]\n        \n       \n        tsum = np.sum(tgnd).astype(int)\n        if tsum == 0:\n            continue\n        \n       \n        count = np.linspace(1, tsum, tsum)\n        \n        \n        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n        \n        \n        topkmap_ = np.mean(count / (tindex))\n        \n        topkmap = topkmap + topkmap_\n   \n    topkmap = topkmap / num_query\n    \n    return topkmap","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.478179Z","iopub.execute_input":"2023-10-12T05:46:41.478908Z","iopub.status.idle":"2023-10-12T05:46:41.496548Z","shell.execute_reply.started":"2023-10-12T05:46:41.478878Z","shell.execute_reply":"2023-10-12T05:46:41.495505Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#TEST\n# resnet","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.497844Z","iopub.execute_input":"2023-10-12T05:46:41.498937Z","iopub.status.idle":"2023-10-12T05:46:41.513349Z","shell.execute_reply.started":"2023-10-12T05:46:41.498906Z","shell.execute_reply":"2023-10-12T05:46:41.512477Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# net","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.514749Z","iopub.execute_input":"2023-10-12T05:46:41.515152Z","iopub.status.idle":"2023-10-12T05:46:41.526856Z","shell.execute_reply.started":"2023-10-12T05:46:41.515095Z","shell.execute_reply":"2023-10-12T05:46:41.525781Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# 训练过程\n\nbest = 0.0\n\ntorch.set_num_threads(num_threads)\n\nif torch.cuda.device_count() > 1:\n    net = nn.DataParallel(net)    \nnet.to(device)\n# Train the Model\nfor epoch in range(num_epochs):  \n    net.train()\n\n    \n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs, feature, _ = net(images)\n\n\n        loss1 = criterion(outputs, labels)\n  \n        loss2 = torch.mean(torch.abs(torch.pow(torch.abs(feature) - torch.ones(feature.size()).to(device), 3)))\n        loss = loss1 + 0.1 * loss2\n        loss.backward()\n        optimizer.step()\n\n#         if (i + 1) % (len(oxford102TrainDataset) // batch_size / 2) == 0:\n        print ('Epoch [%d/%d], Iter [%d/%d] Loss1: %.4f Loss2: %.4f'\n               % (epoch + 1, num_epochs, i + 1, len(sessiondatabaseDataset) // train_batch_size,\n                  loss1.item(), loss2.item()))\n\n   \n    net.eval()  \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n           \n            outputs, _, _ = net(images)\n          \n            _, predicted = torch.max(outputs.cpu().data, 1)\n\n            total += labels.size(0)\n\n            correct += (predicted == labels).sum()\n\n    print('Test Accuracy of the model: %.2f %%' % (100.0 * correct / total))\n\n    if 1.0 * correct / total > best:\n        best = 1.0 * correct / total\n        torch.save(net.state_dict(), 'temp44.pkl')    \n    print('best: %.2f %%' % (best * 100.0))\n    net.eval()\n    retrievalB, retrievalL, queryB, queryL = compress(train_loader, val_loader, net)\n    print('---calculate map1---')\n    result1 = calculate_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL)\n    #torch.save(net.state_dict(), 'map1_{}_module_all_obj.pkl'.format(round(result1,3)))\n    print(result1)\n    retrievalB, retrievalL, queryB, queryL = compress(database_loader, val_loader, net)\n    print('---calculate map2---')\n    result2 = calculate_map(qB=queryB, rB=retrievalB, queryL=queryL, retrievalL=retrievalL)\n    print(result2)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-12T05:46:41.528497Z","iopub.execute_input":"2023-10-12T05:46:41.529164Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [1/100], Iter [1/18] Loss1: 4.6536 Loss2: 0.3364\nEpoch [1/100], Iter [2/18] Loss1: 4.8666 Loss2: 0.4167\nEpoch [1/100], Iter [3/18] Loss1: 4.5895 Loss2: 0.3801\nEpoch [1/100], Iter [4/18] Loss1: 4.3597 Loss2: 0.4041\nEpoch [1/100], Iter [5/18] Loss1: 4.0144 Loss2: 0.5041\nEpoch [1/100], Iter [6/18] Loss1: 4.4218 Loss2: 0.4224\nEpoch [1/100], Iter [7/18] Loss1: 4.5003 Loss2: 0.4443\nEpoch [1/100], Iter [8/18] Loss1: 4.3988 Loss2: 0.4843\nEpoch [1/100], Iter [9/18] Loss1: 4.2672 Loss2: 0.3736\nEpoch [1/100], Iter [10/18] Loss1: 4.1235 Loss2: 0.4155\nEpoch [1/100], Iter [11/18] Loss1: 4.2033 Loss2: 0.3858\nEpoch [1/100], Iter [12/18] Loss1: 4.1936 Loss2: 0.3505\nEpoch [1/100], Iter [13/18] Loss1: 4.2194 Loss2: 0.4665\nEpoch [1/100], Iter [14/18] Loss1: 4.0284 Loss2: 0.3838\nEpoch [1/100], Iter [15/18] Loss1: 3.8886 Loss2: 0.3464\nTest Accuracy of the model: 3.33 %\nbest: 3.33 %\n---calculate map1---\n0.1174570714922674\n---calculate map2---\n0.4147603402755616\nEpoch [2/100], Iter [1/18] Loss1: 4.0468 Loss2: 0.4608\nEpoch [2/100], Iter [2/18] Loss1: 3.9558 Loss2: 0.3702\nEpoch [2/100], Iter [3/18] Loss1: 3.9179 Loss2: 0.3835\nEpoch [2/100], Iter [4/18] Loss1: 3.8343 Loss2: 0.3743\nEpoch [2/100], Iter [5/18] Loss1: 3.7721 Loss2: 0.3974\nEpoch [2/100], Iter [6/18] Loss1: 3.9503 Loss2: 0.3578\nEpoch [2/100], Iter [7/18] Loss1: 3.7250 Loss2: 0.3945\nEpoch [2/100], Iter [8/18] Loss1: 3.7484 Loss2: 0.4817\nEpoch [2/100], Iter [9/18] Loss1: 3.7619 Loss2: 0.4696\nEpoch [2/100], Iter [10/18] Loss1: 3.7199 Loss2: 0.4722\nEpoch [2/100], Iter [11/18] Loss1: 3.8787 Loss2: 0.3708\nEpoch [2/100], Iter [12/18] Loss1: 3.6705 Loss2: 0.4278\nEpoch [2/100], Iter [13/18] Loss1: 3.6237 Loss2: 0.3471\nEpoch [2/100], Iter [14/18] Loss1: 4.0498 Loss2: 0.4505\nEpoch [2/100], Iter [15/18] Loss1: 3.5699 Loss2: 0.4096\nTest Accuracy of the model: 10.83 %\nbest: 10.83 %\n---calculate map1---\n0.1273311451518975\n---calculate map2---\n0.4204876921174\nEpoch [3/100], Iter [1/18] Loss1: 3.4800 Loss2: 0.3888\nEpoch [3/100], Iter [2/18] Loss1: 3.2997 Loss2: 0.4298\nEpoch [3/100], Iter [3/18] Loss1: 3.5109 Loss2: 0.5084\nEpoch [3/100], Iter [4/18] Loss1: 3.5178 Loss2: 0.3613\nEpoch [3/100], Iter [5/18] Loss1: 3.4466 Loss2: 0.5168\nEpoch [3/100], Iter [6/18] Loss1: 3.7962 Loss2: 0.3581\nEpoch [3/100], Iter [7/18] Loss1: 3.5095 Loss2: 0.4990\nEpoch [3/100], Iter [8/18] Loss1: 3.6224 Loss2: 0.4220\nEpoch [3/100], Iter [9/18] Loss1: 3.4291 Loss2: 0.3962\nEpoch [3/100], Iter [10/18] Loss1: 3.1775 Loss2: 0.3762\nEpoch [3/100], Iter [11/18] Loss1: 3.3736 Loss2: 0.3969\nEpoch [3/100], Iter [12/18] Loss1: 3.1389 Loss2: 0.3455\nEpoch [3/100], Iter [13/18] Loss1: 3.3716 Loss2: 0.4504\nEpoch [3/100], Iter [14/18] Loss1: 3.5025 Loss2: 0.3685\nEpoch [3/100], Iter [15/18] Loss1: 3.1796 Loss2: 0.3602\nTest Accuracy of the model: 15.83 %\nbest: 15.83 %\n---calculate map1---\n0.13682201000784314\n---calculate map2---\n0.4482643342896495\nEpoch [4/100], Iter [1/18] Loss1: 3.1197 Loss2: 0.3556\nEpoch [4/100], Iter [2/18] Loss1: 3.1269 Loss2: 0.4020\nEpoch [4/100], Iter [3/18] Loss1: 3.5392 Loss2: 0.3769\nEpoch [4/100], Iter [4/18] Loss1: 3.1215 Loss2: 0.3177\nEpoch [4/100], Iter [5/18] Loss1: 3.3696 Loss2: 0.3262\nEpoch [4/100], Iter [6/18] Loss1: 3.4575 Loss2: 0.4051\nEpoch [4/100], Iter [7/18] Loss1: 3.2870 Loss2: 0.3085\nEpoch [4/100], Iter [8/18] Loss1: 3.3338 Loss2: 0.3878\nEpoch [4/100], Iter [9/18] Loss1: 3.2332 Loss2: 0.3713\nEpoch [4/100], Iter [10/18] Loss1: 3.2943 Loss2: 0.4313\nEpoch [4/100], Iter [11/18] Loss1: 3.5271 Loss2: 0.3446\nEpoch [4/100], Iter [12/18] Loss1: 3.5675 Loss2: 0.4270\nEpoch [4/100], Iter [13/18] Loss1: 3.0567 Loss2: 0.3880\nEpoch [4/100], Iter [14/18] Loss1: 3.4637 Loss2: 0.4365\nEpoch [4/100], Iter [15/18] Loss1: 3.3073 Loss2: 0.3525\nTest Accuracy of the model: 16.67 %\nbest: 16.67 %\n---calculate map1---\n0.14355241602981117\n---calculate map2---\n0.4516246074360108\nEpoch [5/100], Iter [1/18] Loss1: 3.2397 Loss2: 0.4310\nEpoch [5/100], Iter [2/18] Loss1: 3.3898 Loss2: 0.3219\nEpoch [5/100], Iter [3/18] Loss1: 3.2671 Loss2: 0.5370\nEpoch [5/100], Iter [4/18] Loss1: 3.0651 Loss2: 0.3096\nEpoch [5/100], Iter [5/18] Loss1: 3.4931 Loss2: 0.3826\nEpoch [5/100], Iter [6/18] Loss1: 3.0957 Loss2: 0.3385\nEpoch [5/100], Iter [7/18] Loss1: 3.4836 Loss2: 0.4934\nEpoch [5/100], Iter [8/18] Loss1: 3.3694 Loss2: 0.3566\nEpoch [5/100], Iter [9/18] Loss1: 2.9801 Loss2: 0.3956\nEpoch [5/100], Iter [10/18] Loss1: 3.2789 Loss2: 0.4082\nEpoch [5/100], Iter [11/18] Loss1: 3.1469 Loss2: 0.3887\nEpoch [5/100], Iter [12/18] Loss1: 3.1474 Loss2: 0.4829\nEpoch [5/100], Iter [13/18] Loss1: 3.0317 Loss2: 0.3541\nEpoch [5/100], Iter [14/18] Loss1: 2.9433 Loss2: 0.4104\nEpoch [5/100], Iter [15/18] Loss1: 3.2800 Loss2: 0.3336\nTest Accuracy of the model: 9.17 %\nbest: 16.67 %\n---calculate map1---\n0.13712758633188352\n---calculate map2---\n0.47661678751543735\nEpoch [6/100], Iter [1/18] Loss1: 2.8542 Loss2: 0.3419\nEpoch [6/100], Iter [2/18] Loss1: 3.0227 Loss2: 0.3727\nEpoch [6/100], Iter [3/18] Loss1: 3.1278 Loss2: 0.4448\nEpoch [6/100], Iter [4/18] Loss1: 3.1830 Loss2: 0.3507\nEpoch [6/100], Iter [5/18] Loss1: 3.1037 Loss2: 0.3218\nEpoch [6/100], Iter [6/18] Loss1: 2.9221 Loss2: 0.3265\nEpoch [6/100], Iter [7/18] Loss1: 3.3805 Loss2: 0.3291\nEpoch [6/100], Iter [8/18] Loss1: 2.7917 Loss2: 0.3402\nEpoch [6/100], Iter [9/18] Loss1: 2.8920 Loss2: 0.3610\nEpoch [6/100], Iter [10/18] Loss1: 3.3185 Loss2: 0.3216\nEpoch [6/100], Iter [11/18] Loss1: 2.9293 Loss2: 0.4173\nEpoch [6/100], Iter [12/18] Loss1: 3.0680 Loss2: 0.3851\nEpoch [6/100], Iter [13/18] Loss1: 2.7074 Loss2: 0.3494\nEpoch [6/100], Iter [14/18] Loss1: 2.9570 Loss2: 0.3728\nEpoch [6/100], Iter [15/18] Loss1: 2.8602 Loss2: 0.3876\nTest Accuracy of the model: 13.33 %\nbest: 16.67 %\n---calculate map1---\n0.1523791072861837\n---calculate map2---\n0.4718681102911173\nEpoch [7/100], Iter [1/18] Loss1: 2.6101 Loss2: 0.4481\nEpoch [7/100], Iter [2/18] Loss1: 3.1529 Loss2: 0.3647\nEpoch [7/100], Iter [3/18] Loss1: 3.1152 Loss2: 0.3519\nEpoch [7/100], Iter [4/18] Loss1: 2.8439 Loss2: 0.3592\nEpoch [7/100], Iter [5/18] Loss1: 3.1484 Loss2: 0.3561\nEpoch [7/100], Iter [6/18] Loss1: 2.7708 Loss2: 0.3516\nEpoch [7/100], Iter [7/18] Loss1: 3.1601 Loss2: 0.4563\nEpoch [7/100], Iter [8/18] Loss1: 2.8636 Loss2: 0.3111\nEpoch [7/100], Iter [9/18] Loss1: 3.5944 Loss2: 0.3276\nEpoch [7/100], Iter [10/18] Loss1: 2.9426 Loss2: 0.3254\nEpoch [7/100], Iter [11/18] Loss1: 2.5897 Loss2: 0.3122\nEpoch [7/100], Iter [12/18] Loss1: 3.2315 Loss2: 0.3775\nEpoch [7/100], Iter [13/18] Loss1: 3.1174 Loss2: 0.3481\nEpoch [7/100], Iter [14/18] Loss1: 3.9426 Loss2: 0.4103\nEpoch [7/100], Iter [15/18] Loss1: 3.3996 Loss2: 0.4347\nTest Accuracy of the model: 5.83 %\nbest: 16.67 %\n---calculate map1---\n0.15158624694615136\n---calculate map2---\n0.4602265445617031\nEpoch [8/100], Iter [1/18] Loss1: 2.6508 Loss2: 0.3882\nEpoch [8/100], Iter [2/18] Loss1: 2.9951 Loss2: 0.3706\nEpoch [8/100], Iter [3/18] Loss1: 3.1632 Loss2: 0.6686\nEpoch [8/100], Iter [4/18] Loss1: 2.5532 Loss2: 0.4080\nEpoch [8/100], Iter [5/18] Loss1: 2.7599 Loss2: 0.3161\nEpoch [8/100], Iter [6/18] Loss1: 2.7239 Loss2: 0.4341\nEpoch [8/100], Iter [7/18] Loss1: 2.7248 Loss2: 0.4364\nEpoch [8/100], Iter [8/18] Loss1: 2.7777 Loss2: 0.3402\nEpoch [8/100], Iter [9/18] Loss1: 3.2883 Loss2: 0.4176\nEpoch [8/100], Iter [10/18] Loss1: 3.2377 Loss2: 0.4328\nEpoch [8/100], Iter [11/18] Loss1: 3.1725 Loss2: 0.3711\nEpoch [8/100], Iter [12/18] Loss1: 2.9890 Loss2: 0.3224\nEpoch [8/100], Iter [13/18] Loss1: 3.1209 Loss2: 0.3258\nEpoch [8/100], Iter [14/18] Loss1: 2.8495 Loss2: 0.3086\nEpoch [8/100], Iter [15/18] Loss1: 3.2675 Loss2: 0.2944\nTest Accuracy of the model: 20.83 %\nbest: 20.83 %\n---calculate map1---\n0.184555170456348\n---calculate map2---\n0.46524606885490355\nEpoch [9/100], Iter [1/18] Loss1: 2.9993 Loss2: 0.2890\nEpoch [9/100], Iter [2/18] Loss1: 2.8674 Loss2: 0.3886\nEpoch [9/100], Iter [3/18] Loss1: 2.7494 Loss2: 0.3252\nEpoch [9/100], Iter [4/18] Loss1: 2.9748 Loss2: 0.4150\nEpoch [9/100], Iter [5/18] Loss1: 2.6809 Loss2: 0.3146\nEpoch [9/100], Iter [6/18] Loss1: 2.7560 Loss2: 0.3353\nEpoch [9/100], Iter [7/18] Loss1: 2.4293 Loss2: 0.3705\nEpoch [9/100], Iter [8/18] Loss1: 2.6869 Loss2: 0.3852\nEpoch [9/100], Iter [9/18] Loss1: 2.8380 Loss2: 0.3489\nEpoch [9/100], Iter [10/18] Loss1: 2.8229 Loss2: 0.3479\nEpoch [9/100], Iter [11/18] Loss1: 2.4310 Loss2: 0.3538\nEpoch [9/100], Iter [12/18] Loss1: 2.9996 Loss2: 0.3004\nEpoch [9/100], Iter [13/18] Loss1: 2.7179 Loss2: 0.3208\nEpoch [9/100], Iter [14/18] Loss1: 2.6501 Loss2: 0.3727\nEpoch [9/100], Iter [15/18] Loss1: 2.9456 Loss2: 0.3344\nTest Accuracy of the model: 15.83 %\nbest: 20.83 %\n---calculate map1---\n0.19679146602631972\n---calculate map2---\n0.5571871506925619\nEpoch [10/100], Iter [1/18] Loss1: 2.4612 Loss2: 0.4074\nEpoch [10/100], Iter [2/18] Loss1: 2.9927 Loss2: 0.2959\nEpoch [10/100], Iter [3/18] Loss1: 3.0825 Loss2: 0.3613\nEpoch [10/100], Iter [4/18] Loss1: 2.2932 Loss2: 0.2939\nEpoch [10/100], Iter [5/18] Loss1: 2.6802 Loss2: 0.3225\nEpoch [10/100], Iter [6/18] Loss1: 2.6162 Loss2: 0.3756\nEpoch [10/100], Iter [7/18] Loss1: 2.1747 Loss2: 0.3838\nEpoch [10/100], Iter [8/18] Loss1: 2.3380 Loss2: 0.3609\nEpoch [10/100], Iter [9/18] Loss1: 2.5648 Loss2: 0.3572\nEpoch [10/100], Iter [10/18] Loss1: 2.8039 Loss2: 0.3518\nEpoch [10/100], Iter [11/18] Loss1: 2.5142 Loss2: 0.4752\nEpoch [10/100], Iter [12/18] Loss1: 2.8016 Loss2: 0.2989\nEpoch [10/100], Iter [13/18] Loss1: 2.6429 Loss2: 0.3496\nEpoch [10/100], Iter [14/18] Loss1: 2.5850 Loss2: 0.3877\nEpoch [10/100], Iter [15/18] Loss1: 2.9335 Loss2: 0.3311\nTest Accuracy of the model: 13.33 %\nbest: 20.83 %\n---calculate map1---\n0.19409122450572341\n---calculate map2---\n0.5373028073787752\nEpoch [11/100], Iter [1/18] Loss1: 2.5778 Loss2: 0.3874\nEpoch [11/100], Iter [2/18] Loss1: 2.9406 Loss2: 0.3356\nEpoch [11/100], Iter [3/18] Loss1: 2.6377 Loss2: 0.2976\nEpoch [11/100], Iter [4/18] Loss1: 2.6680 Loss2: 0.2792\nEpoch [11/100], Iter [5/18] Loss1: 3.0115 Loss2: 0.3172\nEpoch [11/100], Iter [6/18] Loss1: 2.5901 Loss2: 0.2937\nEpoch [11/100], Iter [7/18] Loss1: 2.4834 Loss2: 0.3128\nEpoch [11/100], Iter [8/18] Loss1: 2.5859 Loss2: 0.3453\nEpoch [11/100], Iter [9/18] Loss1: 2.2567 Loss2: 0.3568\nEpoch [11/100], Iter [10/18] Loss1: 2.5917 Loss2: 0.3741\nEpoch [11/100], Iter [11/18] Loss1: 2.3713 Loss2: 0.3767\nEpoch [11/100], Iter [12/18] Loss1: 2.4318 Loss2: 0.3226\nEpoch [11/100], Iter [13/18] Loss1: 2.5687 Loss2: 0.2955\nEpoch [11/100], Iter [14/18] Loss1: 2.7115 Loss2: 0.3794\nEpoch [11/100], Iter [15/18] Loss1: 3.0057 Loss2: 0.4410\nTest Accuracy of the model: 21.67 %\nbest: 21.67 %\n---calculate map1---\n0.20380687370031955\n---calculate map2---\n0.5306543054222393\nEpoch [12/100], Iter [1/18] Loss1: 2.1370 Loss2: 0.3737\nEpoch [12/100], Iter [2/18] Loss1: 2.9510 Loss2: 0.4036\nEpoch [12/100], Iter [3/18] Loss1: 2.5817 Loss2: 0.4022\nEpoch [12/100], Iter [4/18] Loss1: 2.2242 Loss2: 0.3800\nEpoch [12/100], Iter [5/18] Loss1: 2.4606 Loss2: 0.3937\nEpoch [12/100], Iter [6/18] Loss1: 2.2637 Loss2: 0.3261\nEpoch [12/100], Iter [7/18] Loss1: 2.6892 Loss2: 0.4180\nEpoch [12/100], Iter [8/18] Loss1: 2.6382 Loss2: 0.4354\nEpoch [12/100], Iter [9/18] Loss1: 3.0677 Loss2: 0.3294\nEpoch [12/100], Iter [10/18] Loss1: 2.8541 Loss2: 0.4032\nEpoch [12/100], Iter [11/18] Loss1: 2.7536 Loss2: 0.3670\nEpoch [12/100], Iter [12/18] Loss1: 2.4054 Loss2: 0.3742\nEpoch [12/100], Iter [13/18] Loss1: 2.8148 Loss2: 0.3604\nEpoch [12/100], Iter [14/18] Loss1: 2.3217 Loss2: 0.4240\nEpoch [12/100], Iter [15/18] Loss1: 2.6419 Loss2: 0.3754\nTest Accuracy of the model: 28.33 %\nbest: 28.33 %\n---calculate map1---\n0.20005543504817924\n---calculate map2---\n0.5728494430696291\nEpoch [13/100], Iter [1/18] Loss1: 2.1929 Loss2: 0.3456\nEpoch [13/100], Iter [2/18] Loss1: 2.5509 Loss2: 0.3247\nEpoch [13/100], Iter [3/18] Loss1: 2.2915 Loss2: 0.3456\nEpoch [13/100], Iter [4/18] Loss1: 2.4153 Loss2: 0.3125\nEpoch [13/100], Iter [5/18] Loss1: 2.7919 Loss2: 0.3363\nEpoch [13/100], Iter [6/18] Loss1: 2.8620 Loss2: 0.2988\nEpoch [13/100], Iter [7/18] Loss1: 2.6275 Loss2: 0.3324\nEpoch [13/100], Iter [8/18] Loss1: 2.3590 Loss2: 0.3617\nEpoch [13/100], Iter [9/18] Loss1: 2.1593 Loss2: 0.3534\nEpoch [13/100], Iter [10/18] Loss1: 2.5170 Loss2: 0.3048\nEpoch [13/100], Iter [11/18] Loss1: 2.6335 Loss2: 0.3707\nEpoch [13/100], Iter [12/18] Loss1: 2.2499 Loss2: 0.3309\nEpoch [13/100], Iter [13/18] Loss1: 2.2409 Loss2: 0.3332\nEpoch [13/100], Iter [14/18] Loss1: 2.8080 Loss2: 0.3756\nEpoch [13/100], Iter [15/18] Loss1: 2.4822 Loss2: 0.3144\nTest Accuracy of the model: 48.33 %\nbest: 48.33 %\n---calculate map1---\n0.2773604013135088\n---calculate map2---\n0.6394569919119495\nEpoch [14/100], Iter [1/18] Loss1: 2.2962 Loss2: 0.3220\nEpoch [14/100], Iter [2/18] Loss1: 2.0997 Loss2: 0.3145\nEpoch [14/100], Iter [3/18] Loss1: 2.1235 Loss2: 0.4392\nEpoch [14/100], Iter [4/18] Loss1: 2.3544 Loss2: 0.3037\nEpoch [14/100], Iter [5/18] Loss1: 2.2732 Loss2: 0.3639\nEpoch [14/100], Iter [6/18] Loss1: 2.5744 Loss2: 0.3707\nEpoch [14/100], Iter [7/18] Loss1: 2.3045 Loss2: 0.3654\nEpoch [14/100], Iter [8/18] Loss1: 2.2478 Loss2: 0.4010\nEpoch [14/100], Iter [9/18] Loss1: 2.3984 Loss2: 0.3535\nEpoch [14/100], Iter [10/18] Loss1: 2.1657 Loss2: 0.3101\nEpoch [14/100], Iter [11/18] Loss1: 2.6858 Loss2: 0.3199\nEpoch [14/100], Iter [12/18] Loss1: 2.5930 Loss2: 0.3405\nEpoch [14/100], Iter [13/18] Loss1: 2.1989 Loss2: 0.3360\nEpoch [14/100], Iter [14/18] Loss1: 2.3601 Loss2: 0.3620\nEpoch [14/100], Iter [15/18] Loss1: 2.6383 Loss2: 0.3352\nTest Accuracy of the model: 5.00 %\nbest: 48.33 %\n---calculate map1---\n0.15603948777010107\n---calculate map2---\n0.45382713095901683\nEpoch [15/100], Iter [1/18] Loss1: 3.0561 Loss2: 0.3715\nEpoch [15/100], Iter [2/18] Loss1: 2.4396 Loss2: 0.3074\nEpoch [15/100], Iter [3/18] Loss1: 2.1923 Loss2: 0.3019\nEpoch [15/100], Iter [4/18] Loss1: 2.3350 Loss2: 0.3046\nEpoch [15/100], Iter [5/18] Loss1: 3.0051 Loss2: 0.3316\nEpoch [15/100], Iter [6/18] Loss1: 2.1050 Loss2: 0.3515\nEpoch [15/100], Iter [7/18] Loss1: 1.9408 Loss2: 0.3436\nEpoch [15/100], Iter [8/18] Loss1: 2.5444 Loss2: 0.2916\nEpoch [15/100], Iter [9/18] Loss1: 2.7664 Loss2: 0.3776\nEpoch [15/100], Iter [10/18] Loss1: 2.9349 Loss2: 0.3741\nEpoch [15/100], Iter [11/18] Loss1: 2.1937 Loss2: 0.3903\nEpoch [15/100], Iter [12/18] Loss1: 2.6816 Loss2: 0.3233\nEpoch [15/100], Iter [13/18] Loss1: 2.7624 Loss2: 0.3428\nEpoch [15/100], Iter [14/18] Loss1: 2.3217 Loss2: 0.4068\nEpoch [15/100], Iter [15/18] Loss1: 2.3414 Loss2: 0.3029\nTest Accuracy of the model: 28.33 %\nbest: 48.33 %\n---calculate map1---\n0.2182257226681295\n---calculate map2---\n0.6133254342473247\nEpoch [16/100], Iter [1/18] Loss1: 2.2230 Loss2: 0.3906\nEpoch [16/100], Iter [2/18] Loss1: 2.2551 Loss2: 0.4501\nEpoch [16/100], Iter [3/18] Loss1: 2.0771 Loss2: 0.3409\nEpoch [16/100], Iter [4/18] Loss1: 1.9849 Loss2: 0.3333\nEpoch [16/100], Iter [5/18] Loss1: 2.3689 Loss2: 0.3403\nEpoch [16/100], Iter [6/18] Loss1: 1.9988 Loss2: 0.3534\nEpoch [16/100], Iter [7/18] Loss1: 1.9176 Loss2: 0.3302\nEpoch [16/100], Iter [8/18] Loss1: 2.2948 Loss2: 0.3288\nEpoch [16/100], Iter [9/18] Loss1: 2.1144 Loss2: 0.3466\nEpoch [16/100], Iter [10/18] Loss1: 2.3813 Loss2: 0.3797\nEpoch [16/100], Iter [11/18] Loss1: 2.2041 Loss2: 0.3704\nEpoch [16/100], Iter [12/18] Loss1: 2.5659 Loss2: 0.4381\nEpoch [16/100], Iter [13/18] Loss1: 2.5800 Loss2: 0.3235\nEpoch [16/100], Iter [14/18] Loss1: 2.2512 Loss2: 0.3139\nEpoch [16/100], Iter [15/18] Loss1: 2.5204 Loss2: 0.3878\nTest Accuracy of the model: 20.00 %\nbest: 48.33 %\n---calculate map1---\n0.21024258184984906\n---calculate map2---\n0.5627791982034769\nEpoch [17/100], Iter [1/18] Loss1: 2.1076 Loss2: 0.3380\nEpoch [17/100], Iter [2/18] Loss1: 2.2147 Loss2: 0.3326\nEpoch [17/100], Iter [3/18] Loss1: 2.7793 Loss2: 0.3701\nEpoch [17/100], Iter [4/18] Loss1: 2.6047 Loss2: 0.3981\nEpoch [17/100], Iter [5/18] Loss1: 2.0652 Loss2: 0.3552\nEpoch [17/100], Iter [6/18] Loss1: 2.1231 Loss2: 0.4037\nEpoch [17/100], Iter [7/18] Loss1: 2.1458 Loss2: 0.3749\nEpoch [17/100], Iter [8/18] Loss1: 2.3223 Loss2: 0.4802\nEpoch [17/100], Iter [9/18] Loss1: 2.0384 Loss2: 0.3334\nEpoch [17/100], Iter [10/18] Loss1: 2.4131 Loss2: 0.3572\nEpoch [17/100], Iter [11/18] Loss1: 2.3991 Loss2: 0.3330\nEpoch [17/100], Iter [12/18] Loss1: 2.3079 Loss2: 0.3488\nEpoch [17/100], Iter [13/18] Loss1: 2.5385 Loss2: 0.4333\nEpoch [17/100], Iter [14/18] Loss1: 2.7759 Loss2: 0.3557\nEpoch [17/100], Iter [15/18] Loss1: 2.1429 Loss2: 0.3350\nTest Accuracy of the model: 24.17 %\nbest: 48.33 %\n---calculate map1---\n0.21233106908522434\n---calculate map2---\n0.6190340316099776\nEpoch [18/100], Iter [1/18] Loss1: 2.2661 Loss2: 0.4112\nEpoch [18/100], Iter [2/18] Loss1: 2.7451 Loss2: 0.4420\nEpoch [18/100], Iter [3/18] Loss1: 2.3890 Loss2: 0.3824\nEpoch [18/100], Iter [4/18] Loss1: 2.2254 Loss2: 0.3323\nEpoch [18/100], Iter [5/18] Loss1: 1.9780 Loss2: 0.3909\nEpoch [18/100], Iter [6/18] Loss1: 2.2814 Loss2: 0.4019\nEpoch [18/100], Iter [7/18] Loss1: 2.9568 Loss2: 0.3709\nEpoch [18/100], Iter [8/18] Loss1: 2.4426 Loss2: 0.3267\nEpoch [18/100], Iter [9/18] Loss1: 1.9426 Loss2: 0.3587\nEpoch [18/100], Iter [10/18] Loss1: 2.1730 Loss2: 0.4390\nEpoch [18/100], Iter [11/18] Loss1: 2.6669 Loss2: 0.3667\nEpoch [18/100], Iter [12/18] Loss1: 2.3508 Loss2: 0.3664\nEpoch [18/100], Iter [13/18] Loss1: 2.1486 Loss2: 0.3526\nEpoch [18/100], Iter [14/18] Loss1: 2.0755 Loss2: 0.4391\nEpoch [18/100], Iter [15/18] Loss1: 2.1389 Loss2: 0.3609\nTest Accuracy of the model: 21.67 %\nbest: 48.33 %\n---calculate map1---\n0.2289630169546603\n---calculate map2---\n0.5987990837650444\nEpoch [19/100], Iter [1/18] Loss1: 2.1699 Loss2: 0.3938\nEpoch [19/100], Iter [2/18] Loss1: 2.2241 Loss2: 0.3295\nEpoch [19/100], Iter [3/18] Loss1: 2.3216 Loss2: 0.3514\nEpoch [19/100], Iter [4/18] Loss1: 2.6021 Loss2: 0.3921\nEpoch [19/100], Iter [5/18] Loss1: 3.0365 Loss2: 0.3897\nEpoch [19/100], Iter [6/18] Loss1: 1.7281 Loss2: 0.3894\nEpoch [19/100], Iter [7/18] Loss1: 2.4852 Loss2: 0.3805\nEpoch [19/100], Iter [8/18] Loss1: 2.8870 Loss2: 0.4008\nEpoch [19/100], Iter [9/18] Loss1: 2.0578 Loss2: 0.4158\nEpoch [19/100], Iter [10/18] Loss1: 1.8788 Loss2: 0.5317\nEpoch [19/100], Iter [11/18] Loss1: 2.2242 Loss2: 0.4006\nEpoch [19/100], Iter [12/18] Loss1: 2.0012 Loss2: 0.4126\nEpoch [19/100], Iter [13/18] Loss1: 1.8973 Loss2: 0.3749\nEpoch [19/100], Iter [14/18] Loss1: 2.2115 Loss2: 0.4777\nEpoch [19/100], Iter [15/18] Loss1: 1.5028 Loss2: 0.3362\nTest Accuracy of the model: 23.33 %\nbest: 48.33 %\n---calculate map1---\n0.22112256345133235\n---calculate map2---\n0.5745170979968446\nEpoch [20/100], Iter [1/18] Loss1: 2.1443 Loss2: 0.3812\nEpoch [20/100], Iter [2/18] Loss1: 2.3764 Loss2: 0.4016\nEpoch [20/100], Iter [3/18] Loss1: 2.1957 Loss2: 0.3960\nEpoch [20/100], Iter [4/18] Loss1: 2.4203 Loss2: 0.3470\nEpoch [20/100], Iter [5/18] Loss1: 2.0192 Loss2: 0.3455\nEpoch [20/100], Iter [6/18] Loss1: 2.1704 Loss2: 0.3538\nEpoch [20/100], Iter [7/18] Loss1: 2.4854 Loss2: 0.3521\nEpoch [20/100], Iter [8/18] Loss1: 3.1581 Loss2: 0.3885\nEpoch [20/100], Iter [9/18] Loss1: 2.3858 Loss2: 0.4065\nEpoch [20/100], Iter [10/18] Loss1: 2.0096 Loss2: 0.4104\nEpoch [20/100], Iter [11/18] Loss1: 1.9965 Loss2: 0.3523\nEpoch [20/100], Iter [12/18] Loss1: 2.3542 Loss2: 0.4063\nEpoch [20/100], Iter [13/18] Loss1: 2.2257 Loss2: 0.4271\nEpoch [20/100], Iter [14/18] Loss1: 1.9744 Loss2: 0.3560\nEpoch [20/100], Iter [15/18] Loss1: 2.2667 Loss2: 0.3328\nTest Accuracy of the model: 70.00 %\nbest: 70.00 %\n---calculate map1---\n0.3097913990318719\n---calculate map2---\n0.7021659851813032\nEpoch [21/100], Iter [1/18] Loss1: 2.2993 Loss2: 0.3614\nEpoch [21/100], Iter [2/18] Loss1: 1.7683 Loss2: 0.4050\nEpoch [21/100], Iter [3/18] Loss1: 1.7302 Loss2: 0.3688\nEpoch [21/100], Iter [4/18] Loss1: 2.2304 Loss2: 0.4192\nEpoch [21/100], Iter [5/18] Loss1: 1.7792 Loss2: 0.3785\nEpoch [21/100], Iter [6/18] Loss1: 2.3172 Loss2: 0.4318\nEpoch [21/100], Iter [7/18] Loss1: 1.9858 Loss2: 0.3243\nEpoch [21/100], Iter [8/18] Loss1: 1.6213 Loss2: 0.3573\nEpoch [21/100], Iter [9/18] Loss1: 2.0355 Loss2: 0.3773\nEpoch [21/100], Iter [10/18] Loss1: 2.5482 Loss2: 0.3663\nEpoch [21/100], Iter [11/18] Loss1: 2.0254 Loss2: 0.3979\nEpoch [21/100], Iter [12/18] Loss1: 2.6252 Loss2: 0.3632\nEpoch [21/100], Iter [13/18] Loss1: 2.1836 Loss2: 0.3321\nEpoch [21/100], Iter [14/18] Loss1: 2.1456 Loss2: 0.4199\nEpoch [21/100], Iter [15/18] Loss1: 2.7787 Loss2: 0.3758\nTest Accuracy of the model: 76.67 %\nbest: 76.67 %\n---calculate map1---\n0.3553395991140268\n---calculate map2---\n0.7581395519525365\nEpoch [22/100], Iter [1/18] Loss1: 2.0833 Loss2: 0.4380\nEpoch [22/100], Iter [2/18] Loss1: 2.3212 Loss2: 0.4093\nEpoch [22/100], Iter [3/18] Loss1: 2.3119 Loss2: 0.4038\nEpoch [22/100], Iter [4/18] Loss1: 2.3121 Loss2: 0.3853\nEpoch [22/100], Iter [5/18] Loss1: 2.0278 Loss2: 0.4057\nEpoch [22/100], Iter [6/18] Loss1: 2.1348 Loss2: 0.3489\nEpoch [22/100], Iter [7/18] Loss1: 1.8231 Loss2: 0.3588\nEpoch [22/100], Iter [8/18] Loss1: 2.4244 Loss2: 0.3881\nEpoch [22/100], Iter [9/18] Loss1: 2.5380 Loss2: 0.4195\nEpoch [22/100], Iter [10/18] Loss1: 2.2484 Loss2: 0.3682\nEpoch [22/100], Iter [11/18] Loss1: 2.9546 Loss2: 0.4787\nEpoch [22/100], Iter [12/18] Loss1: 2.4099 Loss2: 0.5591\nEpoch [22/100], Iter [13/18] Loss1: 2.1723 Loss2: 0.4273\nEpoch [22/100], Iter [14/18] Loss1: 1.7545 Loss2: 0.3838\nEpoch [22/100], Iter [15/18] Loss1: 2.5496 Loss2: 0.3911\nTest Accuracy of the model: 46.67 %\nbest: 76.67 %\n---calculate map1---\n0.3124250940043628\n---calculate map2---\n0.7300621794490066\nEpoch [23/100], Iter [1/18] Loss1: 2.2037 Loss2: 0.3539\nEpoch [23/100], Iter [2/18] Loss1: 2.3418 Loss2: 0.4375\nEpoch [23/100], Iter [3/18] Loss1: 2.1649 Loss2: 0.3845\nEpoch [23/100], Iter [4/18] Loss1: 1.5996 Loss2: 0.3876\nEpoch [23/100], Iter [5/18] Loss1: 2.2944 Loss2: 0.4355\nEpoch [23/100], Iter [6/18] Loss1: 1.8542 Loss2: 0.4329\nEpoch [23/100], Iter [7/18] Loss1: 1.7515 Loss2: 0.3715\nEpoch [23/100], Iter [8/18] Loss1: 2.1547 Loss2: 0.3999\nEpoch [23/100], Iter [9/18] Loss1: 1.6885 Loss2: 0.4460\nEpoch [23/100], Iter [10/18] Loss1: 2.2722 Loss2: 0.3851\nEpoch [23/100], Iter [11/18] Loss1: 1.6276 Loss2: 0.3722\nEpoch [23/100], Iter [12/18] Loss1: 2.3074 Loss2: 0.3782\nEpoch [23/100], Iter [13/18] Loss1: 1.9774 Loss2: 0.4004\nEpoch [23/100], Iter [14/18] Loss1: 2.2471 Loss2: 0.3717\nEpoch [23/100], Iter [15/18] Loss1: 1.9844 Loss2: 0.4138\nTest Accuracy of the model: 35.00 %\nbest: 76.67 %\n---calculate map1---\n0.2310881958436478\n---calculate map2---\n0.6365659933569121\nEpoch [24/100], Iter [1/18] Loss1: 2.1217 Loss2: 0.3654\nEpoch [24/100], Iter [2/18] Loss1: 1.9804 Loss2: 0.3286\nEpoch [24/100], Iter [3/18] Loss1: 2.1162 Loss2: 0.4097\nEpoch [24/100], Iter [4/18] Loss1: 2.3349 Loss2: 0.4634\nEpoch [24/100], Iter [5/18] Loss1: 1.9432 Loss2: 0.4123\nEpoch [24/100], Iter [6/18] Loss1: 2.3480 Loss2: 0.4085\nEpoch [24/100], Iter [7/18] Loss1: 2.3898 Loss2: 0.3982\nEpoch [24/100], Iter [8/18] Loss1: 2.1624 Loss2: 0.4792\nEpoch [24/100], Iter [9/18] Loss1: 2.7519 Loss2: 0.4804\nEpoch [24/100], Iter [10/18] Loss1: 2.4683 Loss2: 0.4857\nEpoch [24/100], Iter [11/18] Loss1: 2.2747 Loss2: 0.4238\nEpoch [24/100], Iter [12/18] Loss1: 1.7061 Loss2: 0.3553\nEpoch [24/100], Iter [13/18] Loss1: 1.7629 Loss2: 0.3785\nEpoch [24/100], Iter [14/18] Loss1: 1.8458 Loss2: 0.3856\nEpoch [24/100], Iter [15/18] Loss1: 1.3683 Loss2: 0.4157\nTest Accuracy of the model: 76.67 %\nbest: 76.67 %\n---calculate map1---\n0.3548166672001314\n---calculate map2---\n0.8295123273686827\nEpoch [25/100], Iter [1/18] Loss1: 2.0255 Loss2: 0.3628\nEpoch [25/100], Iter [2/18] Loss1: 1.8213 Loss2: 0.4275\nEpoch [25/100], Iter [3/18] Loss1: 1.8424 Loss2: 0.4536\nEpoch [25/100], Iter [4/18] Loss1: 2.5846 Loss2: 0.4081\nEpoch [25/100], Iter [5/18] Loss1: 2.0082 Loss2: 0.4078\nEpoch [25/100], Iter [6/18] Loss1: 2.1518 Loss2: 0.4383\nEpoch [25/100], Iter [7/18] Loss1: 2.0769 Loss2: 0.4272\nEpoch [25/100], Iter [8/18] Loss1: 1.7818 Loss2: 0.4086\nEpoch [25/100], Iter [9/18] Loss1: 2.0261 Loss2: 0.4472\nEpoch [25/100], Iter [10/18] Loss1: 1.7322 Loss2: 0.5588\nEpoch [25/100], Iter [11/18] Loss1: 2.0938 Loss2: 0.3816\nEpoch [25/100], Iter [12/18] Loss1: 2.4486 Loss2: 0.3910\nEpoch [25/100], Iter [13/18] Loss1: 1.6933 Loss2: 0.3456\nEpoch [25/100], Iter [14/18] Loss1: 1.9946 Loss2: 0.4193\nEpoch [25/100], Iter [15/18] Loss1: 2.3700 Loss2: 0.4120\nTest Accuracy of the model: 70.00 %\nbest: 76.67 %\n---calculate map1---\n0.4063986930395816\n---calculate map2---\n0.8274421856429018\nEpoch [26/100], Iter [1/18] Loss1: 1.9324 Loss2: 0.3842\nEpoch [26/100], Iter [2/18] Loss1: 2.5205 Loss2: 0.4142\nEpoch [26/100], Iter [3/18] Loss1: 1.7408 Loss2: 0.4201\nEpoch [26/100], Iter [4/18] Loss1: 1.7446 Loss2: 0.3709\nEpoch [26/100], Iter [5/18] Loss1: 2.0109 Loss2: 0.4379\nEpoch [26/100], Iter [6/18] Loss1: 2.1171 Loss2: 0.3643\nEpoch [26/100], Iter [7/18] Loss1: 2.3157 Loss2: 0.3865\nEpoch [26/100], Iter [8/18] Loss1: 2.6751 Loss2: 0.4233\nEpoch [26/100], Iter [9/18] Loss1: 2.0161 Loss2: 0.3934\nEpoch [26/100], Iter [10/18] Loss1: 2.1190 Loss2: 0.3912\nEpoch [26/100], Iter [11/18] Loss1: 1.2897 Loss2: 0.4024\nEpoch [26/100], Iter [12/18] Loss1: 2.6370 Loss2: 0.3917\nEpoch [26/100], Iter [13/18] Loss1: 1.7990 Loss2: 0.4460\nEpoch [26/100], Iter [14/18] Loss1: 2.5315 Loss2: 0.5573\nEpoch [26/100], Iter [15/18] Loss1: 2.1166 Loss2: 0.4506\nTest Accuracy of the model: 57.50 %\nbest: 76.67 %\n---calculate map1---\n0.32424478083664077\n---calculate map2---\n0.7481771551926868\nEpoch [27/100], Iter [1/18] Loss1: 2.3086 Loss2: 0.4901\nEpoch [27/100], Iter [2/18] Loss1: 1.9655 Loss2: 0.4614\nEpoch [27/100], Iter [3/18] Loss1: 2.4372 Loss2: 0.4071\nEpoch [27/100], Iter [4/18] Loss1: 1.8870 Loss2: 0.4083\nEpoch [27/100], Iter [5/18] Loss1: 1.8247 Loss2: 0.4520\nEpoch [27/100], Iter [6/18] Loss1: 1.9888 Loss2: 0.4188\nEpoch [27/100], Iter [7/18] Loss1: 2.0740 Loss2: 0.3578\nEpoch [27/100], Iter [8/18] Loss1: 1.6680 Loss2: 0.4077\nEpoch [27/100], Iter [9/18] Loss1: 1.7859 Loss2: 0.3432\nEpoch [27/100], Iter [10/18] Loss1: 2.1053 Loss2: 0.3750\nEpoch [27/100], Iter [11/18] Loss1: 1.7578 Loss2: 0.3949\nEpoch [27/100], Iter [12/18] Loss1: 1.6103 Loss2: 0.3859\nEpoch [27/100], Iter [13/18] Loss1: 2.0377 Loss2: 0.3943\nEpoch [27/100], Iter [14/18] Loss1: 1.5449 Loss2: 0.4285\nEpoch [27/100], Iter [15/18] Loss1: 1.9228 Loss2: 0.4333\nTest Accuracy of the model: 72.50 %\nbest: 76.67 %\n---calculate map1---\n0.41818255576896646\n---calculate map2---\n0.8542461503704044\nEpoch [28/100], Iter [1/18] Loss1: 1.3073 Loss2: 0.4134\nEpoch [28/100], Iter [2/18] Loss1: 1.7504 Loss2: 0.3826\nEpoch [28/100], Iter [3/18] Loss1: 2.3335 Loss2: 0.4109\nEpoch [28/100], Iter [4/18] Loss1: 1.8578 Loss2: 0.4310\nEpoch [28/100], Iter [5/18] Loss1: 1.9818 Loss2: 0.3529\nEpoch [28/100], Iter [6/18] Loss1: 2.0075 Loss2: 0.4250\nEpoch [28/100], Iter [7/18] Loss1: 2.3727 Loss2: 0.4490\nEpoch [28/100], Iter [8/18] Loss1: 1.9955 Loss2: 0.4885\nEpoch [28/100], Iter [9/18] Loss1: 2.2642 Loss2: 0.4615\nEpoch [28/100], Iter [10/18] Loss1: 2.2557 Loss2: 0.3790\nEpoch [28/100], Iter [11/18] Loss1: 2.1209 Loss2: 0.3802\nEpoch [28/100], Iter [12/18] Loss1: 2.0788 Loss2: 0.3923\nEpoch [28/100], Iter [13/18] Loss1: 2.2029 Loss2: 0.3759\nEpoch [28/100], Iter [14/18] Loss1: 1.9158 Loss2: 0.3938\nEpoch [28/100], Iter [15/18] Loss1: 2.1819 Loss2: 0.4036\nTest Accuracy of the model: 51.67 %\nbest: 76.67 %\n---calculate map1---\n0.30792642187188046\n---calculate map2---\n0.7741842360935546\nEpoch [29/100], Iter [1/18] Loss1: 1.4070 Loss2: 0.4023\nEpoch [29/100], Iter [2/18] Loss1: 2.1525 Loss2: 0.3652\nEpoch [29/100], Iter [3/18] Loss1: 1.8080 Loss2: 0.4104\nEpoch [29/100], Iter [4/18] Loss1: 2.6107 Loss2: 0.3602\nEpoch [29/100], Iter [5/18] Loss1: 1.6376 Loss2: 0.4023\nEpoch [29/100], Iter [6/18] Loss1: 1.7464 Loss2: 0.3714\nEpoch [29/100], Iter [7/18] Loss1: 2.2717 Loss2: 0.4248\nEpoch [29/100], Iter [8/18] Loss1: 1.7376 Loss2: 0.3623\nEpoch [29/100], Iter [9/18] Loss1: 1.7804 Loss2: 0.4451\nEpoch [29/100], Iter [10/18] Loss1: 2.4595 Loss2: 0.3368\nEpoch [29/100], Iter [11/18] Loss1: 1.5372 Loss2: 0.3940\nEpoch [29/100], Iter [12/18] Loss1: 2.0605 Loss2: 0.3716\nEpoch [29/100], Iter [13/18] Loss1: 1.8557 Loss2: 0.4143\nEpoch [29/100], Iter [14/18] Loss1: 1.7641 Loss2: 0.6146\nEpoch [29/100], Iter [15/18] Loss1: 2.0623 Loss2: 0.3979\nTest Accuracy of the model: 69.17 %\nbest: 76.67 %\n---calculate map1---\n0.3925904830281974\n---calculate map2---\n0.8123061299919553\nEpoch [30/100], Iter [1/18] Loss1: 2.8198 Loss2: 0.3846\nEpoch [30/100], Iter [2/18] Loss1: 2.4261 Loss2: 0.4704\nEpoch [30/100], Iter [3/18] Loss1: 1.9168 Loss2: 0.3836\nEpoch [30/100], Iter [4/18] Loss1: 1.8880 Loss2: 0.4910\nEpoch [30/100], Iter [5/18] Loss1: 2.0375 Loss2: 0.4242\nEpoch [30/100], Iter [6/18] Loss1: 2.0658 Loss2: 0.4388\nEpoch [30/100], Iter [7/18] Loss1: 2.4186 Loss2: 0.5267\nEpoch [30/100], Iter [8/18] Loss1: 1.9285 Loss2: 0.4715\nEpoch [30/100], Iter [9/18] Loss1: 2.3304 Loss2: 0.3717\nEpoch [30/100], Iter [10/18] Loss1: 2.2281 Loss2: 0.3929\nEpoch [30/100], Iter [11/18] Loss1: 2.3959 Loss2: 0.4233\nEpoch [30/100], Iter [12/18] Loss1: 1.6941 Loss2: 0.4489\nEpoch [30/100], Iter [13/18] Loss1: 2.6445 Loss2: 0.4384\nEpoch [30/100], Iter [14/18] Loss1: 1.7602 Loss2: 0.5170\nEpoch [30/100], Iter [15/18] Loss1: 2.3540 Loss2: 0.4143\nTest Accuracy of the model: 40.83 %\nbest: 76.67 %\n---calculate map1---\n0.2507229162186251\n---calculate map2---\n0.6880111710435709\nEpoch [31/100], Iter [1/18] Loss1: 1.9811 Loss2: 0.3813\nEpoch [31/100], Iter [2/18] Loss1: 2.2540 Loss2: 0.3980\nEpoch [31/100], Iter [3/18] Loss1: 1.6135 Loss2: 0.4239\nEpoch [31/100], Iter [4/18] Loss1: 2.0607 Loss2: 0.4280\nEpoch [31/100], Iter [5/18] Loss1: 1.6260 Loss2: 0.3804\nEpoch [31/100], Iter [6/18] Loss1: 1.9801 Loss2: 0.4126\nEpoch [31/100], Iter [7/18] Loss1: 2.0103 Loss2: 0.4101\nEpoch [31/100], Iter [8/18] Loss1: 1.9511 Loss2: 0.4170\nEpoch [31/100], Iter [9/18] Loss1: 1.6135 Loss2: 0.4539\nEpoch [31/100], Iter [10/18] Loss1: 1.7869 Loss2: 0.4149\nEpoch [31/100], Iter [11/18] Loss1: 1.5009 Loss2: 0.4341\nEpoch [31/100], Iter [12/18] Loss1: 1.9832 Loss2: 0.4936\nEpoch [31/100], Iter [13/18] Loss1: 1.9197 Loss2: 0.3819\nEpoch [31/100], Iter [14/18] Loss1: 2.0538 Loss2: 0.4175\nEpoch [31/100], Iter [15/18] Loss1: 1.8975 Loss2: 0.5701\nTest Accuracy of the model: 38.33 %\nbest: 76.67 %\n---calculate map1---\n0.23461463453771486\n---calculate map2---\n0.6575959717744679\nEpoch [32/100], Iter [1/18] Loss1: 1.9923 Loss2: 0.3925\nEpoch [32/100], Iter [2/18] Loss1: 1.6507 Loss2: 0.4201\nEpoch [32/100], Iter [3/18] Loss1: 1.6466 Loss2: 0.4277\nEpoch [32/100], Iter [4/18] Loss1: 1.7418 Loss2: 0.4346\nEpoch [32/100], Iter [5/18] Loss1: 1.7669 Loss2: 0.3998\nEpoch [32/100], Iter [6/18] Loss1: 1.8706 Loss2: 0.4309\nEpoch [32/100], Iter [7/18] Loss1: 1.7150 Loss2: 0.4695\nEpoch [32/100], Iter [8/18] Loss1: 2.2876 Loss2: 0.4868\nEpoch [32/100], Iter [9/18] Loss1: 1.5798 Loss2: 0.3793\nEpoch [32/100], Iter [10/18] Loss1: 1.5389 Loss2: 0.4190\nEpoch [32/100], Iter [11/18] Loss1: 1.7623 Loss2: 0.3620\nEpoch [32/100], Iter [12/18] Loss1: 2.4079 Loss2: 0.3969\nEpoch [32/100], Iter [13/18] Loss1: 1.9443 Loss2: 0.4353\nEpoch [32/100], Iter [14/18] Loss1: 2.7492 Loss2: 0.4106\nEpoch [32/100], Iter [15/18] Loss1: 2.0202 Loss2: 0.4255\nTest Accuracy of the model: 28.33 %\nbest: 76.67 %\n---calculate map1---\n0.2627023145359668\n---calculate map2---\n0.7020488749102786\nEpoch [33/100], Iter [1/18] Loss1: 2.0092 Loss2: 0.3937\nEpoch [33/100], Iter [2/18] Loss1: 2.6988 Loss2: 0.3899\nEpoch [33/100], Iter [3/18] Loss1: 2.0970 Loss2: 0.4507\nEpoch [33/100], Iter [4/18] Loss1: 1.7120 Loss2: 0.3596\nEpoch [33/100], Iter [5/18] Loss1: 1.6917 Loss2: 0.4289\nEpoch [33/100], Iter [6/18] Loss1: 1.4335 Loss2: 0.4317\nEpoch [33/100], Iter [7/18] Loss1: 1.5360 Loss2: 0.3927\nEpoch [33/100], Iter [8/18] Loss1: 1.5459 Loss2: 0.4755\nEpoch [33/100], Iter [9/18] Loss1: 1.5890 Loss2: 0.3875\nEpoch [33/100], Iter [10/18] Loss1: 1.6424 Loss2: 0.4593\nEpoch [33/100], Iter [11/18] Loss1: 1.5922 Loss2: 0.4176\nEpoch [33/100], Iter [12/18] Loss1: 1.7502 Loss2: 0.4079\nEpoch [33/100], Iter [13/18] Loss1: 1.9930 Loss2: 0.4742\nEpoch [33/100], Iter [14/18] Loss1: 1.5745 Loss2: 0.3913\nEpoch [33/100], Iter [15/18] Loss1: 2.0026 Loss2: 0.4480\nTest Accuracy of the model: 39.17 %\nbest: 76.67 %\n---calculate map1---\n0.27783514437030166\n---calculate map2---\n0.6948477616268872\nEpoch [34/100], Iter [1/18] Loss1: 1.8584 Loss2: 0.4688\nEpoch [34/100], Iter [2/18] Loss1: 1.7765 Loss2: 0.3836\nEpoch [34/100], Iter [3/18] Loss1: 1.3026 Loss2: 0.4634\nEpoch [34/100], Iter [4/18] Loss1: 1.7617 Loss2: 0.3800\nEpoch [34/100], Iter [5/18] Loss1: 1.9731 Loss2: 0.4641\nEpoch [34/100], Iter [6/18] Loss1: 1.9819 Loss2: 0.4313\nEpoch [34/100], Iter [7/18] Loss1: 1.1351 Loss2: 0.4313\nEpoch [34/100], Iter [8/18] Loss1: 1.5188 Loss2: 0.4482\nEpoch [34/100], Iter [9/18] Loss1: 1.9200 Loss2: 0.3947\nEpoch [34/100], Iter [10/18] Loss1: 1.9064 Loss2: 0.4215\nEpoch [34/100], Iter [11/18] Loss1: 2.0765 Loss2: 0.3979\nEpoch [34/100], Iter [12/18] Loss1: 1.3691 Loss2: 0.5136\nEpoch [34/100], Iter [13/18] Loss1: 2.1040 Loss2: 0.4034\nEpoch [34/100], Iter [14/18] Loss1: 1.4926 Loss2: 0.4028\nEpoch [34/100], Iter [15/18] Loss1: 2.0691 Loss2: 0.5600\nTest Accuracy of the model: 80.83 %\nbest: 80.83 %\n---calculate map1---\n0.38965376947737407\n---calculate map2---\n0.845009155931305\nEpoch [35/100], Iter [1/18] Loss1: 2.2569 Loss2: 0.4350\nEpoch [35/100], Iter [2/18] Loss1: 1.7884 Loss2: 0.3812\nEpoch [35/100], Iter [3/18] Loss1: 2.2042 Loss2: 0.3873\nEpoch [35/100], Iter [4/18] Loss1: 1.2428 Loss2: 0.3818\nEpoch [35/100], Iter [5/18] Loss1: 2.1530 Loss2: 0.4731\nEpoch [35/100], Iter [6/18] Loss1: 1.6161 Loss2: 0.4901\nEpoch [35/100], Iter [7/18] Loss1: 1.8748 Loss2: 0.4741\nEpoch [35/100], Iter [8/18] Loss1: 1.9498 Loss2: 0.4234\nEpoch [35/100], Iter [9/18] Loss1: 1.6661 Loss2: 0.4584\nEpoch [35/100], Iter [10/18] Loss1: 1.2662 Loss2: 0.3903\nEpoch [35/100], Iter [11/18] Loss1: 2.3107 Loss2: 0.4351\nEpoch [35/100], Iter [12/18] Loss1: 1.7485 Loss2: 0.4768\nEpoch [35/100], Iter [13/18] Loss1: 1.8432 Loss2: 0.4686\nEpoch [35/100], Iter [14/18] Loss1: 1.7632 Loss2: 0.3994\nEpoch [35/100], Iter [15/18] Loss1: 1.8510 Loss2: 0.4629\nTest Accuracy of the model: 35.83 %\nbest: 80.83 %\n---calculate map1---\n0.30051472428473314\n---calculate map2---\n0.7625663193284108\nEpoch [36/100], Iter [1/18] Loss1: 1.5464 Loss2: 0.4744\nEpoch [36/100], Iter [2/18] Loss1: 1.3646 Loss2: 0.4546\nEpoch [36/100], Iter [3/18] Loss1: 1.8092 Loss2: 0.4680\nEpoch [36/100], Iter [4/18] Loss1: 1.7331 Loss2: 0.4507\nEpoch [36/100], Iter [5/18] Loss1: 1.9763 Loss2: 0.4382\nEpoch [36/100], Iter [6/18] Loss1: 1.8862 Loss2: 0.3958\nEpoch [36/100], Iter [7/18] Loss1: 1.5924 Loss2: 0.5029\nEpoch [36/100], Iter [8/18] Loss1: 1.0651 Loss2: 0.4971\nEpoch [36/100], Iter [9/18] Loss1: 1.7211 Loss2: 0.4550\nEpoch [36/100], Iter [10/18] Loss1: 2.0598 Loss2: 0.3879\nEpoch [36/100], Iter [11/18] Loss1: 2.6710 Loss2: 0.4876\nEpoch [36/100], Iter [12/18] Loss1: 1.9458 Loss2: 0.5374\nEpoch [36/100], Iter [13/18] Loss1: 1.8777 Loss2: 0.4713\nEpoch [36/100], Iter [14/18] Loss1: 2.2766 Loss2: 0.4970\nEpoch [36/100], Iter [15/18] Loss1: 1.8658 Loss2: 0.4275\nTest Accuracy of the model: 30.00 %\nbest: 80.83 %\n---calculate map1---\n0.23284410034427483\n---calculate map2---\n0.6614725549642891\nEpoch [37/100], Iter [1/18] Loss1: 1.5293 Loss2: 0.4470\nEpoch [37/100], Iter [2/18] Loss1: 1.2927 Loss2: 0.4063\nEpoch [37/100], Iter [3/18] Loss1: 2.0689 Loss2: 0.4208\nEpoch [37/100], Iter [4/18] Loss1: 2.1461 Loss2: 0.4016\nEpoch [37/100], Iter [5/18] Loss1: 1.9607 Loss2: 0.4640\nEpoch [37/100], Iter [6/18] Loss1: 2.0471 Loss2: 0.4355\nEpoch [37/100], Iter [7/18] Loss1: 1.1320 Loss2: 0.4504\nEpoch [37/100], Iter [8/18] Loss1: 1.6368 Loss2: 0.4483\nEpoch [37/100], Iter [9/18] Loss1: 1.7248 Loss2: 0.4245\nEpoch [37/100], Iter [10/18] Loss1: 1.9496 Loss2: 0.4525\nEpoch [37/100], Iter [11/18] Loss1: 1.9578 Loss2: 0.5169\nEpoch [37/100], Iter [12/18] Loss1: 1.2868 Loss2: 0.4367\nEpoch [37/100], Iter [13/18] Loss1: 1.8034 Loss2: 0.4288\nEpoch [37/100], Iter [14/18] Loss1: 1.6321 Loss2: 0.5263\nEpoch [37/100], Iter [15/18] Loss1: 1.4121 Loss2: 0.4896\nTest Accuracy of the model: 71.67 %\nbest: 80.83 %\n---calculate map1---\n0.3643816104017491\n---calculate map2---\n0.7965103480985124\nEpoch [38/100], Iter [1/18] Loss1: 1.6213 Loss2: 0.4293\nEpoch [38/100], Iter [2/18] Loss1: 1.5148 Loss2: 0.4472\nEpoch [38/100], Iter [3/18] Loss1: 1.9899 Loss2: 0.4878\nEpoch [38/100], Iter [4/18] Loss1: 2.1788 Loss2: 0.4594\nEpoch [38/100], Iter [5/18] Loss1: 1.8786 Loss2: 0.4517\nEpoch [38/100], Iter [6/18] Loss1: 1.5550 Loss2: 0.5150\nEpoch [38/100], Iter [7/18] Loss1: 1.0214 Loss2: 0.4974\nEpoch [38/100], Iter [8/18] Loss1: 1.6341 Loss2: 0.4180\nEpoch [38/100], Iter [9/18] Loss1: 2.0311 Loss2: 0.4532\nEpoch [38/100], Iter [10/18] Loss1: 1.9927 Loss2: 0.3988\nEpoch [38/100], Iter [11/18] Loss1: 1.9375 Loss2: 0.4503\nEpoch [38/100], Iter [12/18] Loss1: 1.2372 Loss2: 0.3826\nEpoch [38/100], Iter [13/18] Loss1: 1.7609 Loss2: 0.4530\nEpoch [38/100], Iter [14/18] Loss1: 1.9419 Loss2: 0.4862\nEpoch [38/100], Iter [15/18] Loss1: 1.9766 Loss2: 0.4123\nTest Accuracy of the model: 85.00 %\nbest: 85.00 %\n---calculate map1---\n0.3728814693540187\n---calculate map2---\n0.8521415427658712\nEpoch [39/100], Iter [1/18] Loss1: 1.5834 Loss2: 0.5165\nEpoch [39/100], Iter [2/18] Loss1: 2.0675 Loss2: 0.5578\nEpoch [39/100], Iter [3/18] Loss1: 1.7574 Loss2: 0.4601\nEpoch [39/100], Iter [4/18] Loss1: 1.7120 Loss2: 0.4500\nEpoch [39/100], Iter [5/18] Loss1: 1.2393 Loss2: 0.4974\nEpoch [39/100], Iter [6/18] Loss1: 1.6339 Loss2: 0.4888\nEpoch [39/100], Iter [7/18] Loss1: 1.4712 Loss2: 0.3876\nEpoch [39/100], Iter [8/18] Loss1: 2.1727 Loss2: 0.4531\nEpoch [39/100], Iter [9/18] Loss1: 1.4936 Loss2: 0.4173\nEpoch [39/100], Iter [10/18] Loss1: 2.4818 Loss2: 0.5835\nEpoch [39/100], Iter [11/18] Loss1: 1.7089 Loss2: 0.5453\nEpoch [39/100], Iter [12/18] Loss1: 1.4722 Loss2: 0.4020\nEpoch [39/100], Iter [13/18] Loss1: 1.4963 Loss2: 0.4289\nEpoch [39/100], Iter [14/18] Loss1: 2.0815 Loss2: 0.4885\nEpoch [39/100], Iter [15/18] Loss1: 1.6114 Loss2: 0.4787\nTest Accuracy of the model: 72.50 %\nbest: 85.00 %\n---calculate map1---\n0.38105172126681125\n---calculate map2---\n0.8496381795915986\nEpoch [40/100], Iter [1/18] Loss1: 1.3599 Loss2: 0.5637\nEpoch [40/100], Iter [2/18] Loss1: 1.6463 Loss2: 0.4907\nEpoch [40/100], Iter [3/18] Loss1: 2.3073 Loss2: 0.5633\nEpoch [40/100], Iter [4/18] Loss1: 2.0455 Loss2: 0.4336\nEpoch [40/100], Iter [5/18] Loss1: 1.9600 Loss2: 0.4965\nEpoch [40/100], Iter [6/18] Loss1: 1.7804 Loss2: 0.4227\nEpoch [40/100], Iter [7/18] Loss1: 1.7766 Loss2: 0.4026\nEpoch [40/100], Iter [8/18] Loss1: 1.5217 Loss2: 0.4519\nEpoch [40/100], Iter [9/18] Loss1: 1.8366 Loss2: 0.3887\nEpoch [40/100], Iter [10/18] Loss1: 2.0965 Loss2: 0.4685\nEpoch [40/100], Iter [11/18] Loss1: 2.3592 Loss2: 0.4109\nEpoch [40/100], Iter [12/18] Loss1: 1.9600 Loss2: 0.4362\nEpoch [40/100], Iter [13/18] Loss1: 1.7602 Loss2: 0.3999\nEpoch [40/100], Iter [14/18] Loss1: 1.7449 Loss2: 0.3978\nEpoch [40/100], Iter [15/18] Loss1: 2.4044 Loss2: 0.5311\nTest Accuracy of the model: 24.17 %\nbest: 85.00 %\n---calculate map1---\n0.23099810024683037\n---calculate map2---\n0.7059997804927719\nEpoch [41/100], Iter [1/18] Loss1: 1.7541 Loss2: 0.4316\nEpoch [41/100], Iter [2/18] Loss1: 2.4091 Loss2: 0.4608\nEpoch [41/100], Iter [3/18] Loss1: 1.4206 Loss2: 0.3999\nEpoch [41/100], Iter [4/18] Loss1: 1.6592 Loss2: 0.4801\nEpoch [41/100], Iter [5/18] Loss1: 2.0223 Loss2: 0.5468\nEpoch [41/100], Iter [6/18] Loss1: 1.6248 Loss2: 0.3848\nEpoch [41/100], Iter [7/18] Loss1: 1.6859 Loss2: 0.4614\nEpoch [41/100], Iter [8/18] Loss1: 1.2434 Loss2: 0.4044\nEpoch [41/100], Iter [9/18] Loss1: 1.9024 Loss2: 0.3894\nEpoch [41/100], Iter [10/18] Loss1: 1.4166 Loss2: 0.4719\nEpoch [41/100], Iter [11/18] Loss1: 1.6265 Loss2: 0.4630\nEpoch [41/100], Iter [12/18] Loss1: 1.4834 Loss2: 0.4940\nEpoch [41/100], Iter [13/18] Loss1: 2.2801 Loss2: 0.4286\nEpoch [41/100], Iter [14/18] Loss1: 1.8340 Loss2: 0.4188\nEpoch [41/100], Iter [15/18] Loss1: 1.4916 Loss2: 0.3654\nTest Accuracy of the model: 30.00 %\nbest: 85.00 %\n---calculate map1---\n0.22063150911719515\n---calculate map2---\n0.6298559558086472\nEpoch [42/100], Iter [1/18] Loss1: 1.8739 Loss2: 0.4182\nEpoch [42/100], Iter [2/18] Loss1: 1.5276 Loss2: 0.4571\nEpoch [42/100], Iter [3/18] Loss1: 1.2876 Loss2: 0.4503\nEpoch [42/100], Iter [4/18] Loss1: 1.5341 Loss2: 0.4352\nEpoch [42/100], Iter [5/18] Loss1: 2.0470 Loss2: 0.6604\nEpoch [42/100], Iter [6/18] Loss1: 1.8471 Loss2: 0.4690\nEpoch [42/100], Iter [7/18] Loss1: 1.7475 Loss2: 0.4185\nEpoch [42/100], Iter [8/18] Loss1: 1.6982 Loss2: 0.5130\nEpoch [42/100], Iter [9/18] Loss1: 2.2808 Loss2: 0.4439\nEpoch [42/100], Iter [10/18] Loss1: 1.7882 Loss2: 0.4589\nEpoch [42/100], Iter [11/18] Loss1: 1.1829 Loss2: 0.4622\nEpoch [42/100], Iter [12/18] Loss1: 1.2179 Loss2: 0.4651\nEpoch [42/100], Iter [13/18] Loss1: 1.5883 Loss2: 0.5000\nEpoch [42/100], Iter [14/18] Loss1: 1.6887 Loss2: 0.5115\nEpoch [42/100], Iter [15/18] Loss1: 1.8585 Loss2: 0.4229\nTest Accuracy of the model: 69.17 %\nbest: 85.00 %\n---calculate map1---\n0.4370832767314127\n---calculate map2---\n0.8510656174229171\nEpoch [43/100], Iter [1/18] Loss1: 1.9234 Loss2: 0.3972\nEpoch [43/100], Iter [2/18] Loss1: 1.9138 Loss2: 0.4638\nEpoch [43/100], Iter [3/18] Loss1: 1.3601 Loss2: 0.4119\nEpoch [43/100], Iter [4/18] Loss1: 1.4481 Loss2: 0.3813\nEpoch [43/100], Iter [5/18] Loss1: 1.6272 Loss2: 0.4230\nEpoch [43/100], Iter [6/18] Loss1: 1.8421 Loss2: 0.4371\nEpoch [43/100], Iter [7/18] Loss1: 1.5570 Loss2: 0.4399\nEpoch [43/100], Iter [8/18] Loss1: 1.4653 Loss2: 0.4923\nEpoch [43/100], Iter [9/18] Loss1: 1.6714 Loss2: 0.4723\nEpoch [43/100], Iter [10/18] Loss1: 1.3227 Loss2: 0.4696\nEpoch [43/100], Iter [11/18] Loss1: 1.5882 Loss2: 0.4895\n","output_type":"stream"}]}]}