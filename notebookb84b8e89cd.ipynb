{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7681600,"sourceType":"datasetVersion","datasetId":4481795}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nimport torchvision\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.utils.data as util_data\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torchvision.datasets as dsets\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torchvision import models\nimport cv2\nimport time\n\nimport csv\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport shutil\nfrom typing import Any\nimport random\nimport torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-23T12:48:18.424128Z","iopub.execute_input":"2024-02-23T12:48:18.424471Z","iopub.status.idle":"2024-02-23T12:48:26.108074Z","shell.execute_reply.started":"2024-02-23T12:48:18.424445Z","shell.execute_reply":"2024-02-23T12:48:26.106978Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset_root = '/kaggle/input/ployu-data/poly_ll'\n#dataset_root = ‘../input/datagood’\n\ndataset_train_root = os.path.join(dataset_root, 'train')\ndataset_val_root = os.path.join(dataset_root, 'test/')\ndataset_all_dataset_root = os.path.join(dataset_root, 'all_dataset')\n\n\n# win设为0 \nnum_workers = 8\n\nnum_threads =8 \n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.110005Z","iopub.execute_input":"2024-02-23T12:48:26.110480Z","iopub.status.idle":"2024-02-23T12:48:26.117331Z","shell.execute_reply.started":"2024-02-23T12:48:26.110451Z","shell.execute_reply":"2024-02-23T12:48:26.116078Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"num_epochs =100\n# epoch_lr_decrease = 100\nlearning_rate = 0.004\nencode_length =256\nnum_classes =386","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.119062Z","iopub.execute_input":"2024-02-23T12:48:26.119947Z","iopub.status.idle":"2024-02-23T12:48:26.149915Z","shell.execute_reply.started":"2024-02-23T12:48:26.119905Z","shell.execute_reply":"2024-02-23T12:48:26.148780Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 配置训练集DataSet类\nclass SessionDataset(Dataset):\n    \n    # root数据集根目录\n    def __init__(self, root, transform=None):\n        # 对继承自父类的属性进行初始化\n#         super(Oxford102Dataset, self).__init__()\n        \n        #读取图像标签  [  0   0   0 ... 599 599 599]\n        self.labels = np.array([int(x.split('_')[0]) for x in os.listdir(path=root)])\n\n        # 构造图像路径\n\n        self.image_files = np.array([x.path for x in os.scandir(path=root)])\n        \n        # 数据增强\n        self.transform = transform\n        \n    \n    def __getitem__(self, index):\n        \n        # 根据Index读取图像\n        img = cv2.imread(self.image_files[index])\n        \n        #根据index读取标签\n        label = self.labels[index]\n\n\n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n    \n    \n    def __len__(self):\n        return np.size(self.image_files)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.153395Z","iopub.execute_input":"2024-02-23T12:48:26.153918Z","iopub.status.idle":"2024-02-23T12:48:26.164931Z","shell.execute_reply.started":"2024-02-23T12:48:26.153878Z","shell.execute_reply":"2024-02-23T12:48:26.163437Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_transform = {\n    \"train\": transforms.Compose([\n        transforms.ToPILImage(),#将数据类型转变为图片数据\n#         transforms.RandomResizedCrop((224, 224)),#随机裁剪224*224\n        transforms.RandomResizedCrop((128, 128)),#随机裁剪224*224\n        transforms.RandomHorizontalFlip(),#水平防线随机反转\n        transforms.ToTensor(),#转为Tensor\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626)),#标准化处理\n        \n    ]),\n    \"val\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \"all_dataset\": transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.51568358, 0.51568358, 0.51568358), (0.10332626, 0.10332626, 0.10332626))\n\n    ]),\n    \n}","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.166246Z","iopub.execute_input":"2024-02-23T12:48:26.166640Z","iopub.status.idle":"2024-02-23T12:48:26.179930Z","shell.execute_reply.started":"2024-02-23T12:48:26.166609Z","shell.execute_reply":"2024-02-23T12:48:26.178835Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_batch_size = 64#64\nval_batch_size = 64\n\n\n\n\nsession_train_dataset = SessionDataset(root=dataset_train_root,\n                                    transform=data_transform['train'])\n\ntrain_loader = DataLoader(session_train_dataset,\n                         batch_size=train_batch_size,\n                         shuffle=True,\n                         num_workers=num_workers)\n\n\nsession_val_dataset = SessionDataset(root=dataset_val_root,\n                                    transform=data_transform['val'])\nval_loader = DataLoader(session_val_dataset,\n                         batch_size=val_batch_size,\n                         shuffle=False,\n                         num_workers=num_workers)\n\n# 包含整个验证集\n\nsessiondatabaseDataset = SessionDataset(root=dataset_all_dataset_root,\n                                    transform=data_transform['all_dataset'])\n\ndatabase_loader = DataLoader(sessiondatabaseDataset,\n                         batch_size=train_batch_size,\n                         shuffle=False,\n                         num_workers=num_workers)\n\ntrain_num = len(session_train_dataset)#4800\nval_num = len(session_val_dataset)#1200\ndatasetloader_num = len(sessiondatabaseDataset)#6000\nprint(train_num, val_num,datasetloader_num)","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.181313Z","iopub.execute_input":"2024-02-23T12:48:26.181705Z","iopub.status.idle":"2024-02-23T12:48:26.475692Z","shell.execute_reply.started":"2024-02-23T12:48:26.181668Z","shell.execute_reply":"2024-02-23T12:48:26.474620Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"3088 772 3860\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\n__all__ = ['iresnet50_cbam']\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, groups=groups, bias=False, dilation=dilation)\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\nclass CBAM(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super(CBAM, self).__init__()\n        self.channel_gate = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n        self.spatial_gate = nn.Sequential(\n            nn.Conv2d(channels, channels // reduction, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channels // reduction, channels, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        # Channel-wise attention\n        channel_avg = self.channel_gate(x)\n        x = x * channel_avg\n\n        # Spatial-wise attention\n        channel_max = self.spatial_gate(x)\n        x = x * channel_max\n\n        return x\n\nclass IBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1):\n        super(IBasicBlock, self).__init__()\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        self.bn1 = nn.BatchNorm2d(inplanes, eps=1e-05,)\n        self.conv1 = conv3x3(inplanes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.prelu = nn.PReLU(planes)\n        self.conv2 = conv3x3(planes, planes, stride)\n        self.bn3 = nn.BatchNorm2d(planes, eps=1e-05,)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n        out = self.bn1(x)\n        out = self.conv1(out)\n        out = self.bn2(out)\n        out = self.prelu(out)\n        out = self.conv2(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        return out\n\nclass IResNet(nn.Module):\n    fc_scale = 7 * 7\n\n    def __init__(self, block, layers, dropout=0, num_features=512, zero_init_residual=False, groups=1, width_per_group=64, replace_stride_with_dilation=None, fp16=False):\n        super(IResNet, self).__init__()\n        self.fp16 = fp16\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes, eps=1e-05)\n        self.prelu = nn.PReLU(self.inplanes)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=2)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n        self.bn2 = nn.BatchNorm2d(512 * block.expansion, eps=1e-05,)\n        self.dropout = nn.Dropout(p=dropout, inplace=True)\n        self.cbam = CBAM(512 * block.expansion)\n        self.fc = nn.Linear(4 * 512 * block.expansion * self.fc_scale, num_features)\n        self.features = nn.BatchNorm1d(num_features, eps=1e-05)\n        nn.init.constant_(self.features.weight, 1.0)\n        self.features.weight.requires_grad = False\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, 0, 0.1)\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, IBasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion, eps=1e-05, ),\n            )\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(\n                block(self.inplanes, planes, groups=self.groups, base_width=self.base_width, dilation=self.dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        with torch.cuda.amp.autocast(self.fp16):\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.prelu(x)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n            x = self.bn2(x)\n            x = self.dropout(x)\n            x = self.cbam(x)\n            x = torch.flatten(x, 1)\n        x = self.fc(x.float() if self.fp16 else x)\n        x = self.features(x)\n        return x\n\ndef iresnet50_cbam(pretrained=False, progress=True, **kwargs):\n    return IResNet(IBasicBlock, [3, 4, 14, 3], **kwargs)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.477384Z","iopub.execute_input":"2024-02-23T12:48:26.477824Z","iopub.status.idle":"2024-02-23T12:48:26.518838Z","shell.execute_reply.started":"2024-02-23T12:48:26.477787Z","shell.execute_reply":"2024-02-23T12:48:26.517511Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n \n\n\ndef logistic(x, k=1, x0=0):\n    \"\"\"\n    Logistic function: L / (1 + exp(-k * (x - x0)))\n    \n    Parameters:\n    - x: Input values\n    - k: Steepness of the curve (default: 1)\n    - x0: x-value of the sigmoid's midpoint (default: 0)\n    \n    Returns:\n    - Result of the logistic function\n    \"\"\"\n    return 1 / (1 + np.exp(-k * (x - x0)))\n\n\ndef new_dropout(x, level=0.5):  \n    if level < 0. or level >= 1:#level是概率值，必须在0~1之间  \n        raise Exception('Dropout level must be in interval [0, 1].')  \n    retain_prob = 1. - level  \n    #我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样  \n    #硬币 正面的概率为p，n表示每个神经元试验的次数  \n    #因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。 \n     \n   # sample=nn.random.binomial(n=1,p=retain_prob,size=x.shape)#即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了  \n    sample = torch.from_numpy(np.array(logistic(0.3, 1000))).to(device)\n\n#\n  #  sample=np.array(logistic(0.36,x.dim()))#\n#     print (sample)  \n#     print(type(sample))\n    \n    x *=sample          #屏蔽某些神经元\n    \n    x /= retain_prob   #此处是dropout\n    return x\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.520341Z","iopub.execute_input":"2024-02-23T12:48:26.521158Z","iopub.status.idle":"2024-02-23T12:48:26.531850Z","shell.execute_reply.started":"2024-02-23T12:48:26.521128Z","shell.execute_reply":"2024-02-23T12:48:26.530859Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n# new layer\nclass hash(Function):\n    @staticmethod\n    def forward(ctx, input):\n        #ctx.save_for_backward(input)\n        #使用在pytroch中的sign函数得到二值哈希码\n        return torch.sign(input)    \n    @staticmethod\n    def backward(ctx, grad_output):\n        #input,  = ctx.saved_tensors\n        #grad_output = grad_output.data\n        return grad_output\ndef hash_layer(input):\n    return hash.apply(input)\n\n\n    \n    \nclass CNN_ResNet(nn.Module):\n    def __init__(self, encode_length, num_classes):\n        super(CNN_ResNet, self).__init__()\n        self.res = iresnet50_cbam(pretrained=False, num_features=1024).to(device)\n        self.res.fc = nn.Linear(in_features=32768, out_features=1024, bias=True)\n        self.fc_plus = nn.Linear(1024, 1024)\n        self.fc = nn.Linear(1024, num_classes, bias=False)\n\n    def forward(self, x):\n        \n        x = self.res(x)\n        x = new_dropout(x)\n        x = self.fc_plus(x)\n\n        code = (x)\n   \n        output = self.fc(code)\n        \n  \n        return output, x, code\n    \n\nnet = CNN_ResNet(encode_length=encode_length, num_classes=num_classes).to(device)\nprint('ok')","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:26.533173Z","iopub.execute_input":"2024-02-23T12:48:26.533482Z","iopub.status.idle":"2024-02-23T12:48:28.871379Z","shell.execute_reply.started":"2024-02-23T12:48:26.533457Z","shell.execute_reply":"2024-02-23T12:48:28.870214Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"ok\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:28.874436Z","iopub.execute_input":"2024-02-23T12:48:28.874799Z","iopub.status.idle":"2024-02-23T12:48:28.882722Z","shell.execute_reply.started":"2024-02-23T12:48:28.874769Z","shell.execute_reply":"2024-02-23T12:48:28.881561Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def compress(train, test, model, classes=386):\n    retrievalB = list([])\n    retrievalL = list([])\n    with torch.no_grad():\n        for batch_step, (data, target) in enumerate(train):\n            \n            var_data = data.to(device)\n            \n            # code为网络输出的哈希码\n            _,_, code = model(var_data)\n            \n            retrievalB.extend(code.cpu().data.numpy())\n            retrievalL.extend(target)\n            \n        queryB=list([])\n        queryL=list([])\n        for batch_step, (data, target) in enumerate(test):     \n            var_data = data.to(device)\n            _,_, code = model(var_data)\n            \n            queryB.extend(code.cpu().data.numpy())\n            queryL.extend(target)\n            \n        retrievalB=np.array(retrievalB)\n        retrievalL=np.eye(classes)[np.array(retrievalL)]\n            \n        queryB=np.array(queryB)\n        queryL=np.eye(classes)[np.array(queryL)]   \n        \n            \n        return retrievalB, retrievalL, queryB, queryL\n\n\n    \n    \ndef calculate_hamming(B1, B2):\n    \"\"\"\n    :param B1:  vector [n]\n    :param B2:  vector [r*n]\n    :return: hamming distance [r]\n    \"\"\"\n    q = B2.shape[1] # max inner product value\n    distH = 0.5 * (q - np.dot(B1, B2.transpose()))\n    return distH\n\n\ndef calculate_map(qB, rB, queryL, retrievalL):\n    \"\"\"\n       :param qB: {-1,+1}^{mxq} query bits\n       :param rB: {-1,+1}^{nxq} retrieval bits\n       :param queryL: {0,1}^{mxl} query label\n       :param retrievalL: {0,1}^{nxl} retrieval label\n       :return:\n    \"\"\"\n    num_query = queryL.shape[0]\n    map = 0\n    for iter in range(num_query):\n        # gnd : check if exists any retrieval items with same label\n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        # tsum number of items with same label\n        tsum = np.sum(gnd).astype(int)\n        if tsum == 0:\n            continue\n        # sort gnd by hamming dist\n        hamm = calculate_hamming(qB[iter, :], rB)\n        ind = np.argsort(hamm)\n        gnd = gnd[ind]\n\n        count = np.linspace(1, tsum, tsum) # [1,2, tsum]\n        tindex = np.asarray(np.where(gnd == 1)) + 1.0\n        map_ = np.mean(count / (tindex))\n        # print(map_)\n        map = map + map_\n    map = map / num_query\n    return map\n\n\n\ndef calculate_top_map(qB, rB, queryL, retrievalL, topk):\n    \"\"\"\n    :param qB: {-1,+1}^{mxq} query bits\n    :param rB: {-1,+1}^{nxq} retrieval bits\n    :param queryL: {0,1}^{mxl} query label\n    :param retrievalL: {0,1}^{nxl} retrieval label\n    :param topk:\n    :return:\n    \"\"\"\n    num_query = queryL.shape[0]\n    topkmap = 0\n    for iter in range(num_query):\n        \n        # 当前测试集标签 乘 训练集标签的转置\n        # dot是为找到与当前查询二值哈希码与库中的哈希码相似（或者完全相同）的哈希码 >0的结果为True or False\n        # .astype(np.float32)将true变为1， false变为0\n        # gnd应该是标签的相似向量1为相似，0为不相似\n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        hamm = calculate_hamming(qB[iter, :], rB)\n        ind = np.argsort(hamm)\n        gnd = gnd[ind]\n\n        tgnd = gnd[0:topk]\n        tsum = np.sum(tgnd).astype(int)\n        if tsum == 0:\n            continue\n        count = np.linspace(1, tsum, tsum)\n\n        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n        topkmap_ = np.mean(count / (tindex))\n        # print(topkmap_)\n        topkmap = topkmap + topkmap_\n    topkmap = topkmap / num_query\n    return topkmap\n\n\n# 计算mAP\n# rB整个数据集二值哈希码 qB测试集二值哈希码 retrievalL整个标签 queryL测试集标签（标签都是oneHot编码） topk\ndef myCalcTopMap(rB, qB, retrievalL, queryL, topk):\n    \n    # 测试集标签的数量\n    num_query = queryL.shape[0]\n    \n    topkmap = 0\n    \n    # 计算AP\n    for iter in tqdm(range(num_query)):\n        \n        # 当前测试集标签 乘 训练集标签的转置\n        # dot是为找到与当前查询二值哈希码与库中的哈希码相似（或者完全相同）的哈希码 >0的结果为True or False\n        # .astype(np.float32)将true变为1， false变为0\n        # gnd应该是标签的相似向量1为相似，0为不相似\n        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)\n        \n        # 计算当前测试集二值哈希码与全部数据集二值哈希码之间的汉明距离\n        hamm = CalcHammingDist(qB[iter, :], rB)\n        \n        # 取排序后的下标（升序）\n        ind = np.argsort(hamm)\n        \n        # 取按照汉明距离对gnd（标签相似度）排序\n        gnd = gnd[ind]\n        \n        # 取前topK个gnd\n        # tgnd 1,1,0,0,1..\n        tgnd = gnd[0:topk]\n        \n        # 对tgnd求和--tgnd中非1即0\n        # 表示topK中检索正确的个数\n        tsum = np.sum(tgnd).astype(int)\n        if tsum == 0:\n            continue\n        \n        # linspace(start, stop, num)--1,2,3...tsum\n        count = np.linspace(1, tsum, tsum)\n        \n        # asarray-生成ndarray数组，相比array,当输入为ndarray时，不会重新创建，更省内存\n        # tgnd [1,1,,0,0,1..]长度为topk\n        # np.where,找到数组tgnd == 1的下标\n        # tindex含义为tgnd中值为1的下标数组+1\n        tindex = np.asarray(np.where(tgnd == 1)) + 1.0\n        \n        # count [1,2,3,...topk] tindex [1,3,5...]\n        # 其含义是AP?\n        topkmap_ = np.mean(count / (tindex))\n        # AP累加\n        topkmap = topkmap + topkmap_\n    # mAP\n    topkmap = topkmap / num_query\n    \n    return topkmap","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:28.884577Z","iopub.execute_input":"2024-02-23T12:48:28.885055Z","iopub.status.idle":"2024-02-23T12:48:28.917471Z","shell.execute_reply.started":"2024-02-23T12:48:28.885017Z","shell.execute_reply":"2024-02-23T12:48:28.916145Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom scipy.io import savemat\nfrom tqdm import tqdm\n\n# Define the dataset size explicitly\ndataset_size = 3860\ncounter = 1\n\n# Define the output directory for saving .mat files\noutput_dir = \"all_dataset_features\"\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Set the model to evaluation mode\nnet.eval()\n\n# Train the Model\nacc_dict = {}\nbest = 0.0\n\ntorch.set_num_threads(num_threads)\n\n# 根据条件判断是否使用多GPU\nif torch.cuda.device_count() > 1:\n    net = nn.DataParallel(net)\n\nnet.to(device)\n\n# Train the Model\nfor epoch in range(num_epochs):\n    \n    net.train()\n#     adjust_learning_rate(optimizer, epoch)\n    \n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward + Backward + Optimize\n        optimizer.zero_grad()\n        outputs, feature, _ = net(images)\n\n        loss1 = criterion(outputs, labels)\n        loss2 = torch.mean(torch.abs(torch.pow(torch.abs(feature) - torch.ones(feature.size()).to(device), 3)))\n        loss = loss1 + 0.1 * loss2\n        loss.backward()\n        optimizer.step()\n\n        print('Epoch [%d/%d], Iter [%d/%d] Loss1: %.4f Loss2: %.4f'\n              % (epoch + 1, num_epochs, i + 1, len(sessiondatabaseDataset) // train_batch_size,\n                 loss1.item(), loss2.item()))\n\n    # Test the Model\n    net.eval()  # Change model to 'eval' mode\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        \n        for images, labels in val_loader:\n            images = images.to(device)\n            outputs, _, _ = net(images)\n            _, predicted = torch.max(outputs.cpu().data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n\n    print('Test Accuracy of the model: %.2f %%' % (100.0 * correct / total))\n\n    if 1.0 * correct / total > best:\n        best = 1.0 * correct / total\n        torch.save(net.state_dict(), 'temp44.pkl')\n    print('best: %.2f %%' % (best * 100.0))\n\n    # Save intermediate features after 100 epochs\n    if epoch == 99:  # Adjust the epoch number as needed\n        net.eval()\n        counter_before_save = counter  # Save the current counter value\n        with torch.no_grad():\n            for images, _ in tqdm(database_loader, desc=f\"Saving features at epoch {epoch+1}\"):\n                images = images.to(device)\n                _, intermediate_feature, _ = net(images)\n                \n                for j in range(len(intermediate_feature)):\n                    feature_value = intermediate_feature[j].cpu().numpy()\n\n                    # Modify the file name creation to include the counter and group index\n                    group_index = (counter - 1) // 10 + 1\n                    counter_within_group = (counter - 1) % 10 + 1\n                    file_name = os.path.join(output_dir, f\"{group_index}_{counter_within_group}.mat\")\n                    \n                    savemat(file_name, {\"feature_value\": feature_value})\n                    print(f\"Saved intermediate feature value to {file_name}\")\n\n                    counter += 1  # Increment the counter\n\n                    # Check if we've processed all expected images\n                    if counter > dataset_size:\n                        break\n\n                if counter > dataset_size:\n                    break\n\n        print(f\"All {counter - counter_before_save} intermediate features saved at epoch {epoch+1}\")\n\nprint(f\"All {dataset_size} dataset intermediate features saved sequentially.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-23T12:48:28.919535Z","iopub.execute_input":"2024-02-23T12:48:28.920016Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/100], Iter [1/60] Loss1: 6.1492 Loss2: 0.6404\nEpoch [1/100], Iter [2/60] Loss1: 5.8779 Loss2: 0.6677\nEpoch [1/100], Iter [3/60] Loss1: 6.0485 Loss2: 0.6728\nEpoch [1/100], Iter [4/60] Loss1: 6.0568 Loss2: 0.7909\nEpoch [1/100], Iter [5/60] Loss1: 5.9655 Loss2: 0.7387\nEpoch [1/100], Iter [6/60] Loss1: 5.7770 Loss2: 0.7533\nEpoch [1/100], Iter [7/60] Loss1: 5.8277 Loss2: 0.7999\nEpoch [1/100], Iter [8/60] Loss1: 6.2586 Loss2: 0.9818\nEpoch [1/100], Iter [9/60] Loss1: 6.0151 Loss2: 0.8788\nEpoch [1/100], Iter [10/60] Loss1: 5.9706 Loss2: 1.0182\nEpoch [1/100], Iter [11/60] Loss1: 5.7498 Loss2: 0.8484\nEpoch [1/100], Iter [12/60] Loss1: 6.0714 Loss2: 0.8020\nEpoch [1/100], Iter [13/60] Loss1: 6.0802 Loss2: 0.9413\nEpoch [1/100], Iter [14/60] Loss1: 6.0901 Loss2: 0.9129\nEpoch [1/100], Iter [15/60] Loss1: 5.9197 Loss2: 0.9531\nEpoch [1/100], Iter [16/60] Loss1: 5.8175 Loss2: 1.0753\nEpoch [1/100], Iter [17/60] Loss1: 5.7776 Loss2: 0.9131\nEpoch [1/100], Iter [18/60] Loss1: 5.7345 Loss2: 0.9614\nEpoch [1/100], Iter [19/60] Loss1: 5.6532 Loss2: 1.0432\nEpoch [1/100], Iter [20/60] Loss1: 5.8899 Loss2: 1.1757\nEpoch [1/100], Iter [21/60] Loss1: 5.4171 Loss2: 1.0830\nEpoch [1/100], Iter [22/60] Loss1: 5.7520 Loss2: 1.5685\nEpoch [1/100], Iter [23/60] Loss1: 5.8572 Loss2: 1.1631\nEpoch [1/100], Iter [24/60] Loss1: 5.2180 Loss2: 1.4428\nEpoch [1/100], Iter [25/60] Loss1: 5.4525 Loss2: 1.3393\nEpoch [1/100], Iter [26/60] Loss1: 5.7212 Loss2: 1.1600\nEpoch [1/100], Iter [27/60] Loss1: 5.7901 Loss2: 1.2818\nEpoch [1/100], Iter [28/60] Loss1: 5.5742 Loss2: 1.3496\nEpoch [1/100], Iter [29/60] Loss1: 5.5858 Loss2: 1.3450\nEpoch [1/100], Iter [30/60] Loss1: 5.1896 Loss2: 1.2801\nEpoch [1/100], Iter [31/60] Loss1: 5.6029 Loss2: 1.4466\nEpoch [1/100], Iter [32/60] Loss1: 5.6216 Loss2: 1.3374\nEpoch [1/100], Iter [33/60] Loss1: 5.5038 Loss2: 1.2706\nEpoch [1/100], Iter [34/60] Loss1: 5.3916 Loss2: 1.4083\nEpoch [1/100], Iter [35/60] Loss1: 5.5166 Loss2: 1.3549\nEpoch [1/100], Iter [36/60] Loss1: 5.3105 Loss2: 1.4457\nEpoch [1/100], Iter [37/60] Loss1: 5.3148 Loss2: 1.2556\nEpoch [1/100], Iter [38/60] Loss1: 5.1756 Loss2: 1.3222\nEpoch [1/100], Iter [39/60] Loss1: 5.2339 Loss2: 1.2794\nEpoch [1/100], Iter [40/60] Loss1: 5.4183 Loss2: 1.4711\nEpoch [1/100], Iter [41/60] Loss1: 5.1059 Loss2: 1.3536\nEpoch [1/100], Iter [42/60] Loss1: 5.1127 Loss2: 1.3095\nEpoch [1/100], Iter [43/60] Loss1: 5.1039 Loss2: 1.4028\nEpoch [1/100], Iter [44/60] Loss1: 5.2243 Loss2: 1.3054\nEpoch [1/100], Iter [45/60] Loss1: 5.2785 Loss2: 1.4347\nEpoch [1/100], Iter [46/60] Loss1: 5.0414 Loss2: 1.3100\nEpoch [1/100], Iter [47/60] Loss1: 5.2213 Loss2: 1.4308\nEpoch [1/100], Iter [48/60] Loss1: 5.2452 Loss2: 1.3510\nEpoch [1/100], Iter [49/60] Loss1: 4.6338 Loss2: 1.2942\nTest Accuracy of the model: 5.44 %\nbest: 5.44 %\nEpoch [2/100], Iter [1/60] Loss1: 5.0269 Loss2: 1.4028\nEpoch [2/100], Iter [2/60] Loss1: 5.1040 Loss2: 1.4043\nEpoch [2/100], Iter [3/60] Loss1: 4.8741 Loss2: 1.5326\nEpoch [2/100], Iter [4/60] Loss1: 4.6850 Loss2: 1.4246\nEpoch [2/100], Iter [5/60] Loss1: 5.2131 Loss2: 1.4977\nEpoch [2/100], Iter [6/60] Loss1: 4.7486 Loss2: 1.5995\nEpoch [2/100], Iter [7/60] Loss1: 4.6847 Loss2: 1.4862\nEpoch [2/100], Iter [8/60] Loss1: 5.0755 Loss2: 1.6452\nEpoch [2/100], Iter [9/60] Loss1: 4.9066 Loss2: 1.4212\nEpoch [2/100], Iter [10/60] Loss1: 5.2708 Loss2: 2.2299\nEpoch [2/100], Iter [11/60] Loss1: 4.8771 Loss2: 1.3596\nEpoch [2/100], Iter [12/60] Loss1: 4.8061 Loss2: 1.3446\nEpoch [2/100], Iter [13/60] Loss1: 4.8951 Loss2: 1.4681\nEpoch [2/100], Iter [14/60] Loss1: 4.6283 Loss2: 1.4156\nEpoch [2/100], Iter [15/60] Loss1: 4.8668 Loss2: 1.4430\nEpoch [2/100], Iter [16/60] Loss1: 4.7991 Loss2: 1.4397\nEpoch [2/100], Iter [17/60] Loss1: 5.0446 Loss2: 1.5310\nEpoch [2/100], Iter [18/60] Loss1: 4.8313 Loss2: 1.5026\nEpoch [2/100], Iter [19/60] Loss1: 4.6920 Loss2: 1.4354\nEpoch [2/100], Iter [20/60] Loss1: 4.6465 Loss2: 1.3574\nEpoch [2/100], Iter [21/60] Loss1: 4.8132 Loss2: 1.3362\nEpoch [2/100], Iter [22/60] Loss1: 4.8199 Loss2: 1.4581\nEpoch [2/100], Iter [23/60] Loss1: 5.1844 Loss2: 1.5681\nEpoch [2/100], Iter [24/60] Loss1: 5.1640 Loss2: 1.5664\nEpoch [2/100], Iter [25/60] Loss1: 4.5814 Loss2: 1.5001\nEpoch [2/100], Iter [26/60] Loss1: 4.8999 Loss2: 1.3104\nEpoch [2/100], Iter [27/60] Loss1: 4.7693 Loss2: 1.4269\nEpoch [2/100], Iter [28/60] Loss1: 4.5361 Loss2: 1.3294\nEpoch [2/100], Iter [29/60] Loss1: 5.1732 Loss2: 1.3534\nEpoch [2/100], Iter [30/60] Loss1: 4.6694 Loss2: 1.3591\nEpoch [2/100], Iter [31/60] Loss1: 4.5341 Loss2: 1.3403\nEpoch [2/100], Iter [32/60] Loss1: 4.8246 Loss2: 1.3537\nEpoch [2/100], Iter [33/60] Loss1: 4.0142 Loss2: 1.2468\nEpoch [2/100], Iter [34/60] Loss1: 4.4112 Loss2: 1.3592\nEpoch [2/100], Iter [35/60] Loss1: 4.6206 Loss2: 1.3029\nEpoch [2/100], Iter [36/60] Loss1: 4.1004 Loss2: 1.3483\nEpoch [2/100], Iter [37/60] Loss1: 4.9679 Loss2: 1.3290\nEpoch [2/100], Iter [38/60] Loss1: 4.4403 Loss2: 1.2838\nEpoch [2/100], Iter [39/60] Loss1: 4.4078 Loss2: 1.3173\nEpoch [2/100], Iter [40/60] Loss1: 4.3978 Loss2: 1.2870\nEpoch [2/100], Iter [41/60] Loss1: 4.5563 Loss2: 1.3840\nEpoch [2/100], Iter [42/60] Loss1: 4.1552 Loss2: 1.3511\nEpoch [2/100], Iter [43/60] Loss1: 4.6732 Loss2: 1.2740\nEpoch [2/100], Iter [44/60] Loss1: 4.9847 Loss2: 1.2736\nEpoch [2/100], Iter [45/60] Loss1: 4.5592 Loss2: 1.2468\nEpoch [2/100], Iter [46/60] Loss1: 4.0127 Loss2: 1.2223\nEpoch [2/100], Iter [47/60] Loss1: 4.2528 Loss2: 1.2775\nEpoch [2/100], Iter [48/60] Loss1: 4.5214 Loss2: 1.2650\nEpoch [2/100], Iter [49/60] Loss1: 4.3559 Loss2: 1.1395\nTest Accuracy of the model: 15.67 %\nbest: 15.67 %\nEpoch [3/100], Iter [1/60] Loss1: 4.2200 Loss2: 1.2920\nEpoch [3/100], Iter [2/60] Loss1: 4.1173 Loss2: 1.2785\nEpoch [3/100], Iter [3/60] Loss1: 4.2665 Loss2: 1.3465\nEpoch [3/100], Iter [4/60] Loss1: 4.2033 Loss2: 1.3029\nEpoch [3/100], Iter [5/60] Loss1: 4.2537 Loss2: 1.3832\nEpoch [3/100], Iter [6/60] Loss1: 4.2691 Loss2: 1.3049\nEpoch [3/100], Iter [7/60] Loss1: 4.7045 Loss2: 1.2192\nEpoch [3/100], Iter [8/60] Loss1: 3.8584 Loss2: 1.3359\nEpoch [3/100], Iter [9/60] Loss1: 3.8087 Loss2: 1.3782\nEpoch [3/100], Iter [10/60] Loss1: 3.7932 Loss2: 1.2798\nEpoch [3/100], Iter [11/60] Loss1: 4.2022 Loss2: 1.4304\nEpoch [3/100], Iter [12/60] Loss1: 4.2997 Loss2: 1.3363\nEpoch [3/100], Iter [13/60] Loss1: 3.9289 Loss2: 1.3609\nEpoch [3/100], Iter [14/60] Loss1: 4.2520 Loss2: 1.6032\nEpoch [3/100], Iter [15/60] Loss1: 4.1585 Loss2: 1.3705\nEpoch [3/100], Iter [16/60] Loss1: 4.0443 Loss2: 1.4566\nEpoch [3/100], Iter [17/60] Loss1: 4.0756 Loss2: 1.3754\nEpoch [3/100], Iter [18/60] Loss1: 4.1437 Loss2: 1.6570\nEpoch [3/100], Iter [19/60] Loss1: 4.2847 Loss2: 1.3550\nEpoch [3/100], Iter [20/60] Loss1: 4.5646 Loss2: 1.5542\nEpoch [3/100], Iter [21/60] Loss1: 4.2410 Loss2: 1.6447\nEpoch [3/100], Iter [22/60] Loss1: 3.7361 Loss2: 1.5686\nEpoch [3/100], Iter [23/60] Loss1: 3.8338 Loss2: 1.5060\nEpoch [3/100], Iter [24/60] Loss1: 4.0584 Loss2: 1.5683\nEpoch [3/100], Iter [25/60] Loss1: 4.0373 Loss2: 1.5307\nEpoch [3/100], Iter [26/60] Loss1: 3.7703 Loss2: 1.6387\nEpoch [3/100], Iter [27/60] Loss1: 3.8000 Loss2: 1.3683\nEpoch [3/100], Iter [28/60] Loss1: 4.0215 Loss2: 1.5466\nEpoch [3/100], Iter [29/60] Loss1: 3.3553 Loss2: 1.4340\nEpoch [3/100], Iter [30/60] Loss1: 3.7155 Loss2: 1.6114\nEpoch [3/100], Iter [31/60] Loss1: 3.6200 Loss2: 1.6143\nEpoch [3/100], Iter [32/60] Loss1: 3.7703 Loss2: 1.4001\nEpoch [3/100], Iter [33/60] Loss1: 3.8891 Loss2: 1.6254\nEpoch [3/100], Iter [34/60] Loss1: 4.0725 Loss2: 1.4518\nEpoch [3/100], Iter [35/60] Loss1: 3.6237 Loss2: 1.5981\nEpoch [3/100], Iter [36/60] Loss1: 4.0822 Loss2: 1.3759\nEpoch [3/100], Iter [37/60] Loss1: 3.7003 Loss2: 1.6946\nEpoch [3/100], Iter [38/60] Loss1: 3.9371 Loss2: 1.4224\nEpoch [3/100], Iter [39/60] Loss1: 4.0415 Loss2: 1.5125\nEpoch [3/100], Iter [40/60] Loss1: 3.7661 Loss2: 1.6068\nEpoch [3/100], Iter [41/60] Loss1: 3.5581 Loss2: 1.4314\nEpoch [3/100], Iter [42/60] Loss1: 3.5241 Loss2: 1.4084\nEpoch [3/100], Iter [43/60] Loss1: 3.5865 Loss2: 1.4447\nEpoch [3/100], Iter [44/60] Loss1: 3.3614 Loss2: 1.4420\nEpoch [3/100], Iter [45/60] Loss1: 3.8096 Loss2: 1.7027\nEpoch [3/100], Iter [46/60] Loss1: 3.9810 Loss2: 1.4554\nEpoch [3/100], Iter [47/60] Loss1: 3.9597 Loss2: 1.5592\nEpoch [3/100], Iter [48/60] Loss1: 3.9734 Loss2: 1.6373\nEpoch [3/100], Iter [49/60] Loss1: 3.2246 Loss2: 1.3334\nTest Accuracy of the model: 13.21 %\nbest: 15.67 %\nEpoch [4/100], Iter [1/60] Loss1: 3.9172 Loss2: 1.5464\nEpoch [4/100], Iter [2/60] Loss1: 3.7420 Loss2: 1.3272\nEpoch [4/100], Iter [3/60] Loss1: 3.2673 Loss2: 1.5825\nEpoch [4/100], Iter [4/60] Loss1: 3.5460 Loss2: 1.4660\nEpoch [4/100], Iter [5/60] Loss1: 3.8287 Loss2: 1.5029\nEpoch [4/100], Iter [6/60] Loss1: 3.8733 Loss2: 1.5076\nEpoch [4/100], Iter [7/60] Loss1: 3.6504 Loss2: 1.4991\nEpoch [4/100], Iter [8/60] Loss1: 3.1476 Loss2: 1.4099\nEpoch [4/100], Iter [9/60] Loss1: 3.1560 Loss2: 1.4388\nEpoch [4/100], Iter [10/60] Loss1: 3.3214 Loss2: 1.5145\nEpoch [4/100], Iter [11/60] Loss1: 3.8405 Loss2: 1.5093\nEpoch [4/100], Iter [12/60] Loss1: 3.1432 Loss2: 1.4848\nEpoch [4/100], Iter [13/60] Loss1: 4.0656 Loss2: 1.5427\nEpoch [4/100], Iter [14/60] Loss1: 3.8158 Loss2: 1.5649\nEpoch [4/100], Iter [15/60] Loss1: 3.7157 Loss2: 1.5593\nEpoch [4/100], Iter [16/60] Loss1: 3.4366 Loss2: 1.6039\nEpoch [4/100], Iter [17/60] Loss1: 4.0502 Loss2: 1.5513\nEpoch [4/100], Iter [18/60] Loss1: 3.5964 Loss2: 1.6464\nEpoch [4/100], Iter [19/60] Loss1: 3.6268 Loss2: 1.4861\nEpoch [4/100], Iter [20/60] Loss1: 3.4827 Loss2: 1.4991\nEpoch [4/100], Iter [21/60] Loss1: 3.6069 Loss2: 1.7429\nEpoch [4/100], Iter [22/60] Loss1: 4.2100 Loss2: 1.4777\nEpoch [4/100], Iter [23/60] Loss1: 3.2677 Loss2: 1.7667\nEpoch [4/100], Iter [24/60] Loss1: 3.3210 Loss2: 1.5382\nEpoch [4/100], Iter [25/60] Loss1: 3.2769 Loss2: 1.6262\nEpoch [4/100], Iter [26/60] Loss1: 3.9260 Loss2: 1.3829\nEpoch [4/100], Iter [27/60] Loss1: 3.5037 Loss2: 1.5825\nEpoch [4/100], Iter [28/60] Loss1: 4.0167 Loss2: 1.4511\nEpoch [4/100], Iter [29/60] Loss1: 3.0198 Loss2: 1.4515\nEpoch [4/100], Iter [30/60] Loss1: 3.6323 Loss2: 1.5141\nEpoch [4/100], Iter [31/60] Loss1: 3.4129 Loss2: 1.4724\nEpoch [4/100], Iter [32/60] Loss1: 3.6044 Loss2: 1.4604\nEpoch [4/100], Iter [33/60] Loss1: 3.6080 Loss2: 1.6033\nEpoch [4/100], Iter [34/60] Loss1: 3.7798 Loss2: 1.4750\nEpoch [4/100], Iter [35/60] Loss1: 3.1379 Loss2: 1.5659\nEpoch [4/100], Iter [36/60] Loss1: 3.0996 Loss2: 1.4644\nEpoch [4/100], Iter [37/60] Loss1: 3.4224 Loss2: 1.5946\nEpoch [4/100], Iter [38/60] Loss1: 3.1332 Loss2: 1.5414\nEpoch [4/100], Iter [39/60] Loss1: 3.3392 Loss2: 1.5218\nEpoch [4/100], Iter [40/60] Loss1: 3.2306 Loss2: 1.6126\nEpoch [4/100], Iter [41/60] Loss1: 3.2888 Loss2: 1.5030\nEpoch [4/100], Iter [42/60] Loss1: 3.6730 Loss2: 1.7422\nEpoch [4/100], Iter [43/60] Loss1: 3.5428 Loss2: 1.7203\nEpoch [4/100], Iter [44/60] Loss1: 3.3645 Loss2: 1.6333\nEpoch [4/100], Iter [45/60] Loss1: 3.0564 Loss2: 1.6003\nEpoch [4/100], Iter [46/60] Loss1: 3.4499 Loss2: 1.4095\nEpoch [4/100], Iter [47/60] Loss1: 3.3104 Loss2: 1.6248\nEpoch [4/100], Iter [48/60] Loss1: 3.5798 Loss2: 1.6858\nEpoch [4/100], Iter [49/60] Loss1: 3.6276 Loss2: 1.6421\nTest Accuracy of the model: 41.06 %\nbest: 41.06 %\nEpoch [5/100], Iter [1/60] Loss1: 3.0255 Loss2: 1.4530\nEpoch [5/100], Iter [2/60] Loss1: 3.2079 Loss2: 1.3903\nEpoch [5/100], Iter [3/60] Loss1: 3.1716 Loss2: 1.3756\nEpoch [5/100], Iter [4/60] Loss1: 3.5255 Loss2: 1.4567\nEpoch [5/100], Iter [5/60] Loss1: 3.6937 Loss2: 1.5259\nEpoch [5/100], Iter [6/60] Loss1: 3.3140 Loss2: 1.5534\nEpoch [5/100], Iter [7/60] Loss1: 3.5889 Loss2: 1.6253\nEpoch [5/100], Iter [8/60] Loss1: 3.5263 Loss2: 1.7157\nEpoch [5/100], Iter [9/60] Loss1: 2.6194 Loss2: 1.3925\nEpoch [5/100], Iter [10/60] Loss1: 3.8301 Loss2: 1.7602\nEpoch [5/100], Iter [11/60] Loss1: 2.9452 Loss2: 1.5143\nEpoch [5/100], Iter [12/60] Loss1: 3.7711 Loss2: 1.5906\nEpoch [5/100], Iter [13/60] Loss1: 2.9508 Loss2: 1.5877\nEpoch [5/100], Iter [14/60] Loss1: 3.2718 Loss2: 1.6786\nEpoch [5/100], Iter [15/60] Loss1: 3.2264 Loss2: 1.6849\nEpoch [5/100], Iter [16/60] Loss1: 3.4840 Loss2: 1.4293\nEpoch [5/100], Iter [17/60] Loss1: 2.4557 Loss2: 1.4783\nEpoch [5/100], Iter [18/60] Loss1: 3.3688 Loss2: 1.5618\nEpoch [5/100], Iter [19/60] Loss1: 3.0063 Loss2: 1.4299\nEpoch [5/100], Iter [20/60] Loss1: 3.1452 Loss2: 1.5510\nEpoch [5/100], Iter [21/60] Loss1: 3.4051 Loss2: 1.4414\nEpoch [5/100], Iter [22/60] Loss1: 3.5769 Loss2: 1.5049\nEpoch [5/100], Iter [23/60] Loss1: 2.9044 Loss2: 1.5009\nEpoch [5/100], Iter [24/60] Loss1: 3.1333 Loss2: 1.5483\nEpoch [5/100], Iter [25/60] Loss1: 3.4535 Loss2: 1.4552\nEpoch [5/100], Iter [26/60] Loss1: 3.0817 Loss2: 1.4444\nEpoch [5/100], Iter [27/60] Loss1: 3.2149 Loss2: 1.4304\nEpoch [5/100], Iter [28/60] Loss1: 3.1948 Loss2: 1.5267\nEpoch [5/100], Iter [29/60] Loss1: 3.4438 Loss2: 1.4910\nEpoch [5/100], Iter [30/60] Loss1: 3.7536 Loss2: 1.3930\nEpoch [5/100], Iter [31/60] Loss1: 2.9378 Loss2: 1.5385\nEpoch [5/100], Iter [32/60] Loss1: 2.7941 Loss2: 1.4033\nEpoch [5/100], Iter [33/60] Loss1: 2.8116 Loss2: 1.5545\nEpoch [5/100], Iter [34/60] Loss1: 3.0134 Loss2: 1.5508\nEpoch [5/100], Iter [35/60] Loss1: 2.8177 Loss2: 1.5897\nEpoch [5/100], Iter [36/60] Loss1: 2.6838 Loss2: 1.5625\nEpoch [5/100], Iter [37/60] Loss1: 2.8576 Loss2: 1.4396\nEpoch [5/100], Iter [38/60] Loss1: 3.4704 Loss2: 1.5211\nEpoch [5/100], Iter [39/60] Loss1: 3.0306 Loss2: 1.4693\nEpoch [5/100], Iter [40/60] Loss1: 2.8717 Loss2: 1.5983\nEpoch [5/100], Iter [41/60] Loss1: 3.2080 Loss2: 1.5480\nEpoch [5/100], Iter [42/60] Loss1: 2.8912 Loss2: 1.5568\nEpoch [5/100], Iter [43/60] Loss1: 3.4095 Loss2: 1.4358\nEpoch [5/100], Iter [44/60] Loss1: 2.9551 Loss2: 1.5463\nEpoch [5/100], Iter [45/60] Loss1: 2.7725 Loss2: 1.6671\nEpoch [5/100], Iter [46/60] Loss1: 3.0853 Loss2: 1.6303\nEpoch [5/100], Iter [47/60] Loss1: 2.9755 Loss2: 1.5567\nEpoch [5/100], Iter [48/60] Loss1: 2.9527 Loss2: 1.6182\nEpoch [5/100], Iter [49/60] Loss1: 3.1668 Loss2: 1.4090\nTest Accuracy of the model: 31.09 %\nbest: 41.06 %\nEpoch [6/100], Iter [1/60] Loss1: 3.0973 Loss2: 1.5939\nEpoch [6/100], Iter [2/60] Loss1: 3.5408 Loss2: 1.5584\nEpoch [6/100], Iter [3/60] Loss1: 3.2362 Loss2: 1.6163\nEpoch [6/100], Iter [4/60] Loss1: 2.8987 Loss2: 1.4255\nEpoch [6/100], Iter [5/60] Loss1: 2.9969 Loss2: 1.6190\nEpoch [6/100], Iter [6/60] Loss1: 2.6324 Loss2: 1.6027\nEpoch [6/100], Iter [7/60] Loss1: 3.1669 Loss2: 1.5947\nEpoch [6/100], Iter [8/60] Loss1: 3.8075 Loss2: 1.6019\nEpoch [6/100], Iter [9/60] Loss1: 2.6740 Loss2: 1.4936\nEpoch [6/100], Iter [10/60] Loss1: 2.6495 Loss2: 1.5374\nEpoch [6/100], Iter [11/60] Loss1: 2.8407 Loss2: 1.4475\nEpoch [6/100], Iter [12/60] Loss1: 2.7781 Loss2: 1.5873\nEpoch [6/100], Iter [13/60] Loss1: 3.3142 Loss2: 1.5389\nEpoch [6/100], Iter [14/60] Loss1: 3.0055 Loss2: 1.4925\nEpoch [6/100], Iter [15/60] Loss1: 2.9736 Loss2: 1.4243\nEpoch [6/100], Iter [16/60] Loss1: 2.7959 Loss2: 1.4724\nEpoch [6/100], Iter [17/60] Loss1: 2.8644 Loss2: 1.5067\nEpoch [6/100], Iter [18/60] Loss1: 2.7653 Loss2: 1.4548\nEpoch [6/100], Iter [19/60] Loss1: 3.1129 Loss2: 1.4298\nEpoch [6/100], Iter [20/60] Loss1: 2.8144 Loss2: 1.4545\nEpoch [6/100], Iter [21/60] Loss1: 2.8130 Loss2: 1.4516\nEpoch [6/100], Iter [22/60] Loss1: 2.9930 Loss2: 1.5067\nEpoch [6/100], Iter [23/60] Loss1: 2.7709 Loss2: 1.5273\nEpoch [6/100], Iter [24/60] Loss1: 3.2851 Loss2: 1.4214\nEpoch [6/100], Iter [25/60] Loss1: 2.9350 Loss2: 1.4077\nEpoch [6/100], Iter [26/60] Loss1: 3.3792 Loss2: 1.4226\nEpoch [6/100], Iter [27/60] Loss1: 3.1167 Loss2: 1.5749\nEpoch [6/100], Iter [28/60] Loss1: 2.8232 Loss2: 1.6000\nEpoch [6/100], Iter [29/60] Loss1: 2.3744 Loss2: 1.5787\nEpoch [6/100], Iter [30/60] Loss1: 2.5150 Loss2: 1.5288\nEpoch [6/100], Iter [31/60] Loss1: 3.0715 Loss2: 1.5813\nEpoch [6/100], Iter [32/60] Loss1: 2.5446 Loss2: 1.4844\nEpoch [6/100], Iter [33/60] Loss1: 3.1336 Loss2: 1.4876\nEpoch [6/100], Iter [34/60] Loss1: 2.5140 Loss2: 1.4788\nEpoch [6/100], Iter [35/60] Loss1: 2.8157 Loss2: 1.4874\nEpoch [6/100], Iter [36/60] Loss1: 3.0502 Loss2: 1.6642\nEpoch [6/100], Iter [37/60] Loss1: 3.3811 Loss2: 1.8230\nEpoch [6/100], Iter [38/60] Loss1: 3.0088 Loss2: 1.7305\nEpoch [6/100], Iter [39/60] Loss1: 2.6302 Loss2: 1.5326\nEpoch [6/100], Iter [40/60] Loss1: 3.2097 Loss2: 1.4011\nEpoch [6/100], Iter [41/60] Loss1: 2.9332 Loss2: 1.4307\nEpoch [6/100], Iter [42/60] Loss1: 3.0656 Loss2: 1.4929\nEpoch [6/100], Iter [43/60] Loss1: 2.7880 Loss2: 1.4456\nEpoch [6/100], Iter [44/60] Loss1: 3.0221 Loss2: 1.5386\nEpoch [6/100], Iter [45/60] Loss1: 2.8516 Loss2: 1.4378\nEpoch [6/100], Iter [46/60] Loss1: 2.7638 Loss2: 1.4976\nEpoch [6/100], Iter [47/60] Loss1: 3.2010 Loss2: 1.4400\nEpoch [6/100], Iter [48/60] Loss1: 3.2228 Loss2: 1.5428\nEpoch [6/100], Iter [49/60] Loss1: 3.2423 Loss2: 1.1584\nTest Accuracy of the model: 15.80 %\nbest: 41.06 %\nEpoch [7/100], Iter [1/60] Loss1: 2.8495 Loss2: 1.3498\nEpoch [7/100], Iter [2/60] Loss1: 2.5426 Loss2: 1.4764\nEpoch [7/100], Iter [3/60] Loss1: 2.7055 Loss2: 1.4081\nEpoch [7/100], Iter [4/60] Loss1: 3.1971 Loss2: 1.4720\nEpoch [7/100], Iter [5/60] Loss1: 2.5629 Loss2: 1.5001\nEpoch [7/100], Iter [6/60] Loss1: 2.5659 Loss2: 1.4534\nEpoch [7/100], Iter [7/60] Loss1: 2.7076 Loss2: 1.5857\nEpoch [7/100], Iter [8/60] Loss1: 2.6034 Loss2: 1.4290\nEpoch [7/100], Iter [9/60] Loss1: 2.6295 Loss2: 1.4438\nEpoch [7/100], Iter [10/60] Loss1: 2.5011 Loss2: 1.5337\nEpoch [7/100], Iter [11/60] Loss1: 3.3089 Loss2: 1.4207\nEpoch [7/100], Iter [12/60] Loss1: 2.9517 Loss2: 1.4754\nEpoch [7/100], Iter [13/60] Loss1: 2.6670 Loss2: 1.3862\nEpoch [7/100], Iter [14/60] Loss1: 2.6176 Loss2: 1.5679\nEpoch [7/100], Iter [15/60] Loss1: 2.6273 Loss2: 1.5180\nEpoch [7/100], Iter [16/60] Loss1: 2.5529 Loss2: 1.4176\nEpoch [7/100], Iter [17/60] Loss1: 2.5747 Loss2: 1.4997\nEpoch [7/100], Iter [18/60] Loss1: 3.1095 Loss2: 1.4661\nEpoch [7/100], Iter [19/60] Loss1: 3.1737 Loss2: 1.3679\nEpoch [7/100], Iter [20/60] Loss1: 2.9974 Loss2: 1.3926\nEpoch [7/100], Iter [21/60] Loss1: 2.4393 Loss2: 1.3216\nEpoch [7/100], Iter [22/60] Loss1: 3.0625 Loss2: 1.6700\nEpoch [7/100], Iter [23/60] Loss1: 2.4925 Loss2: 1.3782\nEpoch [7/100], Iter [24/60] Loss1: 2.5043 Loss2: 1.3693\nEpoch [7/100], Iter [25/60] Loss1: 2.4975 Loss2: 1.4093\nEpoch [7/100], Iter [26/60] Loss1: 2.8860 Loss2: 1.3131\nEpoch [7/100], Iter [27/60] Loss1: 2.3528 Loss2: 1.3678\nEpoch [7/100], Iter [28/60] Loss1: 2.8074 Loss2: 1.3015\nEpoch [7/100], Iter [29/60] Loss1: 2.5571 Loss2: 1.4109\nEpoch [7/100], Iter [30/60] Loss1: 2.3035 Loss2: 1.4298\nEpoch [7/100], Iter [31/60] Loss1: 2.4707 Loss2: 1.4603\nEpoch [7/100], Iter [32/60] Loss1: 2.3950 Loss2: 1.4181\nEpoch [7/100], Iter [33/60] Loss1: 2.5673 Loss2: 1.5011\nEpoch [7/100], Iter [34/60] Loss1: 2.5261 Loss2: 1.4375\nEpoch [7/100], Iter [35/60] Loss1: 2.0653 Loss2: 1.5082\nEpoch [7/100], Iter [36/60] Loss1: 2.3954 Loss2: 1.4206\nEpoch [7/100], Iter [37/60] Loss1: 2.3249 Loss2: 1.3291\nEpoch [7/100], Iter [38/60] Loss1: 2.6774 Loss2: 1.4079\nEpoch [7/100], Iter [39/60] Loss1: 2.6405 Loss2: 1.4012\nEpoch [7/100], Iter [40/60] Loss1: 2.3858 Loss2: 1.5761\nEpoch [7/100], Iter [41/60] Loss1: 2.1386 Loss2: 1.4037\nEpoch [7/100], Iter [42/60] Loss1: 2.4153 Loss2: 1.5099\nEpoch [7/100], Iter [43/60] Loss1: 2.0841 Loss2: 1.5926\nEpoch [7/100], Iter [44/60] Loss1: 2.9848 Loss2: 1.4361\nEpoch [7/100], Iter [45/60] Loss1: 2.0226 Loss2: 1.4222\nEpoch [7/100], Iter [46/60] Loss1: 2.1717 Loss2: 1.4622\nEpoch [7/100], Iter [47/60] Loss1: 2.6113 Loss2: 1.3994\nEpoch [7/100], Iter [48/60] Loss1: 2.7092 Loss2: 1.4472\nEpoch [7/100], Iter [49/60] Loss1: 2.6006 Loss2: 1.2460\nTest Accuracy of the model: 76.55 %\nbest: 76.55 %\nEpoch [8/100], Iter [1/60] Loss1: 2.2593 Loss2: 1.5754\nEpoch [8/100], Iter [2/60] Loss1: 2.7512 Loss2: 1.4659\nEpoch [8/100], Iter [3/60] Loss1: 2.4616 Loss2: 1.4734\nEpoch [8/100], Iter [4/60] Loss1: 2.6041 Loss2: 1.3786\nEpoch [8/100], Iter [5/60] Loss1: 2.3218 Loss2: 1.3484\nEpoch [8/100], Iter [6/60] Loss1: 2.7533 Loss2: 1.6174\nEpoch [8/100], Iter [7/60] Loss1: 2.6486 Loss2: 1.5309\nEpoch [8/100], Iter [8/60] Loss1: 2.0222 Loss2: 1.4901\nEpoch [8/100], Iter [9/60] Loss1: 2.5175 Loss2: 1.3777\nEpoch [8/100], Iter [10/60] Loss1: 2.8628 Loss2: 1.4996\nEpoch [8/100], Iter [11/60] Loss1: 2.9667 Loss2: 1.3998\nEpoch [8/100], Iter [12/60] Loss1: 2.1025 Loss2: 1.3769\nEpoch [8/100], Iter [13/60] Loss1: 2.3503 Loss2: 1.3257\nEpoch [8/100], Iter [14/60] Loss1: 1.9498 Loss2: 1.4151\nEpoch [8/100], Iter [15/60] Loss1: 2.4439 Loss2: 1.4117\nEpoch [8/100], Iter [16/60] Loss1: 2.3052 Loss2: 1.4208\nEpoch [8/100], Iter [17/60] Loss1: 2.8134 Loss2: 1.4491\nEpoch [8/100], Iter [18/60] Loss1: 2.5569 Loss2: 1.4575\nEpoch [8/100], Iter [19/60] Loss1: 2.3789 Loss2: 1.3970\nEpoch [8/100], Iter [20/60] Loss1: 2.7729 Loss2: 1.4336\nEpoch [8/100], Iter [21/60] Loss1: 2.4606 Loss2: 1.3637\nEpoch [8/100], Iter [22/60] Loss1: 2.3365 Loss2: 1.5601\nEpoch [8/100], Iter [23/60] Loss1: 2.2367 Loss2: 1.4894\nEpoch [8/100], Iter [24/60] Loss1: 2.6549 Loss2: 1.4858\nEpoch [8/100], Iter [25/60] Loss1: 2.0864 Loss2: 1.3951\nEpoch [8/100], Iter [26/60] Loss1: 2.5412 Loss2: 1.3857\nEpoch [8/100], Iter [27/60] Loss1: 2.5311 Loss2: 1.6734\nEpoch [8/100], Iter [28/60] Loss1: 2.5440 Loss2: 1.5464\nEpoch [8/100], Iter [29/60] Loss1: 2.7618 Loss2: 1.3569\nEpoch [8/100], Iter [30/60] Loss1: 1.8695 Loss2: 1.5116\nEpoch [8/100], Iter [31/60] Loss1: 2.2388 Loss2: 1.5610\nEpoch [8/100], Iter [32/60] Loss1: 2.1539 Loss2: 1.4335\nEpoch [8/100], Iter [33/60] Loss1: 3.1926 Loss2: 1.4405\nEpoch [8/100], Iter [34/60] Loss1: 2.1448 Loss2: 1.5308\nEpoch [8/100], Iter [35/60] Loss1: 2.3581 Loss2: 1.4589\nEpoch [8/100], Iter [36/60] Loss1: 2.4971 Loss2: 1.5499\nEpoch [8/100], Iter [37/60] Loss1: 2.0986 Loss2: 1.4200\nEpoch [8/100], Iter [38/60] Loss1: 2.6287 Loss2: 1.5302\nEpoch [8/100], Iter [39/60] Loss1: 2.3702 Loss2: 1.4176\nEpoch [8/100], Iter [40/60] Loss1: 1.8661 Loss2: 1.5350\nEpoch [8/100], Iter [41/60] Loss1: 2.4523 Loss2: 1.4621\nEpoch [8/100], Iter [42/60] Loss1: 2.1332 Loss2: 1.4648\nEpoch [8/100], Iter [43/60] Loss1: 2.9290 Loss2: 1.4138\nEpoch [8/100], Iter [44/60] Loss1: 2.1399 Loss2: 1.4081\nEpoch [8/100], Iter [45/60] Loss1: 2.1595 Loss2: 1.3467\nEpoch [8/100], Iter [46/60] Loss1: 2.0028 Loss2: 1.4000\nEpoch [8/100], Iter [47/60] Loss1: 2.0302 Loss2: 1.4595\nEpoch [8/100], Iter [48/60] Loss1: 2.1131 Loss2: 1.4671\nEpoch [8/100], Iter [49/60] Loss1: 2.1765 Loss2: 1.7730\nTest Accuracy of the model: 83.16 %\nbest: 83.16 %\nEpoch [9/100], Iter [1/60] Loss1: 1.8553 Loss2: 1.3909\nEpoch [9/100], Iter [2/60] Loss1: 2.4849 Loss2: 1.4127\nEpoch [9/100], Iter [3/60] Loss1: 2.4837 Loss2: 1.5087\nEpoch [9/100], Iter [4/60] Loss1: 2.3534 Loss2: 1.3954\nEpoch [9/100], Iter [5/60] Loss1: 1.5910 Loss2: 1.4262\nEpoch [9/100], Iter [6/60] Loss1: 2.3607 Loss2: 1.5941\nEpoch [9/100], Iter [7/60] Loss1: 1.8234 Loss2: 1.4820\nEpoch [9/100], Iter [8/60] Loss1: 2.4470 Loss2: 1.5021\nEpoch [9/100], Iter [9/60] Loss1: 2.0638 Loss2: 1.4469\nEpoch [9/100], Iter [10/60] Loss1: 1.8795 Loss2: 1.4966\nEpoch [9/100], Iter [11/60] Loss1: 1.6223 Loss2: 1.4579\nEpoch [9/100], Iter [12/60] Loss1: 2.5252 Loss2: 1.5034\nEpoch [9/100], Iter [13/60] Loss1: 2.2612 Loss2: 1.4481\nEpoch [9/100], Iter [14/60] Loss1: 2.3890 Loss2: 1.5083\nEpoch [9/100], Iter [15/60] Loss1: 1.8241 Loss2: 1.4532\nEpoch [9/100], Iter [16/60] Loss1: 2.6288 Loss2: 1.3928\nEpoch [9/100], Iter [17/60] Loss1: 2.1595 Loss2: 1.4483\nEpoch [9/100], Iter [18/60] Loss1: 1.9294 Loss2: 1.6618\nEpoch [9/100], Iter [19/60] Loss1: 2.4084 Loss2: 1.6018\nEpoch [9/100], Iter [20/60] Loss1: 2.5417 Loss2: 1.5500\nEpoch [9/100], Iter [21/60] Loss1: 2.0201 Loss2: 1.6356\nEpoch [9/100], Iter [22/60] Loss1: 2.4447 Loss2: 1.5076\nEpoch [9/100], Iter [23/60] Loss1: 1.8380 Loss2: 1.4778\nEpoch [9/100], Iter [24/60] Loss1: 1.9478 Loss2: 1.6585\nEpoch [9/100], Iter [25/60] Loss1: 2.2299 Loss2: 1.4354\nEpoch [9/100], Iter [26/60] Loss1: 2.1726 Loss2: 1.4000\nEpoch [9/100], Iter [27/60] Loss1: 2.0713 Loss2: 1.5126\nEpoch [9/100], Iter [28/60] Loss1: 2.0248 Loss2: 1.3465\nEpoch [9/100], Iter [29/60] Loss1: 1.6258 Loss2: 1.4903\nEpoch [9/100], Iter [30/60] Loss1: 2.3452 Loss2: 1.4917\nEpoch [9/100], Iter [31/60] Loss1: 2.1681 Loss2: 1.4690\nEpoch [9/100], Iter [32/60] Loss1: 2.4858 Loss2: 1.5023\nEpoch [9/100], Iter [33/60] Loss1: 2.4753 Loss2: 1.5031\nEpoch [9/100], Iter [34/60] Loss1: 1.8706 Loss2: 1.5034\nEpoch [9/100], Iter [35/60] Loss1: 2.6607 Loss2: 1.5183\nEpoch [9/100], Iter [36/60] Loss1: 2.3372 Loss2: 1.6013\nEpoch [9/100], Iter [37/60] Loss1: 2.2163 Loss2: 1.4592\nEpoch [9/100], Iter [38/60] Loss1: 2.2322 Loss2: 1.4607\nEpoch [9/100], Iter [39/60] Loss1: 2.2681 Loss2: 1.4646\nEpoch [9/100], Iter [40/60] Loss1: 2.0690 Loss2: 1.4648\nEpoch [9/100], Iter [41/60] Loss1: 2.0166 Loss2: 1.4129\nEpoch [9/100], Iter [42/60] Loss1: 2.0692 Loss2: 1.4582\nEpoch [9/100], Iter [43/60] Loss1: 1.7538 Loss2: 1.5305\nEpoch [9/100], Iter [44/60] Loss1: 2.2123 Loss2: 1.4688\nEpoch [9/100], Iter [45/60] Loss1: 2.3274 Loss2: 1.3560\nEpoch [9/100], Iter [46/60] Loss1: 2.5485 Loss2: 1.4048\nEpoch [9/100], Iter [47/60] Loss1: 2.6691 Loss2: 1.5566\nEpoch [9/100], Iter [48/60] Loss1: 2.3339 Loss2: 1.3455\nEpoch [9/100], Iter [49/60] Loss1: 2.7597 Loss2: 1.4280\nTest Accuracy of the model: 14.77 %\nbest: 83.16 %\nEpoch [10/100], Iter [1/60] Loss1: 2.0019 Loss2: 1.3646\nEpoch [10/100], Iter [2/60] Loss1: 2.0351 Loss2: 1.3421\nEpoch [10/100], Iter [3/60] Loss1: 2.0156 Loss2: 1.3730\nEpoch [10/100], Iter [4/60] Loss1: 2.2238 Loss2: 1.3709\nEpoch [10/100], Iter [5/60] Loss1: 2.0324 Loss2: 1.3542\nEpoch [10/100], Iter [6/60] Loss1: 2.3148 Loss2: 1.3426\nEpoch [10/100], Iter [7/60] Loss1: 2.1587 Loss2: 1.2308\nEpoch [10/100], Iter [8/60] Loss1: 1.8342 Loss2: 1.3376\nEpoch [10/100], Iter [9/60] Loss1: 2.1984 Loss2: 1.3688\nEpoch [10/100], Iter [10/60] Loss1: 2.0574 Loss2: 1.3482\nEpoch [10/100], Iter [11/60] Loss1: 2.2060 Loss2: 1.3991\nEpoch [10/100], Iter [12/60] Loss1: 2.0259 Loss2: 1.3765\nEpoch [10/100], Iter [13/60] Loss1: 2.7961 Loss2: 1.4222\nEpoch [10/100], Iter [14/60] Loss1: 1.8332 Loss2: 1.2707\nEpoch [10/100], Iter [15/60] Loss1: 2.3097 Loss2: 1.4268\nEpoch [10/100], Iter [16/60] Loss1: 2.4893 Loss2: 1.3746\nEpoch [10/100], Iter [17/60] Loss1: 2.1336 Loss2: 1.3803\nEpoch [10/100], Iter [18/60] Loss1: 2.1803 Loss2: 1.4432\nEpoch [10/100], Iter [19/60] Loss1: 2.0204 Loss2: 1.3139\nEpoch [10/100], Iter [20/60] Loss1: 2.6931 Loss2: 1.2858\nEpoch [10/100], Iter [21/60] Loss1: 2.4631 Loss2: 1.3547\nEpoch [10/100], Iter [22/60] Loss1: 2.3522 Loss2: 1.3507\nEpoch [10/100], Iter [23/60] Loss1: 2.0952 Loss2: 1.3101\nEpoch [10/100], Iter [24/60] Loss1: 2.0341 Loss2: 1.2999\nEpoch [10/100], Iter [25/60] Loss1: 2.0704 Loss2: 1.3174\nEpoch [10/100], Iter [26/60] Loss1: 1.9280 Loss2: 1.4613\nEpoch [10/100], Iter [27/60] Loss1: 1.9169 Loss2: 1.2950\nEpoch [10/100], Iter [28/60] Loss1: 2.9545 Loss2: 1.2875\nEpoch [10/100], Iter [29/60] Loss1: 2.3585 Loss2: 1.3703\nEpoch [10/100], Iter [30/60] Loss1: 1.8057 Loss2: 1.2975\nEpoch [10/100], Iter [31/60] Loss1: 2.1421 Loss2: 1.4441\nEpoch [10/100], Iter [32/60] Loss1: 2.3655 Loss2: 1.3470\nEpoch [10/100], Iter [33/60] Loss1: 2.0057 Loss2: 1.3501\nEpoch [10/100], Iter [34/60] Loss1: 1.7563 Loss2: 1.3327\nEpoch [10/100], Iter [35/60] Loss1: 2.3728 Loss2: 1.4044\nEpoch [10/100], Iter [36/60] Loss1: 1.9982 Loss2: 1.3277\nEpoch [10/100], Iter [37/60] Loss1: 2.1841 Loss2: 1.3255\nEpoch [10/100], Iter [38/60] Loss1: 2.1501 Loss2: 1.3587\nEpoch [10/100], Iter [39/60] Loss1: 2.5228 Loss2: 1.2877\nEpoch [10/100], Iter [40/60] Loss1: 2.2733 Loss2: 1.2981\nEpoch [10/100], Iter [41/60] Loss1: 1.6695 Loss2: 1.2739\nEpoch [10/100], Iter [42/60] Loss1: 1.9029 Loss2: 1.2605\nEpoch [10/100], Iter [43/60] Loss1: 1.8628 Loss2: 1.3526\nEpoch [10/100], Iter [44/60] Loss1: 2.1836 Loss2: 1.4821\nEpoch [10/100], Iter [45/60] Loss1: 2.0231 Loss2: 1.3217\nEpoch [10/100], Iter [46/60] Loss1: 2.3720 Loss2: 1.3144\nEpoch [10/100], Iter [47/60] Loss1: 1.7580 Loss2: 1.2904\nEpoch [10/100], Iter [48/60] Loss1: 2.0478 Loss2: 1.2925\nEpoch [10/100], Iter [49/60] Loss1: 2.3964 Loss2: 1.4557\nTest Accuracy of the model: 87.82 %\nbest: 87.82 %\nEpoch [11/100], Iter [1/60] Loss1: 2.0919 Loss2: 1.3849\nEpoch [11/100], Iter [2/60] Loss1: 2.2939 Loss2: 1.4279\nEpoch [11/100], Iter [3/60] Loss1: 1.6267 Loss2: 1.3186\nEpoch [11/100], Iter [4/60] Loss1: 1.8745 Loss2: 1.2589\nEpoch [11/100], Iter [5/60] Loss1: 1.8053 Loss2: 1.3317\nEpoch [11/100], Iter [6/60] Loss1: 2.0547 Loss2: 1.5235\nEpoch [11/100], Iter [7/60] Loss1: 2.4250 Loss2: 1.4653\nEpoch [11/100], Iter [8/60] Loss1: 1.4300 Loss2: 1.4381\nEpoch [11/100], Iter [9/60] Loss1: 1.7065 Loss2: 1.3369\nEpoch [11/100], Iter [10/60] Loss1: 1.9630 Loss2: 1.3509\nEpoch [11/100], Iter [11/60] Loss1: 1.8668 Loss2: 1.2971\nEpoch [11/100], Iter [12/60] Loss1: 2.2768 Loss2: 1.4675\nEpoch [11/100], Iter [13/60] Loss1: 2.1296 Loss2: 1.4694\nEpoch [11/100], Iter [14/60] Loss1: 1.8970 Loss2: 1.5140\nEpoch [11/100], Iter [15/60] Loss1: 2.4542 Loss2: 1.3402\nEpoch [11/100], Iter [16/60] Loss1: 1.9259 Loss2: 1.3324\nEpoch [11/100], Iter [17/60] Loss1: 2.3586 Loss2: 1.3645\nEpoch [11/100], Iter [18/60] Loss1: 2.2022 Loss2: 1.4049\nEpoch [11/100], Iter [19/60] Loss1: 2.0971 Loss2: 1.3781\nEpoch [11/100], Iter [20/60] Loss1: 2.2238 Loss2: 1.3111\nEpoch [11/100], Iter [21/60] Loss1: 2.1829 Loss2: 1.2215\nEpoch [11/100], Iter [22/60] Loss1: 2.2907 Loss2: 1.3326\nEpoch [11/100], Iter [23/60] Loss1: 1.9102 Loss2: 1.2889\nEpoch [11/100], Iter [24/60] Loss1: 2.2361 Loss2: 1.4136\nEpoch [11/100], Iter [25/60] Loss1: 1.9508 Loss2: 1.2020\nEpoch [11/100], Iter [26/60] Loss1: 1.5352 Loss2: 1.2695\nEpoch [11/100], Iter [27/60] Loss1: 2.0368 Loss2: 1.3261\nEpoch [11/100], Iter [28/60] Loss1: 2.0789 Loss2: 1.3530\nEpoch [11/100], Iter [29/60] Loss1: 1.9410 Loss2: 1.2037\nEpoch [11/100], Iter [30/60] Loss1: 1.8246 Loss2: 1.2508\nEpoch [11/100], Iter [31/60] Loss1: 2.3411 Loss2: 1.3370\nEpoch [11/100], Iter [32/60] Loss1: 2.3903 Loss2: 1.1850\nEpoch [11/100], Iter [33/60] Loss1: 1.5530 Loss2: 1.2829\nEpoch [11/100], Iter [34/60] Loss1: 1.6418 Loss2: 1.3123\nEpoch [11/100], Iter [35/60] Loss1: 2.2224 Loss2: 1.2564\nEpoch [11/100], Iter [36/60] Loss1: 2.3303 Loss2: 1.2490\nEpoch [11/100], Iter [37/60] Loss1: 2.0631 Loss2: 1.3082\nEpoch [11/100], Iter [38/60] Loss1: 2.0575 Loss2: 1.3552\nEpoch [11/100], Iter [39/60] Loss1: 2.1651 Loss2: 1.4021\nEpoch [11/100], Iter [40/60] Loss1: 1.9758 Loss2: 1.2638\nEpoch [11/100], Iter [41/60] Loss1: 2.6180 Loss2: 1.3711\nEpoch [11/100], Iter [42/60] Loss1: 2.0700 Loss2: 1.3029\nEpoch [11/100], Iter [43/60] Loss1: 2.1758 Loss2: 1.3091\nEpoch [11/100], Iter [44/60] Loss1: 1.8497 Loss2: 1.4249\nEpoch [11/100], Iter [45/60] Loss1: 2.0025 Loss2: 1.1857\nEpoch [11/100], Iter [46/60] Loss1: 1.9494 Loss2: 1.4373\nEpoch [11/100], Iter [47/60] Loss1: 2.2364 Loss2: 1.4663\nEpoch [11/100], Iter [48/60] Loss1: 1.6711 Loss2: 1.2626\nEpoch [11/100], Iter [49/60] Loss1: 2.7763 Loss2: 1.3535\nTest Accuracy of the model: 24.09 %\nbest: 87.82 %\nEpoch [12/100], Iter [1/60] Loss1: 1.6316 Loss2: 1.3972\nEpoch [12/100], Iter [2/60] Loss1: 2.2457 Loss2: 1.1879\nEpoch [12/100], Iter [3/60] Loss1: 1.9314 Loss2: 1.2873\nEpoch [12/100], Iter [4/60] Loss1: 2.2971 Loss2: 1.3477\nEpoch [12/100], Iter [5/60] Loss1: 1.7005 Loss2: 1.2205\nEpoch [12/100], Iter [6/60] Loss1: 1.9974 Loss2: 1.3137\nEpoch [12/100], Iter [7/60] Loss1: 2.1548 Loss2: 1.3981\nEpoch [12/100], Iter [8/60] Loss1: 1.7807 Loss2: 1.3852\nEpoch [12/100], Iter [9/60] Loss1: 2.1452 Loss2: 1.2740\nEpoch [12/100], Iter [10/60] Loss1: 1.8636 Loss2: 1.3209\nEpoch [12/100], Iter [11/60] Loss1: 2.0130 Loss2: 1.3420\nEpoch [12/100], Iter [12/60] Loss1: 1.8264 Loss2: 1.2895\nEpoch [12/100], Iter [13/60] Loss1: 2.1216 Loss2: 1.2187\nEpoch [12/100], Iter [14/60] Loss1: 1.8660 Loss2: 1.2514\nEpoch [12/100], Iter [15/60] Loss1: 1.7277 Loss2: 1.2711\nEpoch [12/100], Iter [16/60] Loss1: 2.4235 Loss2: 1.1960\nEpoch [12/100], Iter [17/60] Loss1: 1.7981 Loss2: 1.1528\nEpoch [12/100], Iter [18/60] Loss1: 1.6077 Loss2: 1.3199\nEpoch [12/100], Iter [19/60] Loss1: 2.1482 Loss2: 1.2508\nEpoch [12/100], Iter [20/60] Loss1: 2.1449 Loss2: 1.2868\nEpoch [12/100], Iter [21/60] Loss1: 1.6161 Loss2: 1.2829\nEpoch [12/100], Iter [22/60] Loss1: 1.7582 Loss2: 1.2696\nEpoch [12/100], Iter [23/60] Loss1: 1.9280 Loss2: 1.3835\nEpoch [12/100], Iter [24/60] Loss1: 2.0475 Loss2: 1.2421\nEpoch [12/100], Iter [25/60] Loss1: 2.1057 Loss2: 1.2367\nEpoch [12/100], Iter [26/60] Loss1: 2.6566 Loss2: 1.1899\nEpoch [12/100], Iter [27/60] Loss1: 2.2229 Loss2: 1.2648\nEpoch [12/100], Iter [28/60] Loss1: 1.8614 Loss2: 1.3985\nEpoch [12/100], Iter [29/60] Loss1: 1.3421 Loss2: 1.3122\nEpoch [12/100], Iter [30/60] Loss1: 2.1821 Loss2: 1.3574\nEpoch [12/100], Iter [31/60] Loss1: 2.2681 Loss2: 1.2451\nEpoch [12/100], Iter [32/60] Loss1: 1.7621 Loss2: 1.2760\nEpoch [12/100], Iter [33/60] Loss1: 2.3747 Loss2: 1.3658\nEpoch [12/100], Iter [34/60] Loss1: 1.8810 Loss2: 1.4073\nEpoch [12/100], Iter [35/60] Loss1: 2.0035 Loss2: 1.2261\nEpoch [12/100], Iter [36/60] Loss1: 2.7637 Loss2: 1.2325\nEpoch [12/100], Iter [37/60] Loss1: 2.0473 Loss2: 1.2088\nEpoch [12/100], Iter [38/60] Loss1: 2.1926 Loss2: 1.1295\nEpoch [12/100], Iter [39/60] Loss1: 2.2664 Loss2: 1.2288\nEpoch [12/100], Iter [40/60] Loss1: 1.8045 Loss2: 1.1916\nEpoch [12/100], Iter [41/60] Loss1: 1.8275 Loss2: 1.3571\nEpoch [12/100], Iter [42/60] Loss1: 1.6745 Loss2: 1.2391\nEpoch [12/100], Iter [43/60] Loss1: 2.0642 Loss2: 1.2372\nEpoch [12/100], Iter [44/60] Loss1: 2.3111 Loss2: 1.2678\nEpoch [12/100], Iter [45/60] Loss1: 1.6051 Loss2: 1.3088\nEpoch [12/100], Iter [46/60] Loss1: 1.8555 Loss2: 1.3410\nEpoch [12/100], Iter [47/60] Loss1: 1.9154 Loss2: 1.4080\nEpoch [12/100], Iter [48/60] Loss1: 1.7521 Loss2: 1.2744\nEpoch [12/100], Iter [49/60] Loss1: 1.4472 Loss2: 1.1306\nTest Accuracy of the model: 24.09 %\nbest: 87.82 %\nEpoch [13/100], Iter [1/60] Loss1: 1.9056 Loss2: 1.2812\nEpoch [13/100], Iter [2/60] Loss1: 1.5898 Loss2: 1.2481\nEpoch [13/100], Iter [3/60] Loss1: 2.0161 Loss2: 1.3092\nEpoch [13/100], Iter [4/60] Loss1: 2.0484 Loss2: 1.2071\nEpoch [13/100], Iter [5/60] Loss1: 1.8655 Loss2: 1.2873\nEpoch [13/100], Iter [6/60] Loss1: 1.7595 Loss2: 1.2431\nEpoch [13/100], Iter [7/60] Loss1: 1.5516 Loss2: 1.2728\nEpoch [13/100], Iter [8/60] Loss1: 1.7877 Loss2: 1.2649\nEpoch [13/100], Iter [9/60] Loss1: 1.4779 Loss2: 1.3087\nEpoch [13/100], Iter [10/60] Loss1: 1.8129 Loss2: 1.2074\nEpoch [13/100], Iter [11/60] Loss1: 1.7913 Loss2: 1.4048\nEpoch [13/100], Iter [12/60] Loss1: 1.4846 Loss2: 1.2446\nEpoch [13/100], Iter [13/60] Loss1: 1.8696 Loss2: 1.3287\nEpoch [13/100], Iter [14/60] Loss1: 2.2955 Loss2: 1.3067\nEpoch [13/100], Iter [15/60] Loss1: 2.1812 Loss2: 1.1942\nEpoch [13/100], Iter [16/60] Loss1: 2.1701 Loss2: 1.2004\nEpoch [13/100], Iter [17/60] Loss1: 1.3607 Loss2: 1.3220\nEpoch [13/100], Iter [18/60] Loss1: 2.1237 Loss2: 1.2078\nEpoch [13/100], Iter [19/60] Loss1: 1.4735 Loss2: 1.1706\nEpoch [13/100], Iter [20/60] Loss1: 1.4620 Loss2: 1.1663\nEpoch [13/100], Iter [21/60] Loss1: 1.6091 Loss2: 1.1574\nEpoch [13/100], Iter [22/60] Loss1: 2.3154 Loss2: 1.2727\nEpoch [13/100], Iter [23/60] Loss1: 1.9503 Loss2: 1.1054\nEpoch [13/100], Iter [24/60] Loss1: 2.2143 Loss2: 1.2222\nEpoch [13/100], Iter [25/60] Loss1: 1.9052 Loss2: 1.3212\nEpoch [13/100], Iter [26/60] Loss1: 1.5330 Loss2: 1.2325\nEpoch [13/100], Iter [27/60] Loss1: 1.7345 Loss2: 1.2001\nEpoch [13/100], Iter [28/60] Loss1: 1.8183 Loss2: 1.1929\nEpoch [13/100], Iter [29/60] Loss1: 1.5450 Loss2: 1.2308\nEpoch [13/100], Iter [30/60] Loss1: 1.6752 Loss2: 1.2419\nEpoch [13/100], Iter [31/60] Loss1: 1.9120 Loss2: 1.1286\nEpoch [13/100], Iter [32/60] Loss1: 1.9228 Loss2: 1.2167\nEpoch [13/100], Iter [33/60] Loss1: 1.2966 Loss2: 1.2119\nEpoch [13/100], Iter [34/60] Loss1: 1.5490 Loss2: 1.1640\nEpoch [13/100], Iter [35/60] Loss1: 2.1383 Loss2: 1.3182\nEpoch [13/100], Iter [36/60] Loss1: 1.9236 Loss2: 1.1832\nEpoch [13/100], Iter [37/60] Loss1: 1.4077 Loss2: 1.2815\nEpoch [13/100], Iter [38/60] Loss1: 1.6477 Loss2: 1.3692\nEpoch [13/100], Iter [39/60] Loss1: 1.8789 Loss2: 1.2630\nEpoch [13/100], Iter [40/60] Loss1: 1.3865 Loss2: 1.1887\nEpoch [13/100], Iter [41/60] Loss1: 1.7490 Loss2: 1.1817\nEpoch [13/100], Iter [42/60] Loss1: 1.6284 Loss2: 1.2334\nEpoch [13/100], Iter [43/60] Loss1: 1.2772 Loss2: 1.2214\nEpoch [13/100], Iter [44/60] Loss1: 1.9424 Loss2: 1.2643\nEpoch [13/100], Iter [45/60] Loss1: 1.6089 Loss2: 1.1757\nEpoch [13/100], Iter [46/60] Loss1: 1.7362 Loss2: 1.2963\nEpoch [13/100], Iter [47/60] Loss1: 1.8481 Loss2: 1.3935\nEpoch [13/100], Iter [48/60] Loss1: 2.0587 Loss2: 1.3166\nEpoch [13/100], Iter [49/60] Loss1: 1.6765 Loss2: 1.3561\nTest Accuracy of the model: 29.40 %\nbest: 87.82 %\nEpoch [14/100], Iter [1/60] Loss1: 1.7107 Loss2: 1.4073\nEpoch [14/100], Iter [2/60] Loss1: 2.3646 Loss2: 1.3764\nEpoch [14/100], Iter [3/60] Loss1: 1.7273 Loss2: 1.2666\nEpoch [14/100], Iter [4/60] Loss1: 1.9877 Loss2: 1.2192\nEpoch [14/100], Iter [5/60] Loss1: 1.9291 Loss2: 1.2153\nEpoch [14/100], Iter [6/60] Loss1: 1.4737 Loss2: 1.3202\nEpoch [14/100], Iter [7/60] Loss1: 2.0895 Loss2: 1.4398\nEpoch [14/100], Iter [8/60] Loss1: 2.1948 Loss2: 1.2365\nEpoch [14/100], Iter [9/60] Loss1: 1.6768 Loss2: 1.1404\nEpoch [14/100], Iter [10/60] Loss1: 2.4989 Loss2: 1.2577\nEpoch [14/100], Iter [11/60] Loss1: 1.7496 Loss2: 1.3555\nEpoch [14/100], Iter [12/60] Loss1: 2.3948 Loss2: 1.2275\nEpoch [14/100], Iter [13/60] Loss1: 1.6196 Loss2: 1.3464\nEpoch [14/100], Iter [14/60] Loss1: 1.4552 Loss2: 1.1919\nEpoch [14/100], Iter [15/60] Loss1: 1.5957 Loss2: 1.0922\nEpoch [14/100], Iter [16/60] Loss1: 1.9722 Loss2: 1.1964\nEpoch [14/100], Iter [17/60] Loss1: 1.5151 Loss2: 1.2359\nEpoch [14/100], Iter [18/60] Loss1: 1.6635 Loss2: 1.1292\nEpoch [14/100], Iter [19/60] Loss1: 1.4772 Loss2: 1.2061\nEpoch [14/100], Iter [20/60] Loss1: 1.9169 Loss2: 1.2864\nEpoch [14/100], Iter [21/60] Loss1: 1.4541 Loss2: 1.2434\nEpoch [14/100], Iter [22/60] Loss1: 1.5480 Loss2: 1.2339\nEpoch [14/100], Iter [23/60] Loss1: 1.3510 Loss2: 1.2765\nEpoch [14/100], Iter [24/60] Loss1: 1.7296 Loss2: 1.1510\nEpoch [14/100], Iter [25/60] Loss1: 1.6451 Loss2: 1.2142\nEpoch [14/100], Iter [26/60] Loss1: 1.9132 Loss2: 1.1098\nEpoch [14/100], Iter [27/60] Loss1: 1.8344 Loss2: 1.1419\nEpoch [14/100], Iter [28/60] Loss1: 1.6295 Loss2: 1.1591\nEpoch [14/100], Iter [29/60] Loss1: 1.4456 Loss2: 1.1117\nEpoch [14/100], Iter [30/60] Loss1: 2.0371 Loss2: 1.1131\nEpoch [14/100], Iter [31/60] Loss1: 1.9423 Loss2: 1.1535\nEpoch [14/100], Iter [32/60] Loss1: 1.8014 Loss2: 1.1386\nEpoch [14/100], Iter [33/60] Loss1: 2.1460 Loss2: 1.2188\nEpoch [14/100], Iter [34/60] Loss1: 1.6855 Loss2: 1.2111\nEpoch [14/100], Iter [35/60] Loss1: 1.3389 Loss2: 1.1962\nEpoch [14/100], Iter [36/60] Loss1: 1.5465 Loss2: 1.1808\nEpoch [14/100], Iter [37/60] Loss1: 1.9253 Loss2: 1.1737\nEpoch [14/100], Iter [38/60] Loss1: 1.7288 Loss2: 1.3026\nEpoch [14/100], Iter [39/60] Loss1: 2.0815 Loss2: 1.2519\nEpoch [14/100], Iter [40/60] Loss1: 1.9070 Loss2: 1.2819\nEpoch [14/100], Iter [41/60] Loss1: 1.9135 Loss2: 1.3152\nEpoch [14/100], Iter [43/60] Loss1: 1.5449 Loss2: 1.3589\nEpoch [14/100], Iter [44/60] Loss1: 2.0697 Loss2: 1.1829\nEpoch [14/100], Iter [45/60] Loss1: 1.9956 Loss2: 1.1881\nEpoch [14/100], Iter [46/60] Loss1: 1.6872 Loss2: 1.1212\nEpoch [14/100], Iter [47/60] Loss1: 1.8798 Loss2: 1.2676\nEpoch [14/100], Iter [48/60] Loss1: 1.9623 Loss2: 1.2174\nEpoch [14/100], Iter [49/60] Loss1: 1.7736 Loss2: 1.1169\nTest Accuracy of the model: 55.18 %\nbest: 87.82 %\nEpoch [15/100], Iter [1/60] Loss1: 1.6107 Loss2: 1.2236\nEpoch [15/100], Iter [2/60] Loss1: 1.9402 Loss2: 1.1988\nEpoch [15/100], Iter [3/60] Loss1: 1.5318 Loss2: 1.1594\nEpoch [15/100], Iter [4/60] Loss1: 1.4286 Loss2: 1.1865\nEpoch [15/100], Iter [5/60] Loss1: 1.8831 Loss2: 1.2291\nEpoch [15/100], Iter [6/60] Loss1: 1.8829 Loss2: 1.2409\nEpoch [15/100], Iter [7/60] Loss1: 1.4409 Loss2: 1.1855\nEpoch [15/100], Iter [8/60] Loss1: 1.7615 Loss2: 1.1578\nEpoch [15/100], Iter [9/60] Loss1: 1.7038 Loss2: 1.3228\nEpoch [15/100], Iter [10/60] Loss1: 2.2812 Loss2: 1.3308\nEpoch [15/100], Iter [11/60] Loss1: 1.5639 Loss2: 1.1872\nEpoch [15/100], Iter [12/60] Loss1: 1.6652 Loss2: 1.1747\nEpoch [15/100], Iter [13/60] Loss1: 1.7448 Loss2: 1.1920\nEpoch [15/100], Iter [14/60] Loss1: 1.7190 Loss2: 1.1797\nEpoch [15/100], Iter [15/60] Loss1: 1.7121 Loss2: 1.1210\nEpoch [15/100], Iter [16/60] Loss1: 1.7155 Loss2: 1.0894\nEpoch [15/100], Iter [17/60] Loss1: 1.3696 Loss2: 1.1176\nEpoch [15/100], Iter [18/60] Loss1: 1.8116 Loss2: 1.2090\nEpoch [15/100], Iter [19/60] Loss1: 2.2545 Loss2: 1.1315\nEpoch [15/100], Iter [20/60] Loss1: 1.5614 Loss2: 1.1445\nEpoch [15/100], Iter [21/60] Loss1: 2.0080 Loss2: 1.2603\nEpoch [15/100], Iter [22/60] Loss1: 1.5208 Loss2: 1.1722\nEpoch [15/100], Iter [23/60] Loss1: 1.6962 Loss2: 1.2143\nEpoch [15/100], Iter [24/60] Loss1: 1.1802 Loss2: 1.1602\nEpoch [15/100], Iter [25/60] Loss1: 1.2584 Loss2: 1.1092\nEpoch [15/100], Iter [26/60] Loss1: 1.6441 Loss2: 1.1755\nEpoch [15/100], Iter [27/60] Loss1: 1.4555 Loss2: 1.1424\nEpoch [15/100], Iter [28/60] Loss1: 1.6086 Loss2: 1.2358\nEpoch [15/100], Iter [29/60] Loss1: 1.3578 Loss2: 1.2138\nEpoch [15/100], Iter [30/60] Loss1: 2.2609 Loss2: 1.1546\nEpoch [15/100], Iter [31/60] Loss1: 1.9764 Loss2: 1.1369\nEpoch [15/100], Iter [32/60] Loss1: 1.0137 Loss2: 1.2416\nEpoch [15/100], Iter [33/60] Loss1: 1.4295 Loss2: 1.2339\nEpoch [15/100], Iter [34/60] Loss1: 1.6297 Loss2: 1.2181\nEpoch [15/100], Iter [35/60] Loss1: 1.5574 Loss2: 1.1770\nEpoch [15/100], Iter [36/60] Loss1: 1.7139 Loss2: 1.1655\nEpoch [15/100], Iter [37/60] Loss1: 1.8428 Loss2: 1.2438\nEpoch [15/100], Iter [38/60] Loss1: 1.7270 Loss2: 1.2272\nEpoch [15/100], Iter [39/60] Loss1: 1.6251 Loss2: 1.1605\nEpoch [15/100], Iter [40/60] Loss1: 1.7742 Loss2: 1.2762\nEpoch [15/100], Iter [41/60] Loss1: 1.6144 Loss2: 1.1704\nEpoch [15/100], Iter [42/60] Loss1: 1.4896 Loss2: 1.3010\nEpoch [15/100], Iter [43/60] Loss1: 1.3591 Loss2: 1.2839\nEpoch [15/100], Iter [44/60] Loss1: 1.8171 Loss2: 1.1596\nEpoch [15/100], Iter [45/60] Loss1: 1.5920 Loss2: 1.2213\nEpoch [15/100], Iter [46/60] Loss1: 1.1754 Loss2: 1.1477\nEpoch [15/100], Iter [47/60] Loss1: 1.4104 Loss2: 1.2917\nEpoch [15/100], Iter [48/60] Loss1: 2.0047 Loss2: 1.3464\nEpoch [15/100], Iter [49/60] Loss1: 1.3371 Loss2: 1.4113\nTest Accuracy of the model: 88.60 %\nbest: 88.60 %\nEpoch [16/100], Iter [1/60] Loss1: 1.7796 Loss2: 1.1914\nEpoch [16/100], Iter [2/60] Loss1: 2.0162 Loss2: 1.2950\nEpoch [16/100], Iter [3/60] Loss1: 1.9494 Loss2: 1.3524\nEpoch [16/100], Iter [4/60] Loss1: 1.2852 Loss2: 1.2014\nEpoch [16/100], Iter [5/60] Loss1: 1.6632 Loss2: 1.2031\nEpoch [16/100], Iter [6/60] Loss1: 1.5221 Loss2: 1.2740\nEpoch [16/100], Iter [7/60] Loss1: 1.2133 Loss2: 1.2574\nEpoch [16/100], Iter [8/60] Loss1: 1.6781 Loss2: 1.2369\nEpoch [16/100], Iter [9/60] Loss1: 1.0629 Loss2: 1.1425\nEpoch [16/100], Iter [10/60] Loss1: 1.5892 Loss2: 1.1291\nEpoch [16/100], Iter [11/60] Loss1: 1.1871 Loss2: 1.2082\nEpoch [16/100], Iter [12/60] Loss1: 1.5015 Loss2: 1.2744\nEpoch [16/100], Iter [13/60] Loss1: 1.4841 Loss2: 1.1187\nEpoch [16/100], Iter [14/60] Loss1: 1.6170 Loss2: 1.2548\nEpoch [16/100], Iter [15/60] Loss1: 1.5997 Loss2: 1.1732\nEpoch [16/100], Iter [16/60] Loss1: 1.8535 Loss2: 1.2550\nEpoch [16/100], Iter [17/60] Loss1: 1.7449 Loss2: 1.1061\nEpoch [16/100], Iter [18/60] Loss1: 2.2155 Loss2: 1.1703\nEpoch [16/100], Iter [19/60] Loss1: 1.9597 Loss2: 1.1497\nEpoch [16/100], Iter [20/60] Loss1: 1.5901 Loss2: 1.1937\nEpoch [16/100], Iter [21/60] Loss1: 2.0725 Loss2: 1.0580\nEpoch [16/100], Iter [22/60] Loss1: 1.8549 Loss2: 1.1029\nEpoch [16/100], Iter [23/60] Loss1: 1.4564 Loss2: 1.2070\nEpoch [16/100], Iter [24/60] Loss1: 1.5701 Loss2: 1.0603\nEpoch [16/100], Iter [25/60] Loss1: 1.6742 Loss2: 1.1613\nEpoch [16/100], Iter [26/60] Loss1: 1.8378 Loss2: 1.0138\nEpoch [16/100], Iter [27/60] Loss1: 1.1889 Loss2: 1.1216\nEpoch [16/100], Iter [28/60] Loss1: 2.1766 Loss2: 1.2153\nEpoch [16/100], Iter [29/60] Loss1: 1.8199 Loss2: 1.0706\nEpoch [16/100], Iter [30/60] Loss1: 2.1812 Loss2: 1.1676\nEpoch [16/100], Iter [31/60] Loss1: 1.2331 Loss2: 1.2135\nEpoch [16/100], Iter [32/60] Loss1: 1.0977 Loss2: 1.0869\nEpoch [16/100], Iter [33/60] Loss1: 1.5917 Loss2: 1.2215\nEpoch [16/100], Iter [34/60] Loss1: 1.9849 Loss2: 1.2003\nEpoch [16/100], Iter [35/60] Loss1: 1.5865 Loss2: 1.1344\nEpoch [16/100], Iter [36/60] Loss1: 1.8018 Loss2: 1.1668\nEpoch [16/100], Iter [37/60] Loss1: 1.7725 Loss2: 1.3058\nEpoch [16/100], Iter [38/60] Loss1: 2.1939 Loss2: 1.0963\nEpoch [16/100], Iter [39/60] Loss1: 1.2412 Loss2: 1.2050\nEpoch [16/100], Iter [40/60] Loss1: 1.4646 Loss2: 1.1120\nEpoch [16/100], Iter [41/60] Loss1: 1.5027 Loss2: 1.0431\nEpoch [16/100], Iter [42/60] Loss1: 1.4696 Loss2: 1.1801\nEpoch [16/100], Iter [43/60] Loss1: 1.8106 Loss2: 1.0624\nEpoch [16/100], Iter [44/60] Loss1: 1.7202 Loss2: 1.1226\nEpoch [16/100], Iter [45/60] Loss1: 1.4958 Loss2: 1.2239\nEpoch [16/100], Iter [46/60] Loss1: 1.9842 Loss2: 1.1249\nEpoch [16/100], Iter [47/60] Loss1: 1.8671 Loss2: 1.2389\nEpoch [16/100], Iter [48/60] Loss1: 1.6456 Loss2: 1.0770\nEpoch [16/100], Iter [49/60] Loss1: 1.7502 Loss2: 0.9923\nTest Accuracy of the model: 5.18 %\nbest: 88.60 %\nEpoch [17/100], Iter [1/60] Loss1: 1.7239 Loss2: 1.1431\nEpoch [17/100], Iter [2/60] Loss1: 1.9109 Loss2: 1.0831\nEpoch [17/100], Iter [3/60] Loss1: 1.3156 Loss2: 1.0055\nEpoch [17/100], Iter [4/60] Loss1: 1.6788 Loss2: 1.1701\nEpoch [17/100], Iter [5/60] Loss1: 1.7037 Loss2: 1.2103\nEpoch [17/100], Iter [6/60] Loss1: 1.3713 Loss2: 1.2554\nEpoch [17/100], Iter [7/60] Loss1: 1.8078 Loss2: 1.3386\nEpoch [17/100], Iter [8/60] Loss1: 1.8501 Loss2: 1.1467\nEpoch [17/100], Iter [9/60] Loss1: 2.0716 Loss2: 1.1016\nEpoch [17/100], Iter [10/60] Loss1: 1.4453 Loss2: 1.1237\nEpoch [17/100], Iter [11/60] Loss1: 1.8582 Loss2: 1.2660\nEpoch [17/100], Iter [12/60] Loss1: 1.9021 Loss2: 1.0081\nEpoch [17/100], Iter [13/60] Loss1: 1.8829 Loss2: 1.2157\nEpoch [17/100], Iter [14/60] Loss1: 1.4188 Loss2: 1.2255\nEpoch [17/100], Iter [15/60] Loss1: 1.3705 Loss2: 1.0709\nEpoch [17/100], Iter [16/60] Loss1: 1.5990 Loss2: 1.1382\nEpoch [17/100], Iter [17/60] Loss1: 1.5753 Loss2: 1.1036\nEpoch [17/100], Iter [18/60] Loss1: 1.7533 Loss2: 1.1373\nEpoch [17/100], Iter [19/60] Loss1: 1.6837 Loss2: 0.9828\nEpoch [17/100], Iter [20/60] Loss1: 1.2651 Loss2: 1.0473\nEpoch [17/100], Iter [21/60] Loss1: 1.7331 Loss2: 1.1818\nEpoch [17/100], Iter [22/60] Loss1: 1.1068 Loss2: 1.0955\nEpoch [17/100], Iter [23/60] Loss1: 1.6536 Loss2: 1.2252\nEpoch [17/100], Iter [24/60] Loss1: 1.8967 Loss2: 1.1331\nEpoch [17/100], Iter [25/60] Loss1: 1.2621 Loss2: 1.1948\nEpoch [17/100], Iter [26/60] Loss1: 1.2216 Loss2: 1.2144\nEpoch [17/100], Iter [27/60] Loss1: 1.6085 Loss2: 1.1251\nEpoch [17/100], Iter [28/60] Loss1: 1.3257 Loss2: 1.0728\nEpoch [17/100], Iter [29/60] Loss1: 1.2339 Loss2: 1.0675\nEpoch [17/100], Iter [30/60] Loss1: 1.3362 Loss2: 1.0835\nEpoch [17/100], Iter [31/60] Loss1: 1.8964 Loss2: 1.0727\nEpoch [17/100], Iter [32/60] Loss1: 2.0314 Loss2: 1.0830\nEpoch [17/100], Iter [33/60] Loss1: 1.1517 Loss2: 1.0675\nEpoch [17/100], Iter [34/60] Loss1: 1.5664 Loss2: 1.1223\nEpoch [17/100], Iter [35/60] Loss1: 1.7635 Loss2: 1.0815\nEpoch [17/100], Iter [36/60] Loss1: 1.3695 Loss2: 1.2026\nEpoch [17/100], Iter [37/60] Loss1: 1.9785 Loss2: 1.1644\nEpoch [17/100], Iter [38/60] Loss1: 1.5811 Loss2: 1.1697\nEpoch [17/100], Iter [39/60] Loss1: 1.1397 Loss2: 1.2107\nEpoch [17/100], Iter [40/60] Loss1: 1.3462 Loss2: 1.0961\nEpoch [17/100], Iter [41/60] Loss1: 1.6936 Loss2: 1.1015\nEpoch [17/100], Iter [42/60] Loss1: 1.3827 Loss2: 1.2771\nEpoch [17/100], Iter [43/60] Loss1: 1.7936 Loss2: 1.1833\nEpoch [17/100], Iter [44/60] Loss1: 1.4491 Loss2: 1.2040\nEpoch [17/100], Iter [45/60] Loss1: 1.5910 Loss2: 1.1532\nEpoch [17/100], Iter [46/60] Loss1: 1.4608 Loss2: 1.1018\nEpoch [17/100], Iter [47/60] Loss1: 1.8288 Loss2: 1.0528\nEpoch [17/100], Iter [48/60] Loss1: 1.6526 Loss2: 1.1716\nEpoch [17/100], Iter [49/60] Loss1: 1.7324 Loss2: 0.9706\nTest Accuracy of the model: 54.15 %\nbest: 88.60 %\nEpoch [18/100], Iter [1/60] Loss1: 1.3542 Loss2: 1.1291\nEpoch [18/100], Iter [2/60] Loss1: 1.3649 Loss2: 1.2167\nEpoch [18/100], Iter [3/60] Loss1: 1.6281 Loss2: 1.1813\nEpoch [18/100], Iter [4/60] Loss1: 1.8253 Loss2: 1.1450\nEpoch [18/100], Iter [5/60] Loss1: 1.5415 Loss2: 1.1577\nEpoch [18/100], Iter [6/60] Loss1: 1.6069 Loss2: 1.1338\nEpoch [18/100], Iter [7/60] Loss1: 1.3548 Loss2: 1.2431\nEpoch [18/100], Iter [8/60] Loss1: 1.5779 Loss2: 1.1582\nEpoch [18/100], Iter [9/60] Loss1: 1.5971 Loss2: 1.1573\nEpoch [18/100], Iter [10/60] Loss1: 1.9773 Loss2: 1.1499\nEpoch [18/100], Iter [11/60] Loss1: 1.2536 Loss2: 1.0413\nEpoch [18/100], Iter [12/60] Loss1: 1.0145 Loss2: 1.0986\nEpoch [18/100], Iter [13/60] Loss1: 1.6345 Loss2: 1.2095\nEpoch [18/100], Iter [14/60] Loss1: 1.6271 Loss2: 1.0386\nEpoch [18/100], Iter [15/60] Loss1: 1.2502 Loss2: 1.2161\nEpoch [18/100], Iter [16/60] Loss1: 1.3285 Loss2: 1.1909\nEpoch [18/100], Iter [17/60] Loss1: 1.2615 Loss2: 1.2938\nEpoch [18/100], Iter [18/60] Loss1: 1.4211 Loss2: 1.1647\nEpoch [18/100], Iter [19/60] Loss1: 2.0282 Loss2: 1.1319\nEpoch [18/100], Iter [20/60] Loss1: 1.2184 Loss2: 1.1921\nEpoch [18/100], Iter [21/60] Loss1: 1.5808 Loss2: 1.0965\nEpoch [18/100], Iter [22/60] Loss1: 1.6246 Loss2: 1.0947\nEpoch [18/100], Iter [23/60] Loss1: 1.1360 Loss2: 1.0823\nEpoch [18/100], Iter [24/60] Loss1: 1.4641 Loss2: 1.0959\nEpoch [18/100], Iter [25/60] Loss1: 1.5061 Loss2: 1.0670\nEpoch [18/100], Iter [26/60] Loss1: 1.4550 Loss2: 1.1863\nEpoch [18/100], Iter [27/60] Loss1: 1.3626 Loss2: 1.1670\nEpoch [18/100], Iter [28/60] Loss1: 1.6077 Loss2: 1.0519\nEpoch [18/100], Iter [29/60] Loss1: 1.2167 Loss2: 1.1319\nEpoch [18/100], Iter [30/60] Loss1: 1.8072 Loss2: 1.0808\nEpoch [18/100], Iter [31/60] Loss1: 1.5522 Loss2: 1.0983\nEpoch [18/100], Iter [32/60] Loss1: 1.0443 Loss2: 1.0806\nEpoch [18/100], Iter [33/60] Loss1: 1.2155 Loss2: 1.1082\nEpoch [18/100], Iter [34/60] Loss1: 1.6503 Loss2: 1.1080\nEpoch [18/100], Iter [35/60] Loss1: 1.6246 Loss2: 1.0753\nEpoch [18/100], Iter [36/60] Loss1: 1.4052 Loss2: 1.1102\nEpoch [18/100], Iter [37/60] Loss1: 1.6972 Loss2: 1.1989\nEpoch [18/100], Iter [38/60] Loss1: 1.4317 Loss2: 1.1241\nEpoch [18/100], Iter [39/60] Loss1: 1.2022 Loss2: 1.1104\nEpoch [18/100], Iter [40/60] Loss1: 1.4573 Loss2: 1.0943\nEpoch [18/100], Iter [41/60] Loss1: 1.4672 Loss2: 1.1642\nEpoch [18/100], Iter [42/60] Loss1: 1.3366 Loss2: 1.1171\nEpoch [18/100], Iter [43/60] Loss1: 1.2615 Loss2: 1.1673\nEpoch [18/100], Iter [44/60] Loss1: 1.8494 Loss2: 1.1823\nEpoch [18/100], Iter [45/60] Loss1: 1.3482 Loss2: 1.1948\nEpoch [18/100], Iter [46/60] Loss1: 1.3769 Loss2: 1.2452\nEpoch [18/100], Iter [47/60] Loss1: 1.2315 Loss2: 1.2643\nEpoch [18/100], Iter [48/60] Loss1: 1.1524 Loss2: 1.1655\nEpoch [18/100], Iter [49/60] Loss1: 2.0368 Loss2: 1.4327\nTest Accuracy of the model: 20.34 %\nbest: 88.60 %\nEpoch [19/100], Iter [1/60] Loss1: 1.6181 Loss2: 1.1901\nEpoch [19/100], Iter [2/60] Loss1: 1.5802 Loss2: 1.1628\nEpoch [19/100], Iter [3/60] Loss1: 1.3019 Loss2: 1.0907\nEpoch [19/100], Iter [4/60] Loss1: 1.2641 Loss2: 1.2200\nEpoch [19/100], Iter [5/60] Loss1: 1.3688 Loss2: 1.1639\nEpoch [19/100], Iter [6/60] Loss1: 1.4148 Loss2: 1.2002\nEpoch [19/100], Iter [7/60] Loss1: 1.1764 Loss2: 1.1393\nEpoch [19/100], Iter [8/60] Loss1: 1.3489 Loss2: 1.1990\nEpoch [19/100], Iter [9/60] Loss1: 1.2800 Loss2: 1.1972\nEpoch [19/100], Iter [10/60] Loss1: 1.4296 Loss2: 1.0469\nEpoch [19/100], Iter [11/60] Loss1: 1.3940 Loss2: 1.2104\nEpoch [19/100], Iter [12/60] Loss1: 1.3245 Loss2: 1.1615\nEpoch [19/100], Iter [13/60] Loss1: 1.6471 Loss2: 1.1030\nEpoch [19/100], Iter [14/60] Loss1: 1.3526 Loss2: 1.0883\nEpoch [19/100], Iter [15/60] Loss1: 1.6089 Loss2: 1.1742\nEpoch [19/100], Iter [16/60] Loss1: 1.4067 Loss2: 1.1616\nEpoch [19/100], Iter [17/60] Loss1: 1.0781 Loss2: 1.0814\nEpoch [19/100], Iter [18/60] Loss1: 1.5449 Loss2: 1.2797\nEpoch [19/100], Iter [19/60] Loss1: 1.3208 Loss2: 1.2033\nEpoch [19/100], Iter [20/60] Loss1: 1.6430 Loss2: 1.2142\nEpoch [19/100], Iter [21/60] Loss1: 1.4834 Loss2: 1.1831\nEpoch [19/100], Iter [22/60] Loss1: 1.0041 Loss2: 1.0912\nEpoch [19/100], Iter [23/60] Loss1: 1.2082 Loss2: 1.1593\nEpoch [19/100], Iter [24/60] Loss1: 1.2860 Loss2: 1.1859\nEpoch [19/100], Iter [25/60] Loss1: 1.2566 Loss2: 1.2013\nEpoch [19/100], Iter [26/60] Loss1: 1.2057 Loss2: 1.1832\nEpoch [19/100], Iter [27/60] Loss1: 1.2160 Loss2: 1.1257\nEpoch [19/100], Iter [28/60] Loss1: 1.3066 Loss2: 1.2139\nEpoch [19/100], Iter [29/60] Loss1: 1.6036 Loss2: 1.1117\nEpoch [19/100], Iter [30/60] Loss1: 1.3254 Loss2: 1.1360\nEpoch [19/100], Iter [31/60] Loss1: 1.2295 Loss2: 1.0622\nEpoch [19/100], Iter [32/60] Loss1: 1.4362 Loss2: 1.2107\nEpoch [19/100], Iter [33/60] Loss1: 1.6362 Loss2: 1.1349\nEpoch [19/100], Iter [34/60] Loss1: 1.3382 Loss2: 1.0338\nEpoch [19/100], Iter [35/60] Loss1: 1.2442 Loss2: 1.1857\nEpoch [19/100], Iter [36/60] Loss1: 1.3594 Loss2: 1.0907\nEpoch [19/100], Iter [37/60] Loss1: 1.5082 Loss2: 1.1618\nEpoch [19/100], Iter [38/60] Loss1: 1.4894 Loss2: 1.0567\nEpoch [19/100], Iter [39/60] Loss1: 1.4340 Loss2: 1.0839\nEpoch [19/100], Iter [40/60] Loss1: 1.4885 Loss2: 1.1308\nEpoch [19/100], Iter [41/60] Loss1: 1.4190 Loss2: 1.2225\nEpoch [19/100], Iter [42/60] Loss1: 1.7213 Loss2: 1.1925\nEpoch [19/100], Iter [43/60] Loss1: 0.8151 Loss2: 1.1789\nEpoch [19/100], Iter [44/60] Loss1: 1.5806 Loss2: 1.1451\nEpoch [19/100], Iter [45/60] Loss1: 1.6713 Loss2: 1.1857\nEpoch [19/100], Iter [46/60] Loss1: 1.4517 Loss2: 1.1696\nEpoch [19/100], Iter [47/60] Loss1: 1.7264 Loss2: 1.1156\nEpoch [19/100], Iter [48/60] Loss1: 0.7987 Loss2: 1.1316\nEpoch [19/100], Iter [49/60] Loss1: 1.3344 Loss2: 0.9355\nTest Accuracy of the model: 19.82 %\nbest: 88.60 %\nEpoch [20/100], Iter [1/60] Loss1: 1.4145 Loss2: 1.1489\nEpoch [20/100], Iter [2/60] Loss1: 1.2249 Loss2: 1.1031\nEpoch [20/100], Iter [3/60] Loss1: 1.3513 Loss2: 1.0930\nEpoch [20/100], Iter [4/60] Loss1: 1.4172 Loss2: 1.1281\nEpoch [20/100], Iter [5/60] Loss1: 1.0788 Loss2: 1.0864\nEpoch [20/100], Iter [6/60] Loss1: 1.0207 Loss2: 1.0423\nEpoch [20/100], Iter [7/60] Loss1: 1.1474 Loss2: 1.0841\nEpoch [20/100], Iter [8/60] Loss1: 1.1548 Loss2: 1.1355\nEpoch [20/100], Iter [9/60] Loss1: 1.2708 Loss2: 1.1477\nEpoch [20/100], Iter [10/60] Loss1: 1.2400 Loss2: 1.2143\nEpoch [20/100], Iter [11/60] Loss1: 1.7093 Loss2: 1.1665\nEpoch [20/100], Iter [12/60] Loss1: 1.6399 Loss2: 1.2104\nEpoch [20/100], Iter [13/60] Loss1: 1.4648 Loss2: 1.1936\nEpoch [20/100], Iter [14/60] Loss1: 1.1956 Loss2: 1.1410\nEpoch [20/100], Iter [15/60] Loss1: 1.6691 Loss2: 1.1447\nEpoch [20/100], Iter [16/60] Loss1: 1.2404 Loss2: 1.2611\nEpoch [20/100], Iter [17/60] Loss1: 1.2686 Loss2: 1.2199\nEpoch [20/100], Iter [18/60] Loss1: 1.5075 Loss2: 1.1473\nEpoch [20/100], Iter [19/60] Loss1: 1.1852 Loss2: 1.2871\nEpoch [20/100], Iter [20/60] Loss1: 1.1240 Loss2: 1.2506\nEpoch [20/100], Iter [21/60] Loss1: 1.2794 Loss2: 1.1499\nEpoch [20/100], Iter [22/60] Loss1: 1.8897 Loss2: 1.3303\nEpoch [20/100], Iter [23/60] Loss1: 1.4742 Loss2: 1.0702\nEpoch [20/100], Iter [24/60] Loss1: 1.3070 Loss2: 1.2253\nEpoch [20/100], Iter [25/60] Loss1: 1.3348 Loss2: 1.0835\nEpoch [20/100], Iter [26/60] Loss1: 1.2616 Loss2: 1.0785\nEpoch [20/100], Iter [27/60] Loss1: 1.6313 Loss2: 1.0679\nEpoch [20/100], Iter [28/60] Loss1: 1.6258 Loss2: 1.1587\nEpoch [20/100], Iter [29/60] Loss1: 1.0664 Loss2: 1.0623\nEpoch [20/100], Iter [30/60] Loss1: 1.7283 Loss2: 1.1326\nEpoch [20/100], Iter [31/60] Loss1: 0.9477 Loss2: 1.0345\nEpoch [20/100], Iter [32/60] Loss1: 1.3873 Loss2: 1.0692\nEpoch [20/100], Iter [33/60] Loss1: 1.5401 Loss2: 1.0773\nEpoch [20/100], Iter [34/60] Loss1: 1.6639 Loss2: 1.0647\nEpoch [20/100], Iter [35/60] Loss1: 1.2362 Loss2: 0.9794\nEpoch [20/100], Iter [36/60] Loss1: 1.7762 Loss2: 1.0356\nEpoch [20/100], Iter [37/60] Loss1: 1.6445 Loss2: 1.0988\nEpoch [20/100], Iter [38/60] Loss1: 1.2423 Loss2: 1.1174\nEpoch [20/100], Iter [39/60] Loss1: 1.4904 Loss2: 1.1140\nEpoch [20/100], Iter [40/60] Loss1: 1.8233 Loss2: 1.1469\nEpoch [20/100], Iter [41/60] Loss1: 1.3402 Loss2: 1.0943\nEpoch [20/100], Iter [42/60] Loss1: 1.6613 Loss2: 1.1400\nEpoch [20/100], Iter [43/60] Loss1: 1.5930 Loss2: 1.0322\nEpoch [20/100], Iter [44/60] Loss1: 1.1965 Loss2: 0.9993\nEpoch [20/100], Iter [45/60] Loss1: 1.2945 Loss2: 0.9555\nEpoch [20/100], Iter [46/60] Loss1: 1.6617 Loss2: 1.0638\nEpoch [20/100], Iter [47/60] Loss1: 1.0080 Loss2: 1.1198\nEpoch [20/100], Iter [48/60] Loss1: 1.5616 Loss2: 1.0302\nEpoch [20/100], Iter [49/60] Loss1: 1.5323 Loss2: 1.1624\nTest Accuracy of the model: 57.38 %\nbest: 88.60 %\nEpoch [21/100], Iter [1/60] Loss1: 1.5554 Loss2: 1.0934\nEpoch [21/100], Iter [2/60] Loss1: 1.2212 Loss2: 1.2008\nEpoch [21/100], Iter [3/60] Loss1: 1.0708 Loss2: 1.1703\nEpoch [21/100], Iter [4/60] Loss1: 1.2162 Loss2: 1.0243\nEpoch [21/100], Iter [5/60] Loss1: 1.6031 Loss2: 1.0884\nEpoch [21/100], Iter [6/60] Loss1: 1.4718 Loss2: 1.1964\nEpoch [21/100], Iter [7/60] Loss1: 1.1715 Loss2: 1.0699\nEpoch [21/100], Iter [8/60] Loss1: 1.7823 Loss2: 1.0856\nEpoch [21/100], Iter [9/60] Loss1: 1.2853 Loss2: 1.1575\nEpoch [21/100], Iter [10/60] Loss1: 1.6576 Loss2: 1.0267\nEpoch [21/100], Iter [11/60] Loss1: 1.4630 Loss2: 0.9846\nEpoch [21/100], Iter [12/60] Loss1: 1.4412 Loss2: 1.1635\nEpoch [21/100], Iter [13/60] Loss1: 1.3494 Loss2: 1.1490\nEpoch [21/100], Iter [14/60] Loss1: 0.9975 Loss2: 1.0951\nEpoch [21/100], Iter [15/60] Loss1: 1.3848 Loss2: 1.1680\nEpoch [21/100], Iter [16/60] Loss1: 1.0069 Loss2: 1.0723\nEpoch [21/100], Iter [17/60] Loss1: 1.5303 Loss2: 1.0584\nEpoch [21/100], Iter [18/60] Loss1: 1.5880 Loss2: 1.1173\nEpoch [21/100], Iter [19/60] Loss1: 1.6862 Loss2: 1.0873\nEpoch [21/100], Iter [20/60] Loss1: 1.2475 Loss2: 1.0897\nEpoch [21/100], Iter [21/60] Loss1: 1.1768 Loss2: 1.0378\nEpoch [21/100], Iter [22/60] Loss1: 1.2736 Loss2: 0.9789\nEpoch [21/100], Iter [23/60] Loss1: 1.3654 Loss2: 1.1211\nEpoch [21/100], Iter [24/60] Loss1: 1.1263 Loss2: 1.1752\nEpoch [21/100], Iter [25/60] Loss1: 1.2865 Loss2: 1.1808\nEpoch [21/100], Iter [26/60] Loss1: 1.5748 Loss2: 1.1539\nEpoch [21/100], Iter [27/60] Loss1: 1.1593 Loss2: 1.0703\nEpoch [21/100], Iter [28/60] Loss1: 1.0493 Loss2: 1.0723\nEpoch [21/100], Iter [29/60] Loss1: 1.3590 Loss2: 1.0854\nEpoch [21/100], Iter [30/60] Loss1: 1.5085 Loss2: 1.1204\nEpoch [21/100], Iter [31/60] Loss1: 1.3948 Loss2: 0.9965\nEpoch [21/100], Iter [32/60] Loss1: 1.5066 Loss2: 1.1447\nEpoch [21/100], Iter [33/60] Loss1: 1.8692 Loss2: 1.0540\nEpoch [21/100], Iter [34/60] Loss1: 1.5128 Loss2: 1.0462\nEpoch [21/100], Iter [35/60] Loss1: 1.3965 Loss2: 1.0210\nEpoch [21/100], Iter [36/60] Loss1: 1.2748 Loss2: 1.0394\nEpoch [21/100], Iter [37/60] Loss1: 1.1520 Loss2: 1.0261\nEpoch [21/100], Iter [38/60] Loss1: 1.3137 Loss2: 1.0847\nEpoch [21/100], Iter [39/60] Loss1: 1.1144 Loss2: 1.1377\nEpoch [21/100], Iter [40/60] Loss1: 1.1229 Loss2: 1.1066\nEpoch [21/100], Iter [41/60] Loss1: 2.0155 Loss2: 1.1597\nEpoch [21/100], Iter [42/60] Loss1: 1.2319 Loss2: 1.0328\nEpoch [21/100], Iter [43/60] Loss1: 1.4314 Loss2: 1.1047\nEpoch [21/100], Iter [44/60] Loss1: 1.5060 Loss2: 1.1091\nEpoch [21/100], Iter [45/60] Loss1: 1.6385 Loss2: 1.0481\nEpoch [21/100], Iter [46/60] Loss1: 1.6974 Loss2: 1.0751\nEpoch [21/100], Iter [47/60] Loss1: 1.0188 Loss2: 1.0720\nEpoch [21/100], Iter [48/60] Loss1: 1.3387 Loss2: 1.0646\nEpoch [21/100], Iter [49/60] Loss1: 1.7026 Loss2: 1.0828\nTest Accuracy of the model: 32.12 %\nbest: 88.60 %\nEpoch [22/100], Iter [1/60] Loss1: 1.0593 Loss2: 1.0635\nEpoch [22/100], Iter [2/60] Loss1: 1.6002 Loss2: 1.1537\nEpoch [22/100], Iter [3/60] Loss1: 1.2295 Loss2: 1.0429\nEpoch [22/100], Iter [4/60] Loss1: 1.3606 Loss2: 1.1809\nEpoch [22/100], Iter [5/60] Loss1: 1.3837 Loss2: 1.1609\nEpoch [22/100], Iter [6/60] Loss1: 1.6211 Loss2: 1.2009\nEpoch [22/100], Iter [7/60] Loss1: 1.2973 Loss2: 1.1591\nEpoch [22/100], Iter [8/60] Loss1: 1.5482 Loss2: 1.0894\nEpoch [22/100], Iter [9/60] Loss1: 1.2279 Loss2: 1.0519\nEpoch [22/100], Iter [10/60] Loss1: 1.2569 Loss2: 1.0692\nEpoch [22/100], Iter [11/60] Loss1: 1.4687 Loss2: 1.1727\nEpoch [22/100], Iter [12/60] Loss1: 1.6431 Loss2: 1.0790\nEpoch [22/100], Iter [13/60] Loss1: 1.1248 Loss2: 1.1702\nEpoch [22/100], Iter [14/60] Loss1: 1.6810 Loss2: 1.0133\nEpoch [22/100], Iter [15/60] Loss1: 1.2205 Loss2: 1.1056\nEpoch [22/100], Iter [16/60] Loss1: 1.5442 Loss2: 1.0807\nEpoch [22/100], Iter [17/60] Loss1: 1.1842 Loss2: 1.0837\nEpoch [22/100], Iter [18/60] Loss1: 0.9684 Loss2: 1.1060\nEpoch [22/100], Iter [19/60] Loss1: 1.3278 Loss2: 1.1322\nEpoch [22/100], Iter [20/60] Loss1: 1.4726 Loss2: 1.1947\nEpoch [22/100], Iter [21/60] Loss1: 1.4713 Loss2: 1.1350\nEpoch [22/100], Iter [22/60] Loss1: 1.2894 Loss2: 1.1208\nEpoch [22/100], Iter [23/60] Loss1: 2.1738 Loss2: 1.1852\nEpoch [22/100], Iter [24/60] Loss1: 1.1064 Loss2: 1.1194\nEpoch [22/100], Iter [25/60] Loss1: 1.0138 Loss2: 1.0738\nEpoch [22/100], Iter [26/60] Loss1: 1.2968 Loss2: 1.1498\nEpoch [22/100], Iter [27/60] Loss1: 1.1177 Loss2: 0.9878\nEpoch [22/100], Iter [28/60] Loss1: 1.5406 Loss2: 1.1250\nEpoch [22/100], Iter [29/60] Loss1: 1.3937 Loss2: 0.9653\nEpoch [22/100], Iter [30/60] Loss1: 1.4788 Loss2: 1.0460\nEpoch [22/100], Iter [31/60] Loss1: 1.9167 Loss2: 0.9750\nEpoch [22/100], Iter [32/60] Loss1: 1.2695 Loss2: 1.0762\nEpoch [22/100], Iter [33/60] Loss1: 1.1837 Loss2: 1.0637\nEpoch [22/100], Iter [34/60] Loss1: 1.5263 Loss2: 0.9460\nEpoch [22/100], Iter [35/60] Loss1: 1.1197 Loss2: 0.9895\nEpoch [22/100], Iter [36/60] Loss1: 1.3959 Loss2: 1.0017\nEpoch [22/100], Iter [37/60] Loss1: 1.3374 Loss2: 1.0353\nEpoch [22/100], Iter [38/60] Loss1: 1.0185 Loss2: 1.0195\nEpoch [22/100], Iter [39/60] Loss1: 1.1061 Loss2: 1.0461\nEpoch [22/100], Iter [40/60] Loss1: 1.4821 Loss2: 1.0355\nEpoch [22/100], Iter [41/60] Loss1: 1.1009 Loss2: 1.0069\nEpoch [22/100], Iter [42/60] Loss1: 1.2855 Loss2: 0.9484\nEpoch [22/100], Iter [43/60] Loss1: 1.1951 Loss2: 1.0077\nEpoch [22/100], Iter [44/60] Loss1: 1.1934 Loss2: 0.9851\nEpoch [22/100], Iter [45/60] Loss1: 1.3836 Loss2: 0.9751\nEpoch [22/100], Iter [46/60] Loss1: 1.0226 Loss2: 0.9828\nEpoch [22/100], Iter [47/60] Loss1: 1.2630 Loss2: 1.0819\nEpoch [22/100], Iter [48/60] Loss1: 1.3666 Loss2: 1.0577\nEpoch [22/100], Iter [49/60] Loss1: 1.2194 Loss2: 0.9685\nTest Accuracy of the model: 33.55 %\nbest: 88.60 %\nEpoch [23/100], Iter [1/60] Loss1: 1.3622 Loss2: 0.9931\nEpoch [23/100], Iter [2/60] Loss1: 0.9247 Loss2: 1.0246\nEpoch [23/100], Iter [3/60] Loss1: 1.3490 Loss2: 1.0734\nEpoch [23/100], Iter [4/60] Loss1: 1.0398 Loss2: 1.0017\nEpoch [23/100], Iter [5/60] Loss1: 1.3793 Loss2: 1.0429\nEpoch [23/100], Iter [6/60] Loss1: 1.3181 Loss2: 1.0963\nEpoch [23/100], Iter [7/60] Loss1: 1.1262 Loss2: 1.1011\nEpoch [23/100], Iter [8/60] Loss1: 1.3825 Loss2: 1.1070\nEpoch [23/100], Iter [9/60] Loss1: 1.3471 Loss2: 1.0846\nEpoch [23/100], Iter [10/60] Loss1: 1.7224 Loss2: 0.9923\nEpoch [23/100], Iter [11/60] Loss1: 1.3374 Loss2: 1.0304\nEpoch [23/100], Iter [12/60] Loss1: 1.4368 Loss2: 1.1949\nEpoch [23/100], Iter [13/60] Loss1: 1.3512 Loss2: 0.9889\nEpoch [23/100], Iter [14/60] Loss1: 1.1163 Loss2: 1.1475\nEpoch [23/100], Iter [15/60] Loss1: 1.4404 Loss2: 1.0357\nEpoch [23/100], Iter [16/60] Loss1: 1.5166 Loss2: 1.0322\nEpoch [23/100], Iter [17/60] Loss1: 1.2794 Loss2: 1.1399\nEpoch [23/100], Iter [18/60] Loss1: 1.4450 Loss2: 1.0002\nEpoch [23/100], Iter [19/60] Loss1: 0.9267 Loss2: 0.9842\nEpoch [23/100], Iter [20/60] Loss1: 1.5923 Loss2: 1.0905\nEpoch [23/100], Iter [21/60] Loss1: 0.8164 Loss2: 1.0394\nEpoch [23/100], Iter [22/60] Loss1: 1.0908 Loss2: 1.0670\nEpoch [23/100], Iter [23/60] Loss1: 1.6831 Loss2: 1.0470\nEpoch [23/100], Iter [24/60] Loss1: 0.9623 Loss2: 0.9814\nEpoch [23/100], Iter [25/60] Loss1: 1.3629 Loss2: 1.0611\nEpoch [23/100], Iter [26/60] Loss1: 1.2859 Loss2: 1.0589\nEpoch [23/100], Iter [27/60] Loss1: 0.9897 Loss2: 1.0442\nEpoch [23/100], Iter [28/60] Loss1: 1.4068 Loss2: 1.1236\nEpoch [23/100], Iter [29/60] Loss1: 1.2914 Loss2: 1.0011\nEpoch [23/100], Iter [30/60] Loss1: 1.1416 Loss2: 1.0231\nEpoch [23/100], Iter [31/60] Loss1: 1.2327 Loss2: 1.0423\nEpoch [23/100], Iter [32/60] Loss1: 1.2478 Loss2: 1.0598\nEpoch [23/100], Iter [33/60] Loss1: 1.5385 Loss2: 1.0980\nEpoch [23/100], Iter [34/60] Loss1: 0.7908 Loss2: 0.9526\nEpoch [23/100], Iter [35/60] Loss1: 1.2062 Loss2: 1.0888\nEpoch [23/100], Iter [36/60] Loss1: 1.1222 Loss2: 1.0144\nEpoch [23/100], Iter [37/60] Loss1: 1.3325 Loss2: 1.0455\nEpoch [23/100], Iter [38/60] Loss1: 1.0068 Loss2: 1.1276\nEpoch [23/100], Iter [39/60] Loss1: 1.6117 Loss2: 0.9941\nEpoch [23/100], Iter [40/60] Loss1: 1.4165 Loss2: 1.1162\nEpoch [23/100], Iter [41/60] Loss1: 1.0815 Loss2: 1.0172\nEpoch [23/100], Iter [42/60] Loss1: 1.0220 Loss2: 1.0522\nEpoch [23/100], Iter [43/60] Loss1: 1.1020 Loss2: 1.1618\nEpoch [23/100], Iter [44/60] Loss1: 1.4883 Loss2: 1.0824\nEpoch [23/100], Iter [45/60] Loss1: 1.5230 Loss2: 1.0444\nEpoch [23/100], Iter [46/60] Loss1: 1.5641 Loss2: 1.1238\nEpoch [23/100], Iter [47/60] Loss1: 1.1331 Loss2: 1.1283\nEpoch [23/100], Iter [48/60] Loss1: 1.0060 Loss2: 1.0640\nEpoch [23/100], Iter [49/60] Loss1: 1.7762 Loss2: 0.9177\nTest Accuracy of the model: 91.58 %\nbest: 91.58 %\nEpoch [24/100], Iter [1/60] Loss1: 0.8743 Loss2: 1.0400\nEpoch [24/100], Iter [2/60] Loss1: 1.2910 Loss2: 1.0536\nEpoch [24/100], Iter [3/60] Loss1: 1.3343 Loss2: 0.9886\nEpoch [24/100], Iter [4/60] Loss1: 0.9289 Loss2: 1.0002\nEpoch [24/100], Iter [5/60] Loss1: 1.3521 Loss2: 1.1337\nEpoch [24/100], Iter [6/60] Loss1: 1.4679 Loss2: 1.0908\nEpoch [24/100], Iter [7/60] Loss1: 1.3963 Loss2: 1.0617\nEpoch [24/100], Iter [8/60] Loss1: 1.0206 Loss2: 1.0397\nEpoch [24/100], Iter [9/60] Loss1: 1.1558 Loss2: 1.0973\nEpoch [24/100], Iter [10/60] Loss1: 1.3005 Loss2: 1.0747\nEpoch [24/100], Iter [11/60] Loss1: 0.9728 Loss2: 1.0591\nEpoch [24/100], Iter [12/60] Loss1: 0.8828 Loss2: 1.0900\nEpoch [24/100], Iter [13/60] Loss1: 1.3021 Loss2: 1.0223\nEpoch [24/100], Iter [14/60] Loss1: 1.5417 Loss2: 1.0329\nEpoch [24/100], Iter [15/60] Loss1: 1.2968 Loss2: 1.0061\nEpoch [24/100], Iter [16/60] Loss1: 1.3652 Loss2: 1.0607\nEpoch [24/100], Iter [17/60] Loss1: 1.4922 Loss2: 1.1134\nEpoch [24/100], Iter [18/60] Loss1: 1.4539 Loss2: 1.0653\nEpoch [24/100], Iter [19/60] Loss1: 1.3069 Loss2: 1.0265\nEpoch [24/100], Iter [20/60] Loss1: 1.4009 Loss2: 1.0426\nEpoch [24/100], Iter [21/60] Loss1: 1.5912 Loss2: 1.0632\nEpoch [24/100], Iter [22/60] Loss1: 0.9836 Loss2: 1.0741\nEpoch [24/100], Iter [23/60] Loss1: 1.3908 Loss2: 1.0359\nEpoch [24/100], Iter [24/60] Loss1: 1.0137 Loss2: 1.1062\nEpoch [24/100], Iter [25/60] Loss1: 0.9928 Loss2: 0.9879\nEpoch [24/100], Iter [26/60] Loss1: 1.3226 Loss2: 0.9961\nEpoch [24/100], Iter [27/60] Loss1: 1.3874 Loss2: 0.9887\nEpoch [24/100], Iter [28/60] Loss1: 1.3934 Loss2: 1.0535\nEpoch [24/100], Iter [29/60] Loss1: 1.7259 Loss2: 1.0364\nEpoch [24/100], Iter [30/60] Loss1: 1.0027 Loss2: 1.0278\nEpoch [24/100], Iter [31/60] Loss1: 1.8805 Loss2: 1.1137\nEpoch [24/100], Iter [32/60] Loss1: 1.1685 Loss2: 0.9966\nEpoch [24/100], Iter [33/60] Loss1: 1.3452 Loss2: 1.0412\nEpoch [24/100], Iter [34/60] Loss1: 1.9223 Loss2: 0.9836\nEpoch [24/100], Iter [35/60] Loss1: 0.9473 Loss2: 1.0407\nEpoch [24/100], Iter [36/60] Loss1: 1.1895 Loss2: 1.2290\nEpoch [24/100], Iter [37/60] Loss1: 1.2650 Loss2: 1.0970\nEpoch [24/100], Iter [38/60] Loss1: 0.9186 Loss2: 1.0785\nEpoch [24/100], Iter [39/60] Loss1: 1.4142 Loss2: 0.9703\nEpoch [24/100], Iter [40/60] Loss1: 1.1659 Loss2: 1.0226\nEpoch [24/100], Iter [41/60] Loss1: 1.0232 Loss2: 0.9978\nEpoch [24/100], Iter [42/60] Loss1: 1.5946 Loss2: 0.9892\nEpoch [24/100], Iter [43/60] Loss1: 0.8590 Loss2: 0.9684\nEpoch [24/100], Iter [44/60] Loss1: 1.4284 Loss2: 1.0089\nEpoch [24/100], Iter [45/60] Loss1: 0.9530 Loss2: 1.0306\nEpoch [24/100], Iter [46/60] Loss1: 1.5939 Loss2: 0.9666\nEpoch [24/100], Iter [47/60] Loss1: 1.3197 Loss2: 0.9751\nEpoch [24/100], Iter [48/60] Loss1: 1.2825 Loss2: 1.0292\nEpoch [24/100], Iter [49/60] Loss1: 1.8320 Loss2: 0.9030\nTest Accuracy of the model: 73.06 %\nbest: 91.58 %\nEpoch [25/100], Iter [1/60] Loss1: 1.2334 Loss2: 1.0098\nEpoch [25/100], Iter [2/60] Loss1: 1.1968 Loss2: 0.9198\nEpoch [25/100], Iter [3/60] Loss1: 1.0729 Loss2: 1.1074\nEpoch [25/100], Iter [4/60] Loss1: 1.0559 Loss2: 1.1513\nEpoch [25/100], Iter [5/60] Loss1: 1.0098 Loss2: 1.0509\nEpoch [25/100], Iter [6/60] Loss1: 1.3510 Loss2: 1.0031\nEpoch [25/100], Iter [7/60] Loss1: 1.3224 Loss2: 0.9615\nEpoch [25/100], Iter [8/60] Loss1: 1.1864 Loss2: 1.1688\nEpoch [25/100], Iter [9/60] Loss1: 1.8730 Loss2: 1.0829\nEpoch [25/100], Iter [10/60] Loss1: 1.1856 Loss2: 1.0604\nEpoch [25/100], Iter [11/60] Loss1: 0.8282 Loss2: 1.1188\nEpoch [25/100], Iter [12/60] Loss1: 1.3242 Loss2: 1.0741\nEpoch [25/100], Iter [13/60] Loss1: 1.0171 Loss2: 1.1628\nEpoch [25/100], Iter [14/60] Loss1: 1.5410 Loss2: 1.0039\nEpoch [25/100], Iter [15/60] Loss1: 1.0670 Loss2: 1.0687\nEpoch [25/100], Iter [16/60] Loss1: 0.9749 Loss2: 1.0889\nEpoch [25/100], Iter [17/60] Loss1: 1.2601 Loss2: 1.0471\nEpoch [25/100], Iter [18/60] Loss1: 1.6883 Loss2: 0.9921\nEpoch [25/100], Iter [19/60] Loss1: 1.1318 Loss2: 1.0774\nEpoch [25/100], Iter [20/60] Loss1: 1.4458 Loss2: 0.9874\nEpoch [25/100], Iter [21/60] Loss1: 1.1932 Loss2: 1.1345\nEpoch [25/100], Iter [22/60] Loss1: 0.9489 Loss2: 1.0801\nEpoch [25/100], Iter [23/60] Loss1: 1.4454 Loss2: 1.0883\nEpoch [25/100], Iter [24/60] Loss1: 1.0248 Loss2: 0.9410\nEpoch [25/100], Iter [25/60] Loss1: 1.2699 Loss2: 0.9388\nEpoch [25/100], Iter [26/60] Loss1: 1.4167 Loss2: 0.9346\nEpoch [25/100], Iter [27/60] Loss1: 1.1505 Loss2: 1.0078\nEpoch [25/100], Iter [28/60] Loss1: 1.3574 Loss2: 1.0131\nEpoch [25/100], Iter [29/60] Loss1: 1.3949 Loss2: 1.0194\nEpoch [25/100], Iter [30/60] Loss1: 1.2308 Loss2: 0.9303\nEpoch [25/100], Iter [31/60] Loss1: 1.5938 Loss2: 0.9398\nEpoch [25/100], Iter [32/60] Loss1: 1.1583 Loss2: 1.0028\nEpoch [25/100], Iter [33/60] Loss1: 0.9575 Loss2: 1.1310\nEpoch [25/100], Iter [34/60] Loss1: 1.3957 Loss2: 0.9860\nEpoch [25/100], Iter [35/60] Loss1: 0.7166 Loss2: 0.9837\nEpoch [25/100], Iter [36/60] Loss1: 0.9097 Loss2: 1.0421\nEpoch [25/100], Iter [37/60] Loss1: 1.3217 Loss2: 0.9716\nEpoch [25/100], Iter [38/60] Loss1: 1.3742 Loss2: 1.0096\nEpoch [25/100], Iter [39/60] Loss1: 1.1947 Loss2: 0.9845\nEpoch [25/100], Iter [40/60] Loss1: 1.0393 Loss2: 0.9743\nEpoch [25/100], Iter [41/60] Loss1: 1.6758 Loss2: 1.1774\nEpoch [25/100], Iter [42/60] Loss1: 0.9537 Loss2: 1.0318\nEpoch [25/100], Iter [43/60] Loss1: 0.8629 Loss2: 1.0872\nEpoch [25/100], Iter [44/60] Loss1: 1.1493 Loss2: 0.9578\nEpoch [25/100], Iter [45/60] Loss1: 1.2025 Loss2: 1.0481\nEpoch [25/100], Iter [46/60] Loss1: 1.4737 Loss2: 1.0900\nEpoch [25/100], Iter [47/60] Loss1: 0.9982 Loss2: 0.9933\nEpoch [25/100], Iter [48/60] Loss1: 0.9947 Loss2: 0.9657\nEpoch [25/100], Iter [49/60] Loss1: 1.4627 Loss2: 0.9432\nTest Accuracy of the model: 96.24 %\nbest: 96.24 %\nEpoch [26/100], Iter [1/60] Loss1: 0.9280 Loss2: 1.0258\nEpoch [26/100], Iter [2/60] Loss1: 1.3374 Loss2: 1.0314\nEpoch [26/100], Iter [3/60] Loss1: 0.6506 Loss2: 0.9313\nEpoch [26/100], Iter [4/60] Loss1: 1.3380 Loss2: 1.0765\nEpoch [26/100], Iter [5/60] Loss1: 1.2986 Loss2: 0.9412\nEpoch [26/100], Iter [6/60] Loss1: 1.4863 Loss2: 1.0792\nEpoch [26/100], Iter [7/60] Loss1: 1.0900 Loss2: 1.0766\nEpoch [26/100], Iter [8/60] Loss1: 0.9998 Loss2: 1.1731\nEpoch [26/100], Iter [9/60] Loss1: 1.3166 Loss2: 1.0171\nEpoch [26/100], Iter [10/60] Loss1: 1.2705 Loss2: 1.1399\nEpoch [26/100], Iter [11/60] Loss1: 1.2532 Loss2: 1.0744\nEpoch [26/100], Iter [12/60] Loss1: 1.1456 Loss2: 1.0945\nEpoch [26/100], Iter [13/60] Loss1: 1.5423 Loss2: 1.0827\nEpoch [26/100], Iter [14/60] Loss1: 1.3910 Loss2: 1.0974\nEpoch [26/100], Iter [15/60] Loss1: 0.7963 Loss2: 1.0623\nEpoch [26/100], Iter [16/60] Loss1: 1.0614 Loss2: 1.0749\nEpoch [26/100], Iter [17/60] Loss1: 1.0248 Loss2: 1.1160\nEpoch [26/100], Iter [18/60] Loss1: 0.9863 Loss2: 1.0922\nEpoch [26/100], Iter [19/60] Loss1: 1.3537 Loss2: 1.0699\nEpoch [26/100], Iter [20/60] Loss1: 0.8183 Loss2: 0.9998\nEpoch [26/100], Iter [21/60] Loss1: 1.8198 Loss2: 1.1348\nEpoch [26/100], Iter [22/60] Loss1: 0.9981 Loss2: 1.0835\nEpoch [26/100], Iter [23/60] Loss1: 1.0929 Loss2: 1.1093\nEpoch [26/100], Iter [24/60] Loss1: 1.1652 Loss2: 1.1692\nEpoch [26/100], Iter [25/60] Loss1: 1.0981 Loss2: 1.1197\nEpoch [26/100], Iter [26/60] Loss1: 0.9506 Loss2: 1.0400\nEpoch [26/100], Iter [27/60] Loss1: 1.0950 Loss2: 0.9965\nEpoch [26/100], Iter [28/60] Loss1: 1.2683 Loss2: 1.1287\nEpoch [26/100], Iter [29/60] Loss1: 1.0334 Loss2: 1.1705\nEpoch [26/100], Iter [30/60] Loss1: 1.1880 Loss2: 1.0281\nEpoch [26/100], Iter [31/60] Loss1: 1.6212 Loss2: 1.0835\nEpoch [26/100], Iter [32/60] Loss1: 1.1570 Loss2: 1.0702\nEpoch [26/100], Iter [33/60] Loss1: 0.9104 Loss2: 1.0979\nEpoch [26/100], Iter [34/60] Loss1: 0.9902 Loss2: 1.0743\nEpoch [26/100], Iter [35/60] Loss1: 1.3876 Loss2: 1.1561\nEpoch [26/100], Iter [36/60] Loss1: 1.0127 Loss2: 1.0993\nEpoch [26/100], Iter [37/60] Loss1: 1.0541 Loss2: 1.1430\nEpoch [26/100], Iter [38/60] Loss1: 1.7878 Loss2: 1.0910\nEpoch [26/100], Iter [39/60] Loss1: 1.1302 Loss2: 1.1255\nEpoch [26/100], Iter [40/60] Loss1: 1.1820 Loss2: 1.1162\nEpoch [26/100], Iter [41/60] Loss1: 1.2467 Loss2: 1.0816\nEpoch [26/100], Iter [42/60] Loss1: 1.1608 Loss2: 1.0632\nEpoch [26/100], Iter [43/60] Loss1: 1.5107 Loss2: 1.0723\nEpoch [26/100], Iter [44/60] Loss1: 1.3411 Loss2: 1.1392\nEpoch [26/100], Iter [45/60] Loss1: 1.2936 Loss2: 1.0475\nEpoch [26/100], Iter [46/60] Loss1: 1.0690 Loss2: 1.0834\nEpoch [26/100], Iter [47/60] Loss1: 1.1547 Loss2: 1.0207\nEpoch [26/100], Iter [48/60] Loss1: 1.4799 Loss2: 1.0516\nEpoch [26/100], Iter [49/60] Loss1: 2.1437 Loss2: 0.8201\n","output_type":"stream"}]}]}